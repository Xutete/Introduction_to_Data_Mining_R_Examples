[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"book contains documented R examples accompany several chapters popular data mining textbook\nIntroduction Data Mining\nPang-Ning Tan, Michael Steinbach, Anuj Karpatne Vipin Kumar.\ncompanion book can used either edition: 1st edition (Tan, Steinbach, Kumar 2005) 2nd edition (Tan et al. 2017).code examples collected book developed course CS 7331 - Data Mining\ntaught SMU since Spring 2013 regularly updated improved.\nlatest update includes use popular packages meta-package tidyverse (Wickham 2021c) including ggplot2 (Wickham, Chang, et al. 2021) data wrangling visualization along \ncaret (Kuhn 2021) model building.Please use edit function within book visit\nbook’s GitHub project page submit corrections suggest\nimprovements. cite book use:Michael Hahsler (2021). R Companion Introduction Data Mining. Online Book. https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/hope book helps learn use R efficiently data mining projects.Michael Hahsler","code":""},{"path":"index.html","id":"license","chapter":"Preface","heading":"License","text":" online version book licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.cover art based “rocks” stebulus licensed CC 2.0.","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Packages used chapter: ggplot2 (Wickham, Chang, et al. 2021), tidyverse (Wickham 2021c)companion book assumes R RStudio Desktop installed familiar basics R, run R code install packages.new R, working official R manual Introduction R (Venables, Smith, R Core Team 2021) get started.\nmany introduction videos RStudio basic video shows run code install packages suffice.code book uses tidyverse manipulate data ggplot2 visualization.\ngreat introduction useful tools can found freely available web book R Data Science Wickham Grolemund (2017).tidyverse (Wickham 2021c) collection many useful packages work well together \nsharing design principles data structures. tidyverse also includes ggplot2 (Wickham, Chang, et al. 2021) visualization.book, usetidyverse tibbles replace R’s built-data.frames,pipe operator %>% chain functions together, anddata transformation functions like filter(), arrange(), select(), group_by(), \nmutate() provided tidyverse package dplyr.good introduction can found \nSection Data Wrangling (Wickham Grolemund 2017),\n\nuseful reference resource RStudio Data Transformation Cheat Sheet.short example. create tibble price dollars per pound vitamin C content milligrams (mg) per pound three fruit.Now add column vitamin C (mg) dollar buys , filter fruit\nprovides 20 mg order (arrange) data vitamin C per dollar largest smallest.pipes operator %>% lets compose sequence function calls readable way passing value left first argument function right.visualization, use mainly ggplot2.\ngg ggplot2 stands Grammar Graphics introduced \nWilkinson (2005).\nmain idea every graph built basic components:data,coordinate system, andvisual marks representing data (geoms).ggplot2, components combined using + operator.ggplot(data, mapping = aes(x = ..., y = ..., color = ...)) +\ngeom_point()Since typically use Cartesian coordinate system, ggplot uses default.\ngeom_ function uses stat_ function calculate visualizes. example,\ngeom_bar uses stat_count create bar chart counting often value appears data (see ? geom_bar). geom_point just uses stat \"identity\" display points using coordinates .\ngreat introduction can found Chapter Data Visualization (Wickham Grolemund 2017),\nuseful \nRStudio’s Data Visualization Cheat Sheet.can visualize fruit data scatter plot.Alternatively, can visualize vitamin C content fruit using bars.\nNote geom_bar default tries aggregate data counting, just want visualize \nvalue already available tibble, specify identity statistic.","code":"\nlibrary(tidyverse)## ── Attaching packages ────────────── tidyverse 1.3.1 ──## ✓ ggplot2 3.3.5     ✓ purrr   0.3.4\n## ✓ tibble  3.1.2     ✓ dplyr   1.0.7\n## ✓ tidyr   1.1.3     ✓ stringr 1.4.0\n## ✓ readr   1.4.0     ✓ forcats 0.5.1## ── Conflicts ───────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()\nfruit <- tibble(\n  name = c(\"apple\", \"banana\", \"orange\"), \n  price = c(2.5, 2.0, 3.5), \n  vitamin_c = c(20, 45, 250))\nfruit## # A tibble: 3 x 3\n##   name   price vitamin_c\n##   <chr>  <dbl>     <dbl>\n## 1 apple    2.5        20\n## 2 banana   2          45\n## 3 orange   3.5       250\naffordable_vitamin_c_sources <- fruit %>% \n  mutate(vitamin_c_per_dollar = vitamin_c / price) %>% \n  filter(vitamin_c_per_dollar > 20) %>% arrange(desc(vitamin_c_per_dollar))\naffordable_vitamin_c_sources ## # A tibble: 2 x 4\n##   name   price vitamin_c vitamin_c_per_dollar\n##   <chr>  <dbl>     <dbl>                <dbl>\n## 1 orange   3.5       250                 71.4\n## 2 banana   2          45                 22.5\nggplot(fruit, aes(x = price, y = vitamin_c)) + \n  geom_point()\nggplot(fruit, aes(x = name, y = vitamin_c)) + \n  geom_bar(stat = \"identity\")"},{"path":"data.html","id":"data","chapter":"2 Data","heading":"2 Data","text":"chapter gives examples cleaning preparing data data mining.Packages used chapter: arules (Hahsler et al. 2021), caret (Kuhn 2021), factoextra (Kassambara Mundt 2020), GGally (Schloerke et al. 2021), plotly (Sievert et al. 2021), proxy (Meyer Buchta 2021), sampling (Tillé Matei 2021), seriation (Hahsler, Buchta, Hornik 2021), tidyverse (Wickham 2021c)","code":""},{"path":"data.html","id":"the-iris-dataset","chapter":"2 Data","heading":"2.1 The Iris Dataset","text":"use toy dataset comes R. Fisher’s iris data set gives measurements centimeters variables sepal length width petal length width, respectively, 150 50 flowers 3 species iris. species Iris Setosa, Iris Versicolor, Iris Virginica.\ndetails see: ? irisLoad iris data set convert data.frame tibble. Note: datasets come R R packages can loaded data().see data contains 150 rows (flowers) 5 features. tibbles show first rows show features, fit screen width. can force print show features changing width.","code":"\nlibrary(tidyverse)\ndata(iris)\niris <- as_tibble(iris)\niris## # A tibble: 150 x 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1          5.1         3.5          1.4         0.2\n##  2          4.9         3            1.4         0.2\n##  3          4.7         3.2          1.3         0.2\n##  4          4.6         3.1          1.5         0.2\n##  5          5           3.6          1.4         0.2\n##  6          5.4         3.9          1.7         0.4\n##  7          4.6         3.4          1.4         0.3\n##  8          5           3.4          1.5         0.2\n##  9          4.4         2.9          1.4         0.2\n## 10          4.9         3.1          1.5         0.1\n## # … with 140 more rows, and 1 more variable:\n## #   Species <fct>\nprint(iris, width = Inf)## # A tibble: 150 x 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1          5.1         3.5          1.4         0.2\n##  2          4.9         3            1.4         0.2\n##  3          4.7         3.2          1.3         0.2\n##  4          4.6         3.1          1.5         0.2\n##  5          5           3.6          1.4         0.2\n##  6          5.4         3.9          1.7         0.4\n##  7          4.6         3.4          1.4         0.3\n##  8          5           3.4          1.5         0.2\n##  9          4.4         2.9          1.4         0.2\n## 10          4.9         3.1          1.5         0.1\n##    Species\n##    <fct>  \n##  1 setosa \n##  2 setosa \n##  3 setosa \n##  4 setosa \n##  5 setosa \n##  6 setosa \n##  7 setosa \n##  8 setosa \n##  9 setosa \n## 10 setosa \n## # … with 140 more rows"},{"path":"data.html","id":"data-quality","chapter":"2 Data","heading":"2.2 Data Quality","text":"Assessing quality available data crucial start using data.\nStart summary statistics column identify outliers missing values.can also summarize specific columns using statistic function like mean.Another way inspect data use scatterplot matrix (use ggpairs package GGally). plot, can visually identify noise data points ouliers (points far majority points).See can spot one red dot far away others.need complete data many data mining methods. remove missing values (NA) \nduplicates (identical data points might mistake data), often :Note one case (non-unique) gone leaving 149 flowers. data contain missing values, , also dropped.\nTypically, spend time data cleaning.","code":"\nsummary(iris)##   Sepal.Length   Sepal.Width    Petal.Length \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60  \n##  Median :5.80   Median :3.00   Median :4.35  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90  \n##   Petal.Width        Species  \n##  Min.   :0.1   setosa    :50  \n##  1st Qu.:0.3   versicolor:50  \n##  Median :1.3   virginica :50  \n##  Mean   :1.2                  \n##  3rd Qu.:1.8                  \n##  Max.   :2.5\niris %>% summarize_if(is.numeric, mean)## # A tibble: 1 x 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         5.84        3.06         3.76        1.20\nlibrary(GGally)## Registered S3 method overwritten by 'GGally':\n##   method from   \n##   +.gg   ggplot2\nggpairs(iris, aes(color = Species))## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\nclean.data <- iris %>% drop_na() %>% unique()\nsummary(clean.data)##   Sepal.Length   Sepal.Width    Petal.Length \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60  \n##  Median :5.80   Median :3.00   Median :4.30  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.75  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90  \n##   Petal.Width        Species  \n##  Min.   :0.1   setosa    :50  \n##  1st Qu.:0.3   versicolor:50  \n##  Median :1.3   virginica :49  \n##  Mean   :1.2                  \n##  3rd Qu.:1.8                  \n##  Max.   :2.5"},{"path":"data.html","id":"aggregation","chapter":"2 Data","heading":"2.3 Aggregation","text":"Data often contains groups want compare groups.\ngroup iris dataset species calculate summary statistic group.Using information, can compare features differ groups.","code":"\niris %>% group_by(Species) %>% summarize_all(mean)## # A tibble: 3 x 5\n##   Species    Sepal.Length Sepal.Width Petal.Length\n##   <fct>             <dbl>       <dbl>        <dbl>\n## 1 setosa             5.01        3.43         1.46\n## 2 versicolor         5.94        2.77         4.26\n## 3 virginica          6.59        2.97         5.55\n## # … with 1 more variable: Petal.Width <dbl>\niris %>% group_by(Species) %>% summarize_all(median)## # A tibble: 3 x 5\n##   Species    Sepal.Length Sepal.Width Petal.Length\n##   <fct>             <dbl>       <dbl>        <dbl>\n## 1 setosa              5           3.4         1.5 \n## 2 versicolor          5.9         2.8         4.35\n## 3 virginica           6.5         3           5.55\n## # … with 1 more variable: Petal.Width <dbl>"},{"path":"data.html","id":"sampling","chapter":"2 Data","heading":"2.4 Sampling","text":"Sampling often used data mining reduce dataset size.","code":""},{"path":"data.html","id":"random-sampling","chapter":"2 Data","heading":"2.4.1 Random Sampling","text":"built-sample function can sample vector replacement.often want sample rows dataset. can done sampling without replacement vector row indices (using functions seq nrow). sample vector used subset rows dataset.dplyr tidyverse lets us sample rows tibbles directly. set random number generator seed make results reproducible.","code":"\nsample(c(\"A\", \"B\", \"C\"), size = 10, replace = TRUE)##  [1] \"A\" \"A\" \"C\" \"A\" \"B\" \"B\" \"A\" \"C\" \"B\" \"B\"\ntake <- sample(seq(nrow(iris)), size = 15)\ntake##  [1]  47  70  89  33  82  73 106  71   2 112  60  37\n## [13]   1  72 137\niris[take, ]## # A tibble: 15 x 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1          5.1         3.8          1.6         0.2\n##  2          5.6         2.5          3.9         1.1\n##  3          5.6         3            4.1         1.3\n##  4          5.2         4.1          1.5         0.1\n##  5          5.5         2.4          3.7         1  \n##  6          6.3         2.5          4.9         1.5\n##  7          7.6         3            6.6         2.1\n##  8          5.9         3.2          4.8         1.8\n##  9          4.9         3            1.4         0.2\n## 10          6.4         2.7          5.3         1.9\n## 11          5.2         2.7          3.9         1.4\n## 12          5.5         3.5          1.3         0.2\n## 13          5.1         3.5          1.4         0.2\n## 14          6.1         2.8          4           1.3\n## 15          6.3         3.4          5.6         2.4\n## # … with 1 more variable: Species <fct>\nset.seed(1000)\n\ns <- iris %>% slice_sample(n = 15)\nggpairs(s, aes(color = Species))## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`."},{"path":"data.html","id":"stratified-sampling","chapter":"2 Data","heading":"2.4.2 Stratified Sampling","text":"Stratified sampling\nmethod sampling population can partitioned subpopulations,\ncontrolling proportions subpopulation resulting sample.following, subpopulations different types species want\nmake sure sample number (5) flowers . library sampling\nprovides function stratified sampling. column ID_unit resulting data.frame\ncontains row numbers sampled rows. can use slice dplyr select \nsampled rows.","code":"\nlibrary(sampling)\nid2 <- strata(iris, stratanames = \"Species\", size = c(5,5,5), method = \"srswor\")\nid2##        Species ID_unit Prob Stratum\n## 7       setosa       7  0.1       1\n## 9       setosa       9  0.1       1\n## 10      setosa      10  0.1       1\n## 24      setosa      24  0.1       1\n## 48      setosa      48  0.1       1\n## 58  versicolor      58  0.1       2\n## 62  versicolor      62  0.1       2\n## 74  versicolor      74  0.1       2\n## 78  versicolor      78  0.1       2\n## 99  versicolor      99  0.1       2\n## 106  virginica     106  0.1       3\n## 107  virginica     107  0.1       3\n## 127  virginica     127  0.1       3\n## 135  virginica     135  0.1       3\n## 145  virginica     145  0.1       3\ns2 <- iris %>% slice(id2$ID_unit)\nggpairs(s2, aes(color = Species))## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`."},{"path":"data.html","id":"features","chapter":"2 Data","heading":"2.5 Features","text":"","code":""},{"path":"data.html","id":"dimensionality-reduction","chapter":"2 Data","heading":"2.5.1 Dimensionality Reduction","text":"","code":""},{"path":"data.html","id":"principal-components-analysis-pca","chapter":"2 Data","heading":"2.5.1.1 Principal Components Analysis (PCA)","text":"PCA calculates principal components (new orthonormal basis vectors data space) data points first principal component explains variability data, second next .\ndata analysis, PCA used project high-dimensional data points onto first (typically two) principal components visualization scatter plot preprocessing modeling (e.g., k-means clustering).\nPoints closer together high-dimensional space, tend also closer together lower-dimensional space,Look 3d data using interactive 3d plot (needs package plotly). However, 3d plots hard print iris data actually 4 dimensions.principal components can calculated matrix using function prcomp(). select\nnumeric columns (removing species column) convert data.frame matrix \ncalculation.important principal component can also seen using scree plot. plot function result \nprcomp function visualizes much variability data explained additional principal component.Note first principal component (PC1) explains variability iris dataset.find information stored object pc, can inspect \nraw object (display structure).element x contains data points projected principal components.\ncan convert matrix tibble add species column original\ndataset display data projected first two principal components.Since first principal component represents variability, can\nalso show data projected PC1.plot projected data original axes added arrows called \nbiplot. arrows (original axes)\nalign roughly axes projection, correlated (linearly dependent).can also display old new axes.see Petal.Width Petal.Length point direction indicates highly correlated. also roughly aligned PC1 (called Dim1 plot) means PC1 represents variability two variables. Sepal.Width parallel y-axis therefore represented PC2 (Dim2). Petal.Width/Petal.Length Sepal.Width almost 90 degrees, indicating close uncorrelated. Sepal.Length correlated variables \nrepresented , PC1 PC2 projection.Another popular method project data lower dimensions visualization t-distributed stochastic neighbor embedding (t-SNE) available package Rtsne.","code":"\n##library(plotly) # I don't load the package because it's namespace clashes with select in dplyr.\nplotly::plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length, z = ~Sepal.Width,\n  size = ~Petal.Width, color = ~Species, type=\"scatter3d\")## No scatter3d mode specifed:\n##   Setting the mode to markers\n##   Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode## Warning: `line.width` does not currently support\n## multiple values.\n\n## Warning: `line.width` does not currently support\n## multiple values.\n\n## Warning: `line.width` does not currently support\n## multiple values.\npc <- iris %>% select(-Species) %>% as.matrix() %>% prcomp()\nsummary(pc)## Importance of components:\n##                          PC1    PC2    PC3     PC4\n## Standard deviation     2.056 0.4926 0.2797 0.15439\n## Proportion of Variance 0.925 0.0531 0.0171 0.00521\n## Cumulative Proportion  0.925 0.9777 0.9948 1.00000\nplot(pc, type = \"line\")\nstr(pc)## List of 5\n##  $ sdev    : num [1:4] 2.056 0.493 0.28 0.154\n##  $ rotation: num [1:4, 1:4] 0.3614 -0.0845 0.8567 0.3583 -0.6566 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n##   .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\"\n##  $ center  : Named num [1:4] 5.84 3.06 3.76 1.2\n##   ..- attr(*, \"names\")= chr [1:4] \"Sepal.Length\" \"Sepal.Width\" \"Petal.Length\" \"Petal.Width\"\n##  $ scale   : logi FALSE\n##  $ x       : num [1:150, 1:4] -2.68 -2.71 -2.89 -2.75 -2.73 ...\n##   ..- attr(*, \"dimnames\")=List of 2\n##   .. ..$ : NULL\n##   .. ..$ : chr [1:4] \"PC1\" \"PC2\" \"PC3\" \"PC4\"\n##  - attr(*, \"class\")= chr \"prcomp\"\niris_projected <- as_tibble(pc$x) %>% add_column(Species = iris$Species)\nggplot(iris_projected, aes(x = PC1, y = PC2, color = Species)) + \n  geom_point()\nggplot(iris_projected, \n  aes(x = PC1, y = 0, color = Species)) + \n  geom_point() +\n  scale_y_continuous(expand=c(0,0)) +\n  theme(axis.text.y = element_blank(),\n      axis.title.y = element_blank()\n  )\nlibrary(factoextra)## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\nfviz_pca(pc)\nfviz_pca_var(pc)"},{"path":"data.html","id":"multi-dimensional-scaling-mds","chapter":"2 Data","heading":"2.5.1.2 Multi-Dimensional Scaling (MDS)","text":"MDS similar PCA. Instead data points, takes pairwise distances (.e., distance matrix) produces space points placed represent distances well possible. axis space called components similar principal components PCA.First, calculate distance matrix 4-d space iris dataset.Metric (classic) MDS tries reconstruct 2-d space distance matrix. Points smaller distances closer reconstructed space.","code":"\nd <- iris %>% select(-Species) %>% dist()\nfit <- cmdscale(d, k = 2)\ncolnames(fit) <- c(\"comp1\", \"comp2\")\nfit <- as_tibble(fit) %>% add_column(Species = iris$Species)\n\nggplot(fit, aes(x = comp1, y = comp2, color = Species)) + geom_point()"},{"path":"data.html","id":"non-parametric-multidimensional-scaling","chapter":"2 Data","heading":"2.5.1.3 Non-Parametric Multidimensional Scaling","text":"Non-parametric multidimensional scaling performs MDS relaxing need linear relationships. Methods available package MASS functions\nisoMDS sammon.","code":""},{"path":"data.html","id":"feature-selection","chapter":"2 Data","heading":"2.5.2 Feature Selection","text":"Feature selection process identifying features used create model.\ntalk feature selection discuss classification models Chapter 3 Feature Selection Feature Preparation.","code":""},{"path":"data.html","id":"discretize-features","chapter":"2 Data","heading":"2.5.3 Discretize Features","text":"data mining methods require discrete data.\nDiscretization converts continuous features discrete features.Petal.Width continuous feature. perform discretization, look \ndistribution see gives us idea group continuous values \nset discrete values. histogram visualizes distribution single\ncontinuous feature.R function cut performs equal interval width discretization.discretization methods include equal frequency discretization using k-means clustering.\nmethods implemented several R packages. use implementation package\narules visualize results histograms blue lines separate intervals assigned \ndiscrete value.","code":"\nggplot(iris, aes(x = Petal.Width)) + geom_histogram(binwidth = .2)\niris %>% pull(Sepal.Width) %>% cut(breaks = 3)##   [1] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##   [6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [11] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (3.6,4.4]\n##  [16] (3.6,4.4] (3.6,4.4] (2.8,3.6] (3.6,4.4] (3.6,4.4]\n##  [21] (2.8,3.6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [26] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [31] (2.8,3.6] (2.8,3.6] (3.6,4.4] (3.6,4.4] (2.8,3.6]\n##  [36] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n##  [41] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (3.6,4.4]\n##  [46] (2.8,3.6] (3.6,4.4] (2.8,3.6] (3.6,4.4] (2.8,3.6]\n##  [51] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]  \n##  [56] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]  \n##  [61] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6]\n##  [66] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]  \n##  [71] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n##  [76] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2,2.8]  \n##  [81] (2,2.8]   (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n##  [86] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]  \n##  [91] (2,2.8]   (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]  \n##  [96] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8]   (2,2.8]  \n## [101] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## [106] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6]\n## [111] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (2,2.8]  \n## [116] (2.8,3.6] (2.8,3.6] (3.6,4.4] (2,2.8]   (2,2.8]  \n## [121] (2.8,3.6] (2,2.8]   (2,2.8]   (2,2.8]   (2.8,3.6]\n## [126] (2.8,3.6] (2,2.8]   (2.8,3.6] (2,2.8]   (2.8,3.6]\n## [131] (2,2.8]   (3.6,4.4] (2,2.8]   (2,2.8]   (2,2.8]  \n## [136] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## [141] (2.8,3.6] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6]\n## [146] (2.8,3.6] (2,2.8]   (2.8,3.6] (2.8,3.6] (2.8,3.6]\n## Levels: (2,2.8] (2.8,3.6] (3.6,4.4]\nlibrary(arules)## Loading required package: Matrix## \n## Attaching package: 'Matrix'## The following objects are masked from 'package:tidyr':\n## \n##     expand, pack, unpack## \n## Attaching package: 'arules'## The following object is masked from 'package:dplyr':\n## \n##     recode## The following objects are masked from 'package:base':\n## \n##     abbreviate, write\niris %>% pull(Petal.Width) %>% discretize(method = \"interval\", breaks = 3)##   [1] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##   [6] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [11] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [16] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [21] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [26] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [31] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [36] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [41] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [46] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9)\n##  [51] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [56] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [61] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [66] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [71] [1.7,2.5] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [76] [0.9,1.7) [0.9,1.7) [1.7,2.5] [0.9,1.7) [0.9,1.7)\n##  [81] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [86] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [91] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n##  [96] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7)\n## [101] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [106] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [111] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [116] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7)\n## [121] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [126] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7)\n## [131] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) [0.9,1.7)\n## [136] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [141] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## [146] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5]\n## attr(,\"discretized:breaks\")\n## [1] 0.1 0.9 1.7 2.5\n## attr(,\"discretized:method\")\n## [1] interval\n## Levels: [0.1,0.9) [0.9,1.7) [1.7,2.5]\niris %>% pull(Petal.Width) %>% discretize(method = \"frequency\", breaks = 3)##   [1] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##   [5] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##   [9] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [13] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [17] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [21] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [25] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [29] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [33] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [37] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [41] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [45] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867)\n##  [49] [0.1,0.867) [0.1,0.867) [0.867,1.6) [0.867,1.6)\n##  [53] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [57] [1.6,2.5]   [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [61] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [65] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [69] [0.867,1.6) [0.867,1.6) [1.6,2.5]   [0.867,1.6)\n##  [73] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [77] [0.867,1.6) [1.6,2.5]   [0.867,1.6) [0.867,1.6)\n##  [81] [0.867,1.6) [0.867,1.6) [0.867,1.6) [1.6,2.5]  \n##  [85] [0.867,1.6) [1.6,2.5]   [0.867,1.6) [0.867,1.6)\n##  [89] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [93] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n##  [97] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6)\n## [101] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [105] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [109] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [113] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [117] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [0.867,1.6)\n## [121] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [125] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [129] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [133] [1.6,2.5]   [0.867,1.6) [0.867,1.6) [1.6,2.5]  \n## [137] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [141] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [145] [1.6,2.5]   [1.6,2.5]   [1.6,2.5]   [1.6,2.5]  \n## [149] [1.6,2.5]   [1.6,2.5]  \n## attr(,\"discretized:breaks\")\n## [1] 0.100 0.867 1.600 2.500\n## attr(,\"discretized:method\")\n## [1] frequency\n## Levels: [0.1,0.867) [0.867,1.6) [1.6,2.5]\niris %>% pull(Petal.Width) %>% discretize(method = \"cluster\", breaks = 3)##   [1] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##   [4] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##   [7] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [10] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [13] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [16] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [19] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [22] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [25] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [28] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [31] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [34] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [37] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [40] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [43] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [46] [0.1,0.792)  [0.1,0.792)  [0.1,0.792) \n##  [49] [0.1,0.792)  [0.1,0.792)  [0.792,1.71)\n##  [52] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [55] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [58] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [61] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [64] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [67] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [70] [0.792,1.71) [1.71,2.5]   [0.792,1.71)\n##  [73] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [76] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [79] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [82] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [85] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [88] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [91] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [94] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n##  [97] [0.792,1.71) [0.792,1.71) [0.792,1.71)\n## [100] [0.792,1.71) [1.71,2.5]   [1.71,2.5]  \n## [103] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [106] [1.71,2.5]   [0.792,1.71) [1.71,2.5]  \n## [109] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [112] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [115] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [118] [1.71,2.5]   [1.71,2.5]   [0.792,1.71)\n## [121] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [124] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [127] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [130] [0.792,1.71) [1.71,2.5]   [1.71,2.5]  \n## [133] [1.71,2.5]   [0.792,1.71) [0.792,1.71)\n## [136] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [139] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [142] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [145] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## [148] [1.71,2.5]   [1.71,2.5]   [1.71,2.5]  \n## attr(,\"discretized:breaks\")\n## [1] 0.100 0.792 1.705 2.500\n## attr(,\"discretized:method\")\n## [1] cluster\n## Levels: [0.1,0.792) [0.792,1.71) [1.71,2.5]\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(xintercept =\n      iris %>% pull(Petal.Width) %>% discretize(method = \"interval\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\") +\n  labs(title = \"Discretization: interval\", subtitle = \"Blue lines are boundaries\")\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(xintercept =\n      iris %>% pull(Petal.Width) %>% discretize(method = \"frequency\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\") +\n  labs(title = \"Discretization: frequency\", subtitle = \"Blue lines are boundaries\")\nggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) +\n  geom_vline(xintercept =\n      iris %>% pull(Petal.Width) %>% discretize(method = \"cluster\", breaks = 3, onlycuts = TRUE),\n    color = \"blue\") +\n  labs(title = \"Discretization: cluster\", subtitle = \"Blue lines are boundaries\")"},{"path":"data.html","id":"standardize-data","chapter":"2 Data","heading":"2.5.4 Standardize Data","text":"Standardizing (scaling, normalizing) range features values make comparable.\npopular method convert values feature z-scores.\nsubtracting mean (centering) dividing \nstandard deviation (scaling).Note: tidyverse currently simple scale function,\nmake one provides wrapper standard scale function R:standardized feature mean zero measured standard deviations away zero.\n“normal” values fall range [-3,3] (standard deviations).","code":"\nscale_numeric <- function(x) x %>% mutate_if(is.numeric, function(y) as.vector(scale(y)))\n\niris.scaled <- iris %>% scale_numeric()\niris.scaled## # A tibble: 150 x 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##           <dbl>       <dbl>        <dbl>       <dbl>\n##  1       -0.898      1.02          -1.34       -1.31\n##  2       -1.14      -0.132         -1.34       -1.31\n##  3       -1.38       0.327         -1.39       -1.31\n##  4       -1.50       0.0979        -1.28       -1.31\n##  5       -1.02       1.25          -1.34       -1.31\n##  6       -0.535      1.93          -1.17       -1.05\n##  7       -1.50       0.786         -1.34       -1.18\n##  8       -1.02       0.786         -1.28       -1.31\n##  9       -1.74      -0.361         -1.34       -1.31\n## 10       -1.14       0.0979        -1.28       -1.44\n## # … with 140 more rows, and 1 more variable:\n## #   Species <fct>\nsummary(iris.scaled)##   Sepal.Length     Sepal.Width      Petal.Length   \n##  Min.   :-1.864   Min.   :-2.426   Min.   :-1.562  \n##  1st Qu.:-0.898   1st Qu.:-0.590   1st Qu.:-1.222  \n##  Median :-0.052   Median :-0.132   Median : 0.335  \n##  Mean   : 0.000   Mean   : 0.000   Mean   : 0.000  \n##  3rd Qu.: 0.672   3rd Qu.: 0.557   3rd Qu.: 0.760  \n##  Max.   : 2.484   Max.   : 3.080   Max.   : 1.780  \n##   Petal.Width           Species  \n##  Min.   :-1.442   setosa    :50  \n##  1st Qu.:-1.180   versicolor:50  \n##  Median : 0.132   virginica :50  \n##  Mean   : 0.000                  \n##  3rd Qu.: 0.788                  \n##  Max.   : 1.706"},{"path":"data.html","id":"proximities-similarities-and-distances","chapter":"2 Data","heading":"2.6 Proximities: Similarities and Distances","text":"R stores proximity dissimilarities/distances matrices. Similarities first converted dissimilarities.\nDistances symmetric, .e., distance\nB distance B . R therefore stores triangle (typically lower triangle) distance matrix.","code":""},{"path":"data.html","id":"minkowsky-distances","chapter":"2 Data","heading":"2.6.1 Minkowsky Distances","text":"Minkowsky distance family \nmetric distances including Euclidean Manhattan distance. avoid one feature dominate \ndistance calculation, scaled data typically used.Calculate distances matrices first 5 flowers.see lower triangle distance matrices stored (note rows start row 2).","code":"\niris_sample <- iris.scaled %>% select(-Species) %>% slice(1:5)\niris_sample## # A tibble: 5 x 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1       -0.898      1.02          -1.34       -1.31\n## 2       -1.14      -0.132         -1.34       -1.31\n## 3       -1.38       0.327         -1.39       -1.31\n## 4       -1.50       0.0979        -1.28       -1.31\n## 5       -1.02       1.25          -1.34       -1.31\ndist(iris_sample, method=\"euclidean\")##       1     2     3     4\n## 2 1.172                  \n## 3 0.843 0.522            \n## 4 1.100 0.433 0.283      \n## 5 0.259 1.382 0.988 1.246\ndist(iris_sample, method=\"manhattan\")##       1     2     3     4\n## 2 1.389                  \n## 3 1.228 0.757            \n## 4 1.578 0.648 0.463      \n## 5 0.350 1.497 1.337 1.687\ndist(iris_sample, method=\"maximum\")##       1     2     3     4\n## 2 1.147                  \n## 3 0.688 0.459            \n## 4 0.918 0.362 0.229      \n## 5 0.229 1.377 0.918 1.147"},{"path":"data.html","id":"distances-for-binary-data","chapter":"2 Data","heading":"2.6.2 Distances for Binary Data","text":"Binary data can encodes 0 1 (numeric) TRUE FALSE (logical).","code":"\nb <- rbind(\n  c(0,0,0,1,1,1,1,0,0,1),\n  c(0,0,1,1,1,0,0,1,0,0)\n  )\nb##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9]\n## [1,]    0    0    0    1    1    1    1    0    0\n## [2,]    0    0    1    1    1    0    0    1    0\n##      [,10]\n## [1,]     1\n## [2,]     0\nb_logical <- apply(b, MARGIN = 2, as.logical)\nb_logical##       [,1]  [,2]  [,3] [,4] [,5]  [,6]  [,7]  [,8]\n## [1,] FALSE FALSE FALSE TRUE TRUE  TRUE  TRUE FALSE\n## [2,] FALSE FALSE  TRUE TRUE TRUE FALSE FALSE  TRUE\n##       [,9] [,10]\n## [1,] FALSE  TRUE\n## [2,] FALSE FALSE"},{"path":"data.html","id":"hamming-distance","chapter":"2 Data","heading":"2.6.2.1 Hamming Distance","text":"Hamming distance number mis-matches\ntwo binary vectors. 0-1 data equivalent \nManhattan distance also squared Euclidean distance.","code":"\ndist(b, method = \"manhattan\")##   1\n## 2 5\ndist(b, method = \"euclidean\")^2##   1\n## 2 5"},{"path":"data.html","id":"jaccard-index","chapter":"2 Data","heading":"2.6.2.2 Jaccard Index","text":"Jaccard index similarity measure focuses matching 1s. R converts similarity dissimilarity using \\(d_{J} = 1 - s_{J}\\).","code":"\ndist(b, method = \"binary\")##       1\n## 2 0.714"},{"path":"data.html","id":"distances-for-mixed-data","chapter":"2 Data","heading":"2.6.3 Distances for Mixed Data","text":"distance measures work numeric data. Often mixture \nnumbers nominal features like data:Note: Nominal features need factor character (<chr>).","code":"\npeople <- tibble(\n  height = c(      160,    185,    170),\n  weight = c(       52,     90,     75),\n  sex    = c( \"female\", \"male\", \"male\")\n)\npeople## # A tibble: 3 x 3\n##   height weight sex   \n##    <dbl>  <dbl> <chr> \n## 1    160     52 female\n## 2    185     90 male  \n## 3    170     75 male\npeople <- people %>% mutate_if(is.character, factor)\npeople## # A tibble: 3 x 3\n##   height weight sex   \n##    <dbl>  <dbl> <fct> \n## 1    160     52 female\n## 2    185     90 male  \n## 3    170     75 male"},{"path":"data.html","id":"gowers-coefficient","chapter":"2 Data","heading":"2.6.3.1 Gower’s Coefficient","text":"Gower’s coefficient similarity works mixed data calculating appropriate similarity\nfeature \naggregating single measure. package proxy implements\nGower’s coefficient converted distance.Gower’s coefficient calculation implicitly scales data calculates distances feature individualy, need scale\ndata first.","code":"\nlibrary(proxy)## \n## Attaching package: 'proxy'## The following object is masked from 'package:Matrix':\n## \n##     as.matrix## The following objects are masked from 'package:stats':\n## \n##     as.dist, dist## The following object is masked from 'package:base':\n## \n##     as.matrix\nd_Gower <- dist(people, method = \"Gower\")\nd_Gower##       1     2\n## 2 1.000      \n## 3 0.668 0.332"},{"path":"data.html","id":"using-euclidean-distance-with-mixed-data","chapter":"2 Data","heading":"2.6.3.2 Using Euclidean Distance with Mixed Data","text":"Sometimes methods (e.g., k-means) can use Euclidean distance. \ncase, nominal features can converted 0-1 dummy variables. scaling, Euclidean\ndistance result usable distance measure.use package caret create dummy variables.Note feature sex now two columns. want \nheight, weight sex influence distance measure, need\nweight sex columns 1/2 scaling.distance using dummy variables (mostly) consistent Gower’s distance (\nGower’s distance scaled 0 1).","code":"\nlibrary(caret)## Loading required package: lattice## \n## Attaching package: 'caret'## The following object is masked from 'package:sampling':\n## \n##     cluster## The following object is masked from 'package:purrr':\n## \n##     lift\ndata_dummy <- dummyVars(~., people) %>% predict(people)\ndata_dummy##   height weight sex.female sex.male\n## 1    160     52          1        0\n## 2    185     90          0        1\n## 3    170     75          0        1\nweight <- matrix(c(1, 1, 1/2, 1/2), ncol = 4, nrow = nrow(data_dummy), byrow = TRUE)\ndata_dummy_scaled <- scale(data_dummy) * weight\n\nd_dummy <- dist(data_dummy_scaled)\nd_dummy##      1    2\n## 2 3.06     \n## 3 1.89 1.43\nggplot(tibble(d_dummy, d_Gower), aes(x = d_dummy, y = d_Gower)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE)## Don't know how to automatically pick scale for object of type dist. Defaulting to continuous.\n## Don't know how to automatically pick scale for object of type dist. Defaulting to continuous.## `geom_smooth()` using formula 'y ~ x'"},{"path":"data.html","id":"additional-proximity-measures-available-in-package-proxy","chapter":"2 Data","heading":"2.6.4 Additional proximity Measures Available in Package proxy","text":"package proxy implements wide array distances.Note loading package proxy replaces dist function R.\ncan specify dist function use specifying package call. \nexample stats::dist() calls default function R (package stats part R) proxy::dist() calls \nversion package proxy.","code":"\nlibrary(proxy)\npr_DB$get_entry_names()##  [1] \"Jaccard\"         \"Kulczynski1\"    \n##  [3] \"Kulczynski2\"     \"Mountford\"      \n##  [5] \"Fager\"           \"Russel\"         \n##  [7] \"simple matching\" \"Hamman\"         \n##  [9] \"Faith\"           \"Tanimoto\"       \n## [11] \"Dice\"            \"Phi\"            \n## [13] \"Stiles\"          \"Michael\"        \n## [15] \"Mozley\"          \"Yule\"           \n## [17] \"Yule2\"           \"Ochiai\"         \n## [19] \"Simpson\"         \"Braun-Blanquet\" \n## [21] \"cosine\"          \"eJaccard\"       \n## [23] \"eDice\"           \"correlation\"    \n## [25] \"Chi-squared\"     \"Phi-squared\"    \n## [27] \"Tschuprow\"       \"Cramer\"         \n## [29] \"Pearson\"         \"Gower\"          \n## [31] \"Euclidean\"       \"Mahalanobis\"    \n## [33] \"Bhjattacharyya\"  \"Manhattan\"      \n## [35] \"supremum\"        \"Minkowski\"      \n## [37] \"Canberra\"        \"Wave\"           \n## [39] \"divergence\"      \"Kullback\"       \n## [41] \"Bray\"            \"Soergel\"        \n## [43] \"Levenshtein\"     \"Podani\"         \n## [45] \"Chord\"           \"Geodesic\"       \n## [47] \"Whittaker\"       \"Hellinger\"      \n## [49] \"fJaccard\""},{"path":"data.html","id":"relationships-between-features","chapter":"2 Data","heading":"2.7 Relationships Between Features","text":"","code":""},{"path":"data.html","id":"correlation","chapter":"2 Data","heading":"2.7.1 Correlation","text":"Correlation can used \nratio/interval scaled features.\ntypically think Pearson correlation coefficient features (columns).cor calculates correlation matrix pairwise correlations features.\ncorrelation Petal.Length Petal.Width can visualized using scatter plot.geom_smooth adds regression line fitting linear model (lm). points close line indicating strong linear dependence (.e., correlation).can calculate individual correlations specifying two vectors.Note: lets use columns using just names \n(iris, cor(Petal.Length, Petal.Width))\ncor(iris$Petal.Length, iris$Petal.Width).Finally, can test correlation significantly different zero.small p-value (less 0.05) indicates observed correlation significantly different zero. can also seen fact 95% confidence interval span zero.Sepal.Length Sepal.Width show little correlation:{ r} ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() +   geom_smooth(method = \"lm\") (iris, cor(Sepal.Length, Sepal.Width)) (iris, cor.test(Sepal.Length, Sepal.Width))","code":"\ncc <- iris %>% select(-Species) %>% cor()\ncc##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.118        0.872\n## Sepal.Width        -0.118       1.000       -0.428\n## Petal.Length        0.872      -0.428        1.000\n## Petal.Width         0.818      -0.366        0.963\n##              Petal.Width\n## Sepal.Length       0.818\n## Sepal.Width       -0.366\n## Petal.Length       0.963\n## Petal.Width        1.000\nggplot(iris, aes(Petal.Length, Petal.Width)) + \n  geom_point() +\n  geom_smooth(method = \"lm\")## `geom_smooth()` using formula 'y ~ x'\nwith(iris, cor(Petal.Length, Petal.Width))## [1] 0.963\nwith(iris, cor.test(Petal.Length, Petal.Width))## \n##  Pearson's product-moment correlation\n## \n## data:  Petal.Length and Petal.Width\n## t = 43, df = 148, p-value <2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.949 0.973\n## sample estimates:\n##   cor \n## 0.963"},{"path":"data.html","id":"rank-correlation","chapter":"2 Data","heading":"2.7.2 Rank Correlation","text":"Rank correlation used ordinal features.\nshow , first convert continuous features Iris dataset \nordered factors (ordinal) three levels using function cut.Two measures rank correlation Kendall’s Tau Spearman’s Rho.Kendall’s Tau Rank Correlation Coefficient measures \nagreement two rankings (.e., ordinal features).Note: use xtfrm transform ordered factors\nranks, .e., numbers representing order.Spearman’s Rho\nequal Pearson correlation rank values two features.Spearman’s Rho much faster compute large datasets Kendall’s Tau.Comparing rank correlation results Pearson correlation original data shows\nsimilar. indicates discretizing data result loss \nmuch information.","code":"\niris_ord <- iris %>% mutate_if(is.numeric,\n  function(x) cut(x, 3, labels = c(\"short\", \"medium\", \"long\"), ordered = TRUE))\n\niris_ord## # A tibble: 150 x 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##    <ord>        <ord>       <ord>        <ord>      \n##  1 short        medium      short        short      \n##  2 short        medium      short        short      \n##  3 short        medium      short        short      \n##  4 short        medium      short        short      \n##  5 short        medium      short        short      \n##  6 short        long        short        short      \n##  7 short        medium      short        short      \n##  8 short        medium      short        short      \n##  9 short        medium      short        short      \n## 10 short        medium      short        short      \n## # … with 140 more rows, and 1 more variable:\n## #   Species <fct>\nsummary(iris_ord)##  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##  short :59    short :47   short :50    short :50  \n##  medium:71    medium:88   medium:54    medium:54  \n##  long  :20    long  :15   long  :46    long  :46  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50\niris_ord %>% pull(Sepal.Length)##   [1] short  short  short  short  short  short  short \n##   [8] short  short  short  short  short  short  short \n##  [15] medium medium short  short  medium short  short \n##  [22] short  short  short  short  short  short  short \n##  [29] short  short  short  short  short  short  short \n##  [36] short  short  short  short  short  short  short \n##  [43] short  short  short  short  short  short  short \n##  [50] short  long   medium long   short  medium medium\n##  [57] medium short  medium short  short  medium medium\n##  [64] medium medium medium medium medium medium medium\n##  [71] medium medium medium medium medium medium long  \n##  [78] medium medium medium short  short  medium medium\n##  [85] short  medium medium medium medium short  short \n##  [92] medium medium short  medium medium medium medium\n##  [99] short  medium medium medium long   medium medium\n## [106] long   short  long   medium long   medium medium\n## [113] long   medium medium medium medium long   long  \n## [120] medium long   medium long   medium medium long  \n## [127] medium medium medium long   long   long   medium\n## [134] medium medium long   medium medium medium long  \n## [141] medium long   medium long   medium medium medium\n## [148] medium medium medium\n## Levels: short < medium < long\niris_ord %>% select(-Species) %>% sapply(xtfrm) %>% cor(method = \"kendall\")##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.144        0.742\n## Sepal.Width        -0.144       1.000       -0.330\n## Petal.Length        0.742      -0.330        1.000\n## Petal.Width         0.730      -0.315        0.920\n##              Petal.Width\n## Sepal.Length       0.730\n## Sepal.Width       -0.315\n## Petal.Length       0.920\n## Petal.Width        1.000\niris_ord %>% select(-Species) %>% sapply(xtfrm) %>% cor(method = \"spearman\")##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.157        0.794\n## Sepal.Width        -0.157       1.000       -0.366\n## Petal.Length        0.794      -0.366        1.000\n## Petal.Width         0.784      -0.352        0.940\n##              Petal.Width\n## Sepal.Length       0.784\n## Sepal.Width       -0.352\n## Petal.Length       0.940\n## Petal.Width        1.000\niris %>% select(-Species) %>% cor()##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.118        0.872\n## Sepal.Width        -0.118       1.000       -0.428\n## Petal.Length        0.872      -0.428        1.000\n## Petal.Width         0.818      -0.366        0.963\n##              Petal.Width\n## Sepal.Length       0.818\n## Sepal.Width       -0.366\n## Petal.Length       0.963\n## Petal.Width        1.000"},{"path":"data.html","id":"density-estimation","chapter":"2 Data","heading":"2.8 Density Estimation","text":"Density estimation constructions estimate unobservable probability density function (distribution) based observed data.Just plotting data helpful single feature.","code":"\nggplot(iris, aes(x = Petal.Length, y = 0)) + geom_point()"},{"path":"data.html","id":"histograms","chapter":"2 Data","heading":"2.8.1 Histograms","text":"histograms shows distribution counting many values fall within bucket visualizing counts bar chart.\nuse geom_rug place marks\noriginal data points bottom histogram.Two-dimensional distributions can visualized using\n2-d binning hexagonal bins.","code":"\nggplot(iris, aes(x = Petal.Length)) +\n  geom_histogram() +\n  geom_rug(alpha = 1/2)## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_bin2d(bins = 10) +\n  geom_jitter(color = \"red\")\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_hex(bins = 10) +\n  geom_jitter(color = \"red\")"},{"path":"data.html","id":"kernel-density-estimate-kde","chapter":"2 Data","heading":"2.8.2 Kernel Density Estimate (KDE)","text":"Kernel density estimation\nused estimate probability density function feature. works replacing \nvalue kernel function (often Gaussian) adding . result \nestimated probability density function looks like smoothed version histogram.\nbandwidth (bw) kernel controls amount smoothing.","code":"\nggplot(iris, aes(Petal.Length)) +\n  geom_density(bw = .2) +\n  geom_rug(alpha = 1/2)\nggplot(iris, aes(Sepal.Length, Sepal.Width)) +\n  geom_density2d() +\n  geom_jitter()"},{"path":"data.html","id":"exploring-data","chapter":"2 Data","heading":"2.9 Exploring Data","text":"","code":""},{"path":"data.html","id":"basic-statistics","chapter":"2 Data","heading":"2.9.1 Basic statistics","text":"Get summary statistics (using base R)Get mean standard deviation sepal lengthIgnore missing values (Note: data contain , \n)Robust mean (trim 10% observations end distribution)Calculate summary numeric columnsMAD (median absolute deviation)","code":"\nsummary(iris)##   Sepal.Length   Sepal.Width    Petal.Length \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60  \n##  Median :5.80   Median :3.00   Median :4.35  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90  \n##   Petal.Width        Species  \n##  Min.   :0.1   setosa    :50  \n##  1st Qu.:0.3   versicolor:50  \n##  Median :1.3   virginica :50  \n##  Mean   :1.2                  \n##  3rd Qu.:1.8                  \n##  Max.   :2.5\niris %>% pull(Sepal.Length) %>% mean()## [1] 5.84\niris %>% pull(Sepal.Length) %>% sd()## [1] 0.828\niris %>% pull(Sepal.Length) %>% mean(na.rm = TRUE)## [1] 5.84\niris %>% pull(Sepal.Length) %>% mean(trim = .1)## [1] 5.81\niris %>% summarize_if(is.numeric, mean)## # A tibble: 1 x 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         5.84        3.06         3.76        1.20\niris %>% summarize_if(is.numeric, sd)## # A tibble: 1 x 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1        0.828       0.436         1.77       0.762\niris %>% summarize_if(is.numeric, list(min = min, median = median, max = max))## # A tibble: 1 x 12\n##   Sepal.Length_min Sepal.Width_min Petal.Length_min\n##              <dbl>           <dbl>            <dbl>\n## 1              4.3               2                1\n## # … with 9 more variables: Petal.Width_min <dbl>,\n## #   Sepal.Length_median <dbl>,\n## #   Sepal.Width_median <dbl>,\n## #   Petal.Length_median <dbl>,\n## #   Petal.Width_median <dbl>, Sepal.Length_max <dbl>,\n## #   Sepal.Width_max <dbl>, Petal.Length_max <dbl>,\n## #   Petal.Width_max <dbl>\niris %>% summarize_if(is.numeric, mad)## # A tibble: 1 x 4\n##   Sepal.Length Sepal.Width Petal.Length Petal.Width\n##          <dbl>       <dbl>        <dbl>       <dbl>\n## 1         1.04       0.445         1.85        1.04"},{"path":"data.html","id":"tabulate-data","chapter":"2 Data","heading":"2.9.2 Tabulate data","text":"Count number flowers species.base R, can also done using count(iris$Species).discretize data using cut.Cross tabulation used find two discrete features related.table contains number rows contain combination values (e.g.,\nnumber flowers short Sepal.Length species setosa 47).\ncells large counts others low counts, might \nrelationship. iris data, see species setosa mostly short Sepal.Length,\nversicolor virginica longer sepals.Creating cross table tidyverse little involved uses pivot\noperations grouping.can use statistical test \ndetermine significant relationship two features.\nPearson’s chi-squared test independence performed null hypothesis joint distribution cell counts 2-dimensional contingency table product row column marginals. null hypothesis h0 independence rows columns.small p-value indicates null hypothesis independence needs rejected.\nsmall counts (cells counts <5), Fisher’s exact test better.can use nominal feature form groups calculate group-wise\nstatistics continuous features. often use group-wise averages see\ndiffer groups.see species viginica highest average , Sepal.Width.","code":"\niris %>% group_by(Species) %>% summarize(n())## # A tibble: 3 x 2\n##   Species    `n()`\n##   <fct>      <int>\n## 1 setosa        50\n## 2 versicolor    50\n## 3 virginica     50\niris_ord <- iris %>% mutate_if(is.numeric,\n  function(x) cut(x, 3, labels = c(\"short\", \"medium\", \"long\"), ordered = TRUE))\n\niris_ord## # A tibble: 150 x 5\n##    Sepal.Length Sepal.Width Petal.Length Petal.Width\n##    <ord>        <ord>       <ord>        <ord>      \n##  1 short        medium      short        short      \n##  2 short        medium      short        short      \n##  3 short        medium      short        short      \n##  4 short        medium      short        short      \n##  5 short        medium      short        short      \n##  6 short        long        short        short      \n##  7 short        medium      short        short      \n##  8 short        medium      short        short      \n##  9 short        medium      short        short      \n## 10 short        medium      short        short      \n## # … with 140 more rows, and 1 more variable:\n## #   Species <fct>\nsummary(iris_ord)##  Sepal.Length Sepal.Width Petal.Length Petal.Width\n##  short :59    short :47   short :50    short :50  \n##  medium:71    medium:88   medium:54    medium:54  \n##  long  :20    long  :15   long  :46    long  :46  \n##        Species  \n##  setosa    :50  \n##  versicolor:50  \n##  virginica :50\ntbl <- iris_ord %>% select(Sepal.Length, Species) %>% table()\ntbl##             Species\n## Sepal.Length setosa versicolor virginica\n##       short      47         11         1\n##       medium      3         36        32\n##       long        0          3        17\niris_ord %>%\n  select(Species, Sepal.Length) %>%\n### Relationship Between Nominal and Ordinal Features\n  pivot_longer(cols = Sepal.Length) %>%\n  group_by(Species, value) %>% count() %>% ungroup() %>%\n  pivot_wider(names_from = Species, values_from = n)## # A tibble: 3 x 4\n##   value  setosa versicolor virginica\n##   <ord>   <int>      <int>     <int>\n## 1 short      47         11         1\n## 2 medium      3         36        32\n## 3 long       NA          3        17\ntbl %>% chisq.test()## \n##  Pearson's Chi-squared test\n## \n## data:  .\n## X-squared = 112, df = 4, p-value <2e-16\nfisher.test(tbl)## \n##  Fisher's Exact Test for Count Data\n## \n## data:  tbl\n## p-value <2e-16\n## alternative hypothesis: two.sided\niris %>% group_by(Species) %>% summarize(across(Sepal.Length, mean))## # A tibble: 3 x 2\n##   Species    Sepal.Length\n##   <fct>             <dbl>\n## 1 setosa             5.01\n## 2 versicolor         5.94\n## 3 virginica          6.59\niris %>% group_by(Species) %>% summarize_all(mean)## # A tibble: 3 x 5\n##   Species    Sepal.Length Sepal.Width Petal.Length\n##   <fct>             <dbl>       <dbl>        <dbl>\n## 1 setosa             5.01        3.43         1.46\n## 2 versicolor         5.94        2.77         4.26\n## 3 virginica          6.59        2.97         5.55\n## # … with 1 more variable: Petal.Width <dbl>"},{"path":"data.html","id":"percentiles-quantiles","chapter":"2 Data","heading":"2.9.3 Percentiles (Quantiles)","text":"Quantiles\ncutting points dividing range probability distribution \ncontinuous intervals equal probability.\nexample, median empirical 50% quantile\ndividing observations 50% observations smaller median 50% larger median.default quartiles calculated. 25% typically called Q1,\n50% called Q2 median 75% called Q3.interquartile range measure variability robust outliers.\ndefined length Q3 - Q2 covers 50% data middle.","code":"\niris %>% pull(Petal.Length) %>% quantile()##   0%  25%  50%  75% 100% \n## 1.00 1.60 4.35 5.10 6.90\niris %>% summarize(IQR = quantile(Petal.Length, probs = 0.75) - quantile(Petal.Length, probs = 0.25))## # A tibble: 1 x 1\n##     IQR\n##   <dbl>\n## 1   3.5"},{"path":"data.html","id":"visualization","chapter":"2 Data","heading":"2.10 Visualization","text":"","code":""},{"path":"data.html","id":"histogram","chapter":"2 Data","heading":"2.10.1 Histogram","text":"Histograms show distribution single continuous feature.","code":"\nggplot(iris, aes(Petal.Width)) + geom_histogram(bins = 20)"},{"path":"data.html","id":"boxplot","chapter":"2 Data","heading":"2.10.2 Boxplot","text":"Boxplots used compare distribution feature different groups. horizontal line middle boxes group-wise medians,\nboxes span interquartile range. whiskers (vertical lines) span typically 1.4 times \ninterquartile range. Points fall outside range typically outliers shown dots.compare distribution four features using ggplot boxplot,\nfirst transform data long format (.e., feature values combined single column).","code":"\nggplot(iris, aes(Species, Sepal.Length)) + \n  geom_boxplot()\niris %>% group_by(Species) %>% summarize_if(is.numeric, median)## # A tibble: 3 x 5\n##   Species    Sepal.Length Sepal.Width Petal.Length\n##   <fct>             <dbl>       <dbl>        <dbl>\n## 1 setosa              5           3.4         1.5 \n## 2 versicolor          5.9         2.8         4.35\n## 3 virginica           6.5         3           5.55\n## # … with 1 more variable: Petal.Width <dbl>\nlibrary(tidyr)\niris_long <- iris %>% mutate(id = row_number()) %>% pivot_longer(1:4)\nggplot(iris_long, aes(name, value)) + \n  geom_boxplot()"},{"path":"data.html","id":"scatter-plot","chapter":"2 Data","heading":"2.10.3 Scatter plot","text":"Scatter plots show relationship two continuous features.","code":"\nggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) + \n  geom_point()"},{"path":"data.html","id":"scatter-plot-matrix","chapter":"2 Data","heading":"2.10.4 Scatter Plot Matrix","text":"scatter plot matrix show relationship several features\nimplementation package GGally also shows additional plots (histograms, density estimates box plots) correlation coefficients.","code":"\nlibrary(\"GGally\")\nggpairs(iris,  aes(color = Species))## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\n## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`."},{"path":"data.html","id":"data-matrix-visualization","chapter":"2 Data","heading":"2.10.5 Data Matrix Visualization","text":"Matrix visualization shows values matrix using color scale.need long format tidyverse.Smaller values darker. Package seriation provides simpler plotting function.can scale features z-scores make better comparable.reveals red blue blocks. row flower flowers Iris dataset \nsorted species. blue blocks top 50 flowers shiw flowers \nsmaller average Speal.Width red blocks show bottom 50 flowers\nlarger features.Often, reordering data matrices help visualization. reordering technique called seriation.\nIr reorders rows columns place similar points closer together.","code":"\niris_matrix <- iris %>% select(-Species) %>% as.matrix()\niris_long <- as_tibble(iris_matrix) %>% mutate(id = row_number()) %>% pivot_longer(1:4)\nhead(iris_long)## # A tibble: 6 x 3\n##      id name         value\n##   <int> <chr>        <dbl>\n## 1     1 Sepal.Length   5.1\n## 2     1 Sepal.Width    3.5\n## 3     1 Petal.Length   1.4\n## 4     1 Petal.Width    0.2\n## 5     2 Sepal.Length   4.9\n## 6     2 Sepal.Width    3\nggplot(iris_long,\n  aes(x = name, y = id, fill = value)) + geom_tile()\nlibrary(seriation)## Registered S3 methods overwritten by 'registry':\n##   method               from \n##   print.registry_field proxy\n##   print.registry_entry proxy## \n## Attaching package: 'seriation'## The following object is masked from 'package:lattice':\n## \n##     panel.lines\nggpimage(iris_matrix, prop = FALSE)\niris_scaled <- scale(iris_matrix)\nggpimage(iris_scaled, prop = FALSE)\nggpimage(iris_scaled, order = seriate(iris_scaled), prop = FALSE)"},{"path":"data.html","id":"correlation-matrix","chapter":"2 Data","heading":"2.10.6 Correlation Matrix","text":"correlation matrix contains correlation features.Package ggcorrplot provides visualization correlation matrices.Package seriation provides reordered version plot using heatmap.Correlations can also calculates objects trnsposing data matrix.","code":"\ncm1 <- iris %>% select(-Species) %>% as.matrix %>% cor()\ncm1##              Sepal.Length Sepal.Width Petal.Length\n## Sepal.Length        1.000      -0.118        0.872\n## Sepal.Width        -0.118       1.000       -0.428\n## Petal.Length        0.872      -0.428        1.000\n## Petal.Width         0.818      -0.366        0.963\n##              Petal.Width\n## Sepal.Length       0.818\n## Sepal.Width       -0.366\n## Petal.Length       0.963\n## Petal.Width        1.000\nlibrary(ggcorrplot)\nggcorrplot(cm1)\ngghmap(cm1)\ncm2 <- iris %>% select(-Species) %>% as.matrix() %>% t() %>% cor()\n\nggcorrplot(cm2)"},{"path":"data.html","id":"parallel-coordinates-plot","chapter":"2 Data","heading":"2.10.7 Parallel Coordinates Plot","text":"Parallel coordinate plots can visualize several featues single plot. Lines connect values\nobject (flower).plot can improved reordering variables place correlated features next .","code":"\nlibrary(GGally)\nggparcoord(as_tibble(iris), columns = 1:4, groupColumn = 5)\no <- seriate(as.dist(1-cor(iris[,1:4])), method = \"BBURCG\")\nget_order(o)## Petal.Length  Petal.Width Sepal.Length  Sepal.Width \n##            3            4            1            2\nggparcoord(as_tibble(iris), columns = get_order(o), groupColumn = 5)"},{"path":"data.html","id":"more-visulizations","chapter":"2 Data","heading":"2.10.8 More Visulizations","text":"well organized collection visualizations code can found \nR Graph Gallery.","code":""},{"path":"classification-basic-concepts-and-techniques.html","id":"classification-basic-concepts-and-techniques","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3 Classification: Basic Concepts and Techniques","text":"Packages used chapter: caret (Kuhn 2021), FSelector (Romanski, Kotthoff, Schratz 2021), lattice (Sarkar 2021), mlbench (Leisch Dimitriadou. 2021), pROC (Robin et al. 2021), rpart (Therneau Atkinson 2019), rpart.plot (Milborrow 2020), sampling (Tillé Matei 2021), tidyverse (Wickham 2021c)can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 3. Classification: Basic Concepts Techniques","code":""},{"path":"classification-basic-concepts-and-techniques.html","id":"the-zoo-dataset","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.1 The Zoo Dataset","text":"use Zoo dataset included R package mlbench (may install ).\nZoo dataset containing 17 (mostly logical) variables different 101 animals \ndata frame 17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize, type). convert data frame tidyverse tibble (optional).Note: data.frames R can row names. Zoo data set uses animal name row names. tibbles tidyverse support row names. keep animal name can add column animal name.remove animal column learning model! following use data.frame.translate TRUE/FALSE values factors (nominal). often needed building models. Always check summary() make sure data ready model learning.","code":"\ndata(Zoo, package=\"mlbench\")\nhead(Zoo)##           hair feathers  eggs  milk airborne aquatic\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE\n## bear      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## boar      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## buffalo   TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n##          predator toothed backbone breathes venomous\n## aardvark     TRUE    TRUE     TRUE     TRUE    FALSE\n## antelope    FALSE    TRUE     TRUE     TRUE    FALSE\n## bass         TRUE    TRUE     TRUE    FALSE    FALSE\n## bear         TRUE    TRUE     TRUE     TRUE    FALSE\n## boar         TRUE    TRUE     TRUE     TRUE    FALSE\n## buffalo     FALSE    TRUE     TRUE     TRUE    FALSE\n##           fins legs  tail domestic catsize   type\n## aardvark FALSE    4 FALSE    FALSE    TRUE mammal\n## antelope FALSE    4  TRUE    FALSE    TRUE mammal\n## bass      TRUE    0  TRUE    FALSE   FALSE   fish\n## bear     FALSE    4 FALSE    FALSE    TRUE mammal\n## boar     FALSE    4  TRUE    FALSE    TRUE mammal\n## buffalo  FALSE    4  TRUE    FALSE    TRUE mammal\nlibrary(tidyverse)\nas_tibble(Zoo, rownames = \"animal\")## # A tibble: 101 x 18\n##    animal   hair  feathers eggs  milk  airborne aquatic\n##    <chr>    <lgl> <lgl>    <lgl> <lgl> <lgl>    <lgl>  \n##  1 aardvark TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  2 antelope TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  3 bass     FALSE FALSE    TRUE  FALSE FALSE    TRUE   \n##  4 bear     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  5 boar     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  6 buffalo  TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  7 calf     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n##  8 carp     FALSE FALSE    TRUE  FALSE FALSE    TRUE   \n##  9 catfish  FALSE FALSE    TRUE  FALSE FALSE    TRUE   \n## 10 cavy     TRUE  FALSE    FALSE TRUE  FALSE    FALSE  \n## # … with 91 more rows, and 11 more variables:\n## #   predator <lgl>, toothed <lgl>, backbone <lgl>,\n## #   breathes <lgl>, venomous <lgl>, fins <lgl>,\n## #   legs <int>, tail <lgl>, domestic <lgl>,\n## #   catsize <lgl>, type <fct>\nZoo <- Zoo %>%\n  modify_if(is.logical, factor, levels = c(TRUE, FALSE)) %>%\n  modify_if(is.character, factor)\nZoo %>% summary()##     hair     feathers     eggs       milk   \n##  TRUE :43   TRUE :20   TRUE :59   TRUE :41  \n##  FALSE:58   FALSE:81   FALSE:42   FALSE:60  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##   airborne   aquatic    predator   toothed  \n##  TRUE :24   TRUE :36   TRUE :56   TRUE :61  \n##  FALSE:77   FALSE:65   FALSE:45   FALSE:40  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##   backbone   breathes   venomous     fins   \n##  TRUE :83   TRUE :80   TRUE : 8   TRUE :17  \n##  FALSE:18   FALSE:21   FALSE:93   FALSE:84  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##       legs         tail     domestic   catsize  \n##  Min.   :0.00   TRUE :75   TRUE :13   TRUE :44  \n##  1st Qu.:2.00   FALSE:26   FALSE:88   FALSE:57  \n##  Median :4.00                                   \n##  Mean   :2.84                                   \n##  3rd Qu.:4.00                                   \n##  Max.   :8.00                                   \n##                                                 \n##             type   \n##  mammal       :41  \n##  bird         :20  \n##  reptile      : 5  \n##  fish         :13  \n##  amphibian    : 4  \n##  insect       : 8  \n##  mollusc.et.al:10"},{"path":"classification-basic-concepts-and-techniques.html","id":"decision-trees","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.2 Decision Trees","text":"Recursive Partitioning (similar CART) uses Gini index make\nsplitting decisions early stopping (pre-pruning).","code":"\nlibrary(rpart)"},{"path":"classification-basic-concepts-and-techniques.html","id":"create-tree-with-default-settings-uses-pre-pruning","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.2.1 Create Tree With Default Settings (uses pre-pruning)","text":"Notes:\n- %>% supplies data rpart. Since data first argument rpart, syntax data = . used specify data Zoo goes. call equivalent tree_default <- rpart(type ~ ., data = Zoo).\n- formula models type variable features represented .. data = .\nmeans data provided pipe (%>%) passed rpart \nargument data.class variable needs factor (nominal) rpart\ncreate regression tree instead decision tree. Use .factor()\nnecessary.PlottingNote: extra=2 prints leaf node number correctly\nclassified objects data total number objects\ntraining data falling node (correct/total).","code":"\ntree_default <- Zoo %>% rpart(type ~ ., data = .)\ntree_default## n= 101 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##  1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099)  \n##    2) milk=TRUE 41  0 mammal (1 0 0 0 0 0 0) *\n##    3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17)  \n##      6) feathers=TRUE 20  0 bird (0 1 0 0 0 0 0) *\n##      7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25)  \n##       14) fins=TRUE 13  0 fish (0 0 0 1 0 0 0) *\n##       15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37)  \n##         30) backbone=TRUE 9  4 reptile (0 0 0.56 0 0.44 0 0) *\n##         31) backbone=FALSE 18  8 mollusc.et.al (0 0 0 0 0 0.44 0.56) *\nlibrary(rpart.plot)\nrpart.plot(tree_default, extra = 2)## Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables).\n## To silence this warning:\n##     Call rpart.plot with roundint=FALSE,\n##     or rebuild the rpart model with model=TRUE."},{"path":"classification-basic-concepts-and-techniques.html","id":"create-a-full-tree","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.2.2 Create a Full Tree","text":"create full tree, set complexity parameter cp 0 (split even\nimprove tree) set minimum number \nobservations node needed split smallest value 2\n(see: ?rpart.control).\nNote: full trees overfit training data!Training error tree pre-pruningUse function accuracyTraining error full treeGet confusion table statistics (using caret)","code":"\ntree_full <- Zoo %>% rpart(type ~., data = ., control = rpart.control(minsplit = 2, cp = 0))\nrpart.plot(tree_full, extra = 2, roundint=FALSE,\n  box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \"Or\", \"Rd\", \"Pu\")) # specify 7 colors\ntree_full## n= 101 \n## \n## node), split, n, loss, yval, (yprob)\n##       * denotes terminal node\n## \n##   1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099)  \n##     2) milk=TRUE 41  0 mammal (1 0 0 0 0 0 0) *\n##     3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17)  \n##       6) feathers=TRUE 20  0 bird (0 1 0 0 0 0 0) *\n##       7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25)  \n##        14) fins=TRUE 13  0 fish (0 0 0 1 0 0 0) *\n##        15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37)  \n##          30) backbone=TRUE 9  4 reptile (0 0 0.56 0 0.44 0 0)  \n##            60) aquatic=FALSE 4  0 reptile (0 0 1 0 0 0 0) *\n##            61) aquatic=TRUE 5  1 amphibian (0 0 0.2 0 0.8 0 0)  \n##             122) eggs=FALSE 1  0 reptile (0 0 1 0 0 0 0) *\n##             123) eggs=TRUE 4  0 amphibian (0 0 0 0 1 0 0) *\n##          31) backbone=FALSE 18  8 mollusc.et.al (0 0 0 0 0 0.44 0.56)  \n##            62) airborne=TRUE 6  0 insect (0 0 0 0 0 1 0) *\n##            63) airborne=FALSE 12  2 mollusc.et.al (0 0 0 0 0 0.17 0.83)  \n##             126) predator=FALSE 4  2 insect (0 0 0 0 0 0.5 0.5)  \n##               252) legs>=3 2  0 insect (0 0 0 0 0 1 0) *\n##               253) legs< 3 2  0 mollusc.et.al (0 0 0 0 0 0 1) *\n##             127) predator=TRUE 8  0 mollusc.et.al (0 0 0 0 0 0 1) *\npredict(tree_default, Zoo) %>% head ()##          mammal bird reptile fish amphibian insect\n## aardvark      1    0       0    0         0      0\n## antelope      1    0       0    0         0      0\n## bass          0    0       0    1         0      0\n## bear          1    0       0    0         0      0\n## boar          1    0       0    0         0      0\n## buffalo       1    0       0    0         0      0\n##          mollusc.et.al\n## aardvark             0\n## antelope             0\n## bass                 0\n## bear                 0\n## boar                 0\n## buffalo              0\npred <- predict(tree_default, Zoo, type=\"class\")\nhead(pred)## aardvark antelope     bass     bear     boar  buffalo \n##   mammal   mammal     fish   mammal   mammal   mammal \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al\nconfusion_table <- with(Zoo, table(type, pred))\nconfusion_table##                pred\n## type            mammal bird reptile fish amphibian\n##   mammal            41    0       0    0         0\n##   bird               0   20       0    0         0\n##   reptile            0    0       5    0         0\n##   fish               0    0       0   13         0\n##   amphibian          0    0       4    0         0\n##   insect             0    0       0    0         0\n##   mollusc.et.al      0    0       0    0         0\n##                pred\n## type            insect mollusc.et.al\n##   mammal             0             0\n##   bird               0             0\n##   reptile            0             0\n##   fish               0             0\n##   amphibian          0             0\n##   insect             0             8\n##   mollusc.et.al      0            10\ncorrect <- confusion_table %>% diag() %>% sum()\ncorrect## [1] 89\nerror <- confusion_table %>% sum() - correct\nerror## [1] 12\naccuracy <- correct / (correct + error)\naccuracy## [1] 0.881\naccuracy <- function(truth, prediction) {\n    tbl <- table(truth, prediction)\n    sum(diag(tbl))/sum(tbl)\n}\n\naccuracy(Zoo %>% pull(type), pred)## [1] 0.881\naccuracy(Zoo %>% pull(type), predict(tree_full, Zoo, type=\"class\"))## [1] 1\nlibrary(caret)\nconfusionMatrix(data = pred, reference = Zoo %>% pull(type))## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian\n##   mammal            41    0       0    0         0\n##   bird               0   20       0    0         0\n##   reptile            0    0       5    0         4\n##   fish               0    0       0   13         0\n##   amphibian          0    0       0    0         0\n##   insect             0    0       0    0         0\n##   mollusc.et.al      0    0       0    0         0\n##                Reference\n## Prediction      insect mollusc.et.al\n##   mammal             0             0\n##   bird               0             0\n##   reptile            0             0\n##   fish               0             0\n##   amphibian          0             0\n##   insect             0             0\n##   mollusc.et.al      8            10\n## \n## Overall Statistics\n##                                         \n##                Accuracy : 0.881         \n##                  95% CI : (0.802, 0.937)\n##     No Information Rate : 0.406         \n##     P-Value [Acc > NIR] : <2e-16        \n##                                         \n##                   Kappa : 0.843         \n##                                         \n##  Mcnemar's Test P-Value : NA            \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       1.000\n## Pos Pred Value               1.000       1.000\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.406       0.198\n## Detection Rate               0.406       0.198\n## Detection Prevalence         0.406       0.198\n## Balanced Accuracy            1.000       1.000\n##                      Class: reptile Class: fish\n## Sensitivity                  1.0000       1.000\n## Specificity                  0.9583       1.000\n## Pos Pred Value               0.5556       1.000\n## Neg Pred Value               1.0000       1.000\n## Prevalence                   0.0495       0.129\n## Detection Rate               0.0495       0.129\n## Detection Prevalence         0.0891       0.129\n## Balanced Accuracy            0.9792       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                    0.0000        0.0000\n## Specificity                    1.0000        1.0000\n## Pos Pred Value                    NaN           NaN\n## Neg Pred Value                 0.9604        0.9208\n## Prevalence                     0.0396        0.0792\n## Detection Rate                 0.0000        0.0000\n## Detection Prevalence           0.0000        0.0000\n## Balanced Accuracy              0.5000        0.5000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         0.912\n## Pos Pred Value                      0.556\n## Neg Pred Value                      1.000\n## Prevalence                          0.099\n## Detection Rate                      0.099\n## Detection Prevalence                0.178\n## Balanced Accuracy                   0.956"},{"path":"classification-basic-concepts-and-techniques.html","id":"make-predictions-for-new-data","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.2.3 Make Predictions for New Data","text":"Make animal: lion feathered wingsFix columns factors like training set.Make prediction using default tree","code":"\nmy_animal <- tibble(hair = TRUE, feathers = TRUE, eggs = FALSE,\n  milk = TRUE, airborne = TRUE, aquatic = FALSE, predator = TRUE,\n  toothed = TRUE, backbone = TRUE, breathes = TRUE, venomous = FALSE,\n  fins = FALSE, legs = 4, tail = TRUE, domestic = FALSE,\n  catsize = FALSE, type = NA)\nmy_animal <- my_animal %>% modify_if(is.logical, factor, levels = c(TRUE, FALSE))\nmy_animal## # A tibble: 1 x 17\n##   hair  feathers eggs  milk  airborne aquatic predator\n##   <fct> <fct>    <fct> <fct> <fct>    <fct>   <fct>   \n## 1 TRUE  TRUE     FALSE TRUE  TRUE     FALSE   TRUE    \n## # … with 10 more variables: toothed <fct>,\n## #   backbone <fct>, breathes <fct>, venomous <fct>,\n## #   fins <fct>, legs <dbl>, tail <fct>,\n## #   domestic <fct>, catsize <fct>, type <fct>\npredict(tree_default , my_animal, type = \"class\")##      1 \n## mammal \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al"},{"path":"classification-basic-concepts-and-techniques.html","id":"model-evaluation-with-caret","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3 Model Evaluation with Caret","text":"package caret makes preparing training sets, building\nclassification (regression) models evaluation easier. great cheat sheet can found .Cross-validation runs independent can done faster parallel.\nenable multi-core support, caret uses package foreach \nneed load backend. Linux, can use doMC 4 cores. Windows needs different backend like doParallel (see caret cheat sheet ).Set random number generator seed make results reproducible","code":"\nlibrary(caret)\n## Linux backend\n# library(doMC)\n# registerDoMC(cores = 4)\n# getDoParWorkers()\n\n## Windows backend\n# library(doParallel)\n# cl <- makeCluster(4, type=\"SOCK\")\n# registerDoParallel(cl)\nset.seed(2000)"},{"path":"classification-basic-concepts-and-techniques.html","id":"hold-out-test-data","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3.1 Hold out Test Data","text":"Test data used model building process set aside purely testing model.\n, partition data 80% training 20% testing.","code":"\ninTrain <- createDataPartition(y = Zoo$type, p = .8, list = FALSE)\nZoo_train <- Zoo %>% slice(inTrain)\nZoo_test <- Zoo %>% slice(-inTrain)"},{"path":"classification-basic-concepts-and-techniques.html","id":"learn-a-model-and-tune-hyperparameters-on-the-training-data","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.3.2 Learn a Model and Tune Hyperparameters on the Training Data","text":"package caret combines training validation hyperparameter tuning single function called train().\ninternally splits data training validation sets thus \nprovide error estimates different hyperparameter settings. trainControl used\nchoose testing performed.rpart, train tries tune cp parameter (tree complexity)\nusing accuracy chose best model. set minsplit 2 since \nmuch data.\nNote: Parameters used tuning (case cp) need set using\ndata.frame argument tuneGrid! Setting control ignored.Note: Train built 10 trees using training folds value cp reported values accuracy Kappa averages validation folds.model using best tuning parameters\nusing data supplied train() available fit$finalModel.caret also computes variable importance. default uses competing splits\n(splits runners , get chosen tree)\nrpart models (see ? varImp). Toothed \nrunner many splits, never gets chosen!variable importance without competing splits.Note: models provide variable importance function. case caret might calculate varImp ignore model (see ? varImp)!","code":"\nfit <- Zoo_train %>%\n  train(type ~ .,\n    data = . ,\n    method = \"rpart\",\n    control = rpart.control(minsplit = 2),\n    trControl = trainControl(method = \"cv\", number = 10),\n    tuneLength = 5)\nfit## CART \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 77, 74, 75, 73, 74, 76, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa\n##   0.00  0.938     0.919\n##   0.08  0.897     0.868\n##   0.16  0.745     0.664\n##   0.22  0.666     0.554\n##   0.32  0.474     0.190\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was cp = 0.\nrpart.plot(fit$finalModel, extra = 2,\n  box.palette = list(\"Gy\", \"Gn\", \"Bu\", \"Bn\", \"Or\", \"Rd\", \"Pu\"))\nvarImp(fit)## rpart variable importance\n## \n##               Overall\n## toothedFALSE   100.00\n## feathersFALSE   69.81\n## backboneFALSE   63.08\n## milkFALSE       55.56\n## eggsFALSE       53.61\n## hairFALSE       50.52\n## finsFALSE       46.98\n## tailFALSE       28.45\n## breathesFALSE   28.13\n## airborneFALSE   26.27\n## legs            25.86\n## aquaticFALSE     5.96\n## predatorFALSE    2.35\n## venomousFALSE    1.39\n## catsizeFALSE     0.00\n## domesticFALSE    0.00\nimp <- varImp(fit, compete = FALSE)\nimp## rpart variable importance\n## \n##               Overall\n## milkFALSE      100.00\n## feathersFALSE   55.69\n## finsFALSE       39.45\n## toothedFALSE    22.96\n## airborneFALSE   22.48\n## aquaticFALSE     9.99\n## eggsFALSE        6.66\n## legs             5.55\n## predatorFALSE    1.85\n## domesticFALSE    0.00\n## breathesFALSE    0.00\n## catsizeFALSE     0.00\n## tailFALSE        0.00\n## hairFALSE        0.00\n## backboneFALSE    0.00\n## venomousFALSE    0.00\nggplot(imp)"},{"path":"classification-basic-concepts-and-techniques.html","id":"testing-confusion-matrix-and-confidence-interval-for-accuracy","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.4 Testing: Confusion Matrix and Confidence Interval for Accuracy","text":"Use best model test dataCaret’s confusionMatrix() function calculates accuracy, confidence intervals, kappa many evaluation metrics. need use separate test data create confusion matrix based generalization error.notesMany classification algorithms train caret deal well\nmissing values.\nclassification model can deal missing values (e.g., rpart) use na.action = na.pass call train predict.\nOtherwise, need remove observations missing values \nna.omit use imputation replace missing values train model. Make sure \nstill enough observations left.Make sure nominal variables (includes logical variables)\ncoded factors.class variable train caret level names \nkeywords R (e.g., TRUE FALSE). Rename , example,\n“yes” “.”Make sure nominal variables (factors) examples possible\nvalues. methods might problems variable values\nwithout examples. can drop empty levels using droplevels factor.Sampling train might create sample \ncontain examples values nominal (factor) variable. get\nerror message. \nlikely happens variables one rare value. may \nremove variable.","code":"\npred <- predict(fit, newdata = Zoo_test)\npred##  [1] mammal        mammal        mollusc.et.al\n##  [4] insect        mammal        mammal       \n##  [7] mammal        bird          mammal       \n## [10] mammal        bird          fish         \n## [13] fish          mammal        mollusc.et.al\n## [16] bird          insect        bird         \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al\nconfusionMatrix(data = pred, ref = Zoo_test$type)## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian\n##   mammal             8    0       0    0         0\n##   bird               0    4       0    0         0\n##   reptile            0    0       0    0         0\n##   fish               0    0       0    2         0\n##   amphibian          0    0       0    0         0\n##   insect             0    0       1    0         0\n##   mollusc.et.al      0    0       0    0         0\n##                Reference\n## Prediction      insect mollusc.et.al\n##   mammal             0             0\n##   bird               0             0\n##   reptile            0             0\n##   fish               0             0\n##   amphibian          0             0\n##   insect             1             0\n##   mollusc.et.al      0             2\n## \n## Overall Statistics\n##                                         \n##                Accuracy : 0.944         \n##                  95% CI : (0.727, 0.999)\n##     No Information Rate : 0.444         \n##     P-Value [Acc > NIR] : 1.08e-05      \n##                                         \n##                   Kappa : 0.923         \n##                                         \n##  Mcnemar's Test P-Value : NA            \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       1.000\n## Pos Pred Value               1.000       1.000\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.444       0.222\n## Detection Rate               0.444       0.222\n## Detection Prevalence         0.444       0.222\n## Balanced Accuracy            1.000       1.000\n##                      Class: reptile Class: fish\n## Sensitivity                  0.0000       1.000\n## Specificity                  1.0000       1.000\n## Pos Pred Value                  NaN       1.000\n## Neg Pred Value               0.9444       1.000\n## Prevalence                   0.0556       0.111\n## Detection Rate               0.0000       0.111\n## Detection Prevalence         0.0000       0.111\n## Balanced Accuracy            0.5000       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                        NA        1.0000\n## Specificity                         1        0.9412\n## Pos Pred Value                     NA        0.5000\n## Neg Pred Value                     NA        1.0000\n## Prevalence                          0        0.0556\n## Detection Rate                      0        0.0556\n## Detection Prevalence                0        0.1111\n## Balanced Accuracy                  NA        0.9706\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         1.000\n## Pos Pred Value                      1.000\n## Neg Pred Value                      1.000\n## Prevalence                          0.111\n## Detection Rate                      0.111\n## Detection Prevalence                0.111\n## Balanced Accuracy                   1.000"},{"path":"classification-basic-concepts-and-techniques.html","id":"model-comparison","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.5 Model Comparison","text":"compare decision trees k-nearest neighbors (kNN) classifier.\ncreate fixed sampling scheme (10-folds) compare different models\nusing exactly folds. specified trControl training.Build modelsNote: kNN ask train scale data using preProcess = \"scale\". Logicals \nused 0-1 variables Euclidean distance calculation.Compare accuracy folds.caret provides visualizations using package lattice. example, boxplot \ncompare accuracy kappa distribution (10 folds).see kNN performing consistently better folds CART (except outlier folds).Find one models statistically better (\ndifference accuracy zero).p-values tells probability seeing even extreme value (difference accuracy) given null hypothesis (difference = 0) true. better classifier, p-value less .05 0.01. diff automatically applies Bonferroni correction multiple comparisons. case, kNN seems better classifiers perform statistically differently.","code":"\ntrain_index <- createFolds(Zoo_train$type, k = 10)\nrpartFit <- Zoo_train %>% train(type ~ .,\n  data = .,\n  method = \"rpart\",\n  tuneLength = 10,\n  trControl = trainControl(method = \"cv\", indexOut = train_index)\n  )\nknnFit <- Zoo_train %>% train(type ~ .,\n  data = .,\n  method = \"knn\",\n  preProcess = \"scale\",\n    tuneLength = 10,\n    trControl = trainControl(method = \"cv\", indexOut = train_index)\n  )\nresamps <- resamples(list(\n        CART = rpartFit,\n        kNearestNeighbors = knnFit\n        ))\nsummary(resamps)## \n## Call:\n## summary.resamples(object = resamps)\n## \n## Models: CART, kNearestNeighbors \n## Number of resamples: 10 \n## \n## Accuracy \n##                    Min. 1st Qu. Median  Mean 3rd Qu.\n## CART              0.667   0.875  0.889 0.872   0.889\n## kNearestNeighbors 0.875   0.917  1.000 0.965   1.000\n##                   Max. NA's\n## CART                 1    0\n## kNearestNeighbors    1    0\n## \n## Kappa \n##                    Min. 1st Qu. Median  Mean 3rd Qu.\n## CART              0.591   0.833  0.847 0.834   0.857\n## kNearestNeighbors 0.833   0.898  1.000 0.955   1.000\n##                   Max. NA's\n## CART                 1    0\n## kNearestNeighbors    1    0\nlibrary(lattice)\nbwplot(resamps, layout = c(3, 1))\ndifs <- diff(resamps)\ndifs## \n## Call:\n## diff.resamples(x = resamps)\n## \n## Models: CART, kNearestNeighbors \n## Metrics: Accuracy, Kappa \n## Number of differences: 1 \n## p-value adjustment: bonferroni\nsummary(difs)## \n## Call:\n## summary.diff.resamples(object = difs)\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## Accuracy \n##                   CART   kNearestNeighbors\n## CART                     -0.0931          \n## kNearestNeighbors 0.0115                  \n## \n## Kappa \n##                   CART   kNearestNeighbors\n## CART                     -0.121           \n## kNearestNeighbors 0.0104"},{"path":"classification-basic-concepts-and-techniques.html","id":"feature-selection-and-feature-preparation","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.6 Feature Selection and Feature Preparation","text":"Decision trees implicitly select features splitting, can also\nselect features manually.see: http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection#The_Feature_Ranking_Approach","code":"\nlibrary(FSelector)"},{"path":"classification-basic-concepts-and-techniques.html","id":"univariate-feature-importance-score","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.6.1 Univariate Feature Importance Score","text":"scores measure related\nfeature class variable.\ndiscrete features (case), chi-square statistic can used\nderive score.plot importance descending order (using reorder order factor levels used ggplot).Get 5 best featuresUse best 5 features build model (Fselector provides .simple.formula)many alternative ways calculate univariate importance\nscores (see package FSelector). (also) work continuous\nfeatures. One example information gain ratio based entropy used decision tree induction.","code":"\nweights <- Zoo_train %>% chi.squared(type ~ ., data = .) %>%\n  as_tibble(rownames = \"feature\") %>%\n  arrange(desc(attr_importance))\nweights## # A tibble: 16 x 2\n##    feature  attr_importance\n##    <chr>              <dbl>\n##  1 feathers           1    \n##  2 milk               1    \n##  3 backbone           1    \n##  4 toothed            0.975\n##  5 eggs               0.933\n##  6 hair               0.907\n##  7 breathes           0.898\n##  8 airborne           0.848\n##  9 fins               0.845\n## 10 legs               0.828\n## 11 tail               0.779\n## 12 catsize            0.664\n## 13 aquatic            0.655\n## 14 venomous           0.475\n## 15 predator           0.385\n## 16 domestic           0.231\nggplot(weights,\n  aes(x = attr_importance, y = reorder(feature, attr_importance))) +\n  geom_bar(stat = \"identity\") +\n  xlab(\"Importance score\") + ylab(\"Feature\")\nsubset <- cutoff.k(weights %>% column_to_rownames(\"feature\"), 5)\nsubset## [1] \"feathers\" \"milk\"     \"backbone\" \"toothed\" \n## [5] \"eggs\"\nf <- as.simple.formula(subset, \"type\")\nf## type ~ feathers + milk + backbone + toothed + eggs\n## <environment: 0x562e1a9d5638>\nm <- Zoo_train %>% rpart(f, data = .)\nrpart.plot(m, extra = 2, roundint = FALSE)\nZoo_train %>% gain.ratio(type ~ ., data = .) %>%\n  as_tibble(rownames = \"feature\") %>%\n  arrange(desc(attr_importance))## # A tibble: 16 x 2\n##    feature  attr_importance\n##    <chr>              <dbl>\n##  1 milk              1     \n##  2 backbone          1     \n##  3 feathers          1     \n##  4 toothed           0.919 \n##  5 eggs              0.827 \n##  6 breathes          0.821 \n##  7 hair              0.782 \n##  8 fins              0.689 \n##  9 legs              0.682 \n## 10 airborne          0.671 \n## 11 tail              0.573 \n## 12 aquatic           0.391 \n## 13 catsize           0.383 \n## 14 venomous          0.351 \n## 15 predator          0.125 \n## 16 domestic          0.0975"},{"path":"classification-basic-concepts-and-techniques.html","id":"feature-subset-selection","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.6.2 Feature Subset Selection","text":"Often features related calculating importance feature\nindependently optimal. can use greedy search heuristics. \nexample cfs uses correlation/entropy best first search.Black-box feature selection uses evaluator function (black box)\ncalculate score maximized.\nFirst, define evaluation function builds model given subset\nfeatures calculates quality score. use \naverage 5 bootstrap samples (method = \"cv\" can also used instead), tuning (faster), \naverage accuracy score.Start features (class variable type)several (greedy) search strategies available. run\n!","code":"\nZoo_train %>% cfs(type ~ ., data = .)##  [1] \"hair\"     \"feathers\" \"eggs\"     \"milk\"    \n##  [5] \"toothed\"  \"backbone\" \"breathes\" \"fins\"    \n##  [9] \"legs\"     \"tail\"\nevaluator <- function(subset) {\n  model <- Zoo_train %>% train(as.simple.formula(subset, \"type\"),\n    data = .,\n    method = \"rpart\",\n    trControl = trainControl(method = \"boot\", number = 5),\n    tuneLength = 0)\n  results <- model$resample$Accuracy\n  cat(\"Trying features:\", paste(subset, collapse = \" + \"), \"\\n\")\n  m <- mean(results)\n  cat(\"Accuracy:\", round(m, 2), \"\\n\\n\")\n  m\n}\nfeatures <- Zoo_train %>% colnames() %>% setdiff(\"type\")\n##subset <- backward.search(features, evaluator)\n##subset <- forward.search(features, evaluator)\n##subset <- best.first.search(features, evaluator)\n##subset <- hill.climbing.search(features, evaluator)\n##subset"},{"path":"classification-basic-concepts-and-techniques.html","id":"using-dummy-variables-for-factors","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.6.3 Using Dummy Variables for Factors","text":"Nominal features (factors) often encoded series 0-1 dummy variables.\nexample, let us try predict animal predator given type.\nFirst use original encoding type factor several values.Note: splits use multiple values. Building tree become\nextremely slow factor many levels (different values) since tree check possible splits two subsets. situation avoided.Recode type set 0-1 dummy variables using class2ind. See also\n? dummyVars package caret.Using caret original factor encoding automatically translates factors\n(type) 0-1 dummy variables (e.g., typeinsect = 0).\nreason models \ndirectly use factors caret tries consistently work \n.Note: use fixed value tuning parameter cp, \ncreate tuning grid icontains value.","code":"\ntree_predator <- Zoo_train %>% rpart(predator ~ type, data = .)\nrpart.plot(tree_predator, extra = 2, roundint = FALSE)\nZoo_train_dummy <- as_tibble(class2ind(Zoo_train$type)) %>% mutate_all(as.factor) %>%\n  add_column(predator = Zoo_train$predator)\nZoo_train_dummy## # A tibble: 83 x 8\n##    mammal bird  reptile fish  amphibian insect\n##    <fct>  <fct> <fct>   <fct> <fct>     <fct> \n##  1 1      0     0       0     0         0     \n##  2 1      0     0       0     0         0     \n##  3 0      0     0       1     0         0     \n##  4 1      0     0       0     0         0     \n##  5 1      0     0       0     0         0     \n##  6 1      0     0       0     0         0     \n##  7 0      0     0       1     0         0     \n##  8 0      0     0       1     0         0     \n##  9 1      0     0       0     0         0     \n## 10 0      1     0       0     0         0     \n## # … with 73 more rows, and 2 more variables:\n## #   mollusc.et.al <fct>, predator <fct>\ntree_predator <- Zoo_train_dummy %>% rpart(predator ~ ., data = .,\n  control = rpart.control(minsplit = 2, cp = 0.01))\nrpart.plot(tree_predator, roundint = FALSE)\nfit <- Zoo_train %>% train(predator ~ type, data = ., method = \"rpart\",\n  control = rpart.control(minsplit = 2),\n  tuneGrid = data.frame(cp = 0.01))\nfit## CART \n## \n## 83 samples\n##  1 predictor\n##  2 classes: 'TRUE', 'FALSE' \n## \n## No pre-processing\n## Resampling: Bootstrapped (25 reps) \n## Summary of sample sizes: 83, 83, 83, 83, 83, 83, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.606     0.203\n## \n## Tuning parameter 'cp' was held constant at a value\n##  of 0.01\nrpart.plot(fit$finalModel, extra = 2)"},{"path":"classification-basic-concepts-and-techniques.html","id":"class-imbalance","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7 Class Imbalance","text":"Classifiers hard time learn data much observations one class (called majority class). called class imbalance problem.good article problem solutions.Class distributionTo create imbalanced problem, want decide animal reptile.\nFirst, change class variable\nmake binary reptile/reptile classification problem.\nNote: use training data testing. use \nseparate testing data set!forget make class variable factor (nominal variable)\nget regression tree instead classification tree.See class imbalance problem.Create test training data. use 50/50 split make sure test set samples rare reptile class.new class variable clearly balanced. problem\nbuilding tree!","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\ndata(Zoo, package=\"mlbench\")\nggplot(Zoo, aes(y = type)) + geom_bar()\nZoo_reptile <- Zoo %>% mutate(\n  type = factor(Zoo$type == \"reptile\", levels = c(FALSE, TRUE),\n    labels = c(\"nonreptile\", \"reptile\")))\nsummary(Zoo_reptile)##     hair          feathers          eggs        \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:58        FALSE:81        FALSE:42       \n##  TRUE :43        TRUE :20        TRUE :59       \n##                                                 \n##                                                 \n##                                                 \n##     milk          airborne        aquatic       \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:60        FALSE:77        FALSE:65       \n##  TRUE :41        TRUE :24        TRUE :36       \n##                                                 \n##                                                 \n##                                                 \n##   predator        toothed         backbone      \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:45        FALSE:40        FALSE:18       \n##  TRUE :56        TRUE :61        TRUE :83       \n##                                                 \n##                                                 \n##                                                 \n##   breathes        venomous          fins        \n##  Mode :logical   Mode :logical   Mode :logical  \n##  FALSE:21        FALSE:93        FALSE:84       \n##  TRUE :80        TRUE :8         TRUE :17       \n##                                                 \n##                                                 \n##                                                 \n##       legs         tail          domestic      \n##  Min.   :0.00   Mode :logical   Mode :logical  \n##  1st Qu.:2.00   FALSE:26        FALSE:88       \n##  Median :4.00   TRUE :75        TRUE :13       \n##  Mean   :2.84                                  \n##  3rd Qu.:4.00                                  \n##  Max.   :8.00                                  \n##   catsize                type   \n##  Mode :logical   nonreptile:96  \n##  FALSE:57        reptile   : 5  \n##  TRUE :44                       \n##                                 \n##                                 \n## \nggplot(Zoo_reptile, aes(y = type)) + geom_bar()\nset.seed(1234)\ninTrain <- createDataPartition(y = Zoo_reptile$type, p = .5, list = FALSE)\ntraining_reptile <- Zoo_reptile %>% slice(inTrain)\ntesting_reptile <- Zoo_reptile %>% slice(-inTrain)"},{"path":"classification-basic-concepts-and-techniques.html","id":"option-1-use-the-data-as-is-and-hope-for-the-best","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7.1 Option 1: Use the Data As Is and Hope For The Best","text":"Warnings: “missing values resampled performance measures.”\nmeans test folds contain examples classes.\nlikely class imbalance small datasets.tree predicts everything non-reptile. look error \ntest set.Accuracy high, exactly -information rate\nkappa zero. Sensitivity also zero, meaning identify\npositive (reptile). cost missing positive much\nlarger cost associated misclassifying negative, accuracy\ngood measure!\ndealing imbalance, concerned\naccuracy, want increase \nsensitivity, .e., chance identify positive examples.Note: positive class value (one \nwant detect) set manually reptile using positive = \"reptile\".\nOtherwise sensitivity/specificity correctly calculated.","code":"\nfit <- training_reptile %>% train(type ~ .,\n  data = .,\n  method = \"rpart\",\n  trControl = trainControl(method = \"cv\"))## Warning in nominalTrainWorkflow(x = x, y = y, wts =\n## weights, info = trainInfo, : There were missing values\n## in resampled performance measures.\nfit## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 47, 46, 46, 45, 46, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.947     0    \n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n  ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         48       2\n##   reptile             0       0\n##                                         \n##                Accuracy : 0.96          \n##                  95% CI : (0.863, 0.995)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.677         \n##                                         \n##                   Kappa : 0             \n##                                         \n##  Mcnemar's Test P-Value : 0.480         \n##                                         \n##             Sensitivity : 0.00          \n##             Specificity : 1.00          \n##          Pos Pred Value :  NaN          \n##          Neg Pred Value : 0.96          \n##              Prevalence : 0.04          \n##          Detection Rate : 0.00          \n##    Detection Prevalence : 0.00          \n##       Balanced Accuracy : 0.50          \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-basic-concepts-and-techniques.html","id":"option-2-balance-data-with-resampling","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7.2 Option 2: Balance Data With Resampling","text":"use stratified sampling replacement (oversample \nminority/positive class).\nalso use SMOTE (package DMwR) sampling strategies (e.g., package unbalanced). \nuse 50+50 observations (Note: many samples chosen several times).Check unbalanced testing data.Note accuracy information rate!\nHowever, kappa (improvement accuracy randomness) \nsensitivity (ability identify reptiles) increased.tradeoff sensitivity specificity (many identified animals really reptiles)\ntradeoff can controlled using sample\nproportions. can sample reptiles increase sensitivity cost \nlower specificity (effect seen data since test set reptiles).","code":"\nlibrary(sampling)\nset.seed(1000) # for repeatability\n\nid <- strata(training_reptile, stratanames = \"type\", size = c(50, 50), method = \"srswr\")\ntraining_reptile_balanced <- training_reptile %>% slice(id$ID_unit)\ntable(training_reptile_balanced$type)## \n## nonreptile    reptile \n##         50         50\nfit <- training_reptile_balanced %>% train(type ~ .,\n  data = .,\n  method = \"rpart\",\n  trControl = trainControl(method = \"cv\"),\n  control = rpart.control(minsplit = 5))\nfit## CART \n## \n## 100 samples\n##  16 predictor\n##   2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 90, 90, 90, 90, 90, 90, ... \n## Resampling results across tuning parameters:\n## \n##   cp    Accuracy  Kappa\n##   0.18  0.81      0.62 \n##   0.30  0.63      0.26 \n##   0.34  0.53      0.06 \n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was cp = 0.18.\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n  ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         19       0\n##   reptile            29       2\n##                                         \n##                Accuracy : 0.42          \n##                  95% CI : (0.282, 0.568)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1             \n##                                         \n##                   Kappa : 0.05          \n##                                         \n##  Mcnemar's Test P-Value : 2e-07         \n##                                         \n##             Sensitivity : 1.0000        \n##             Specificity : 0.3958        \n##          Pos Pred Value : 0.0645        \n##          Neg Pred Value : 1.0000        \n##              Prevalence : 0.0400        \n##          Detection Rate : 0.0400        \n##    Detection Prevalence : 0.6200        \n##       Balanced Accuracy : 0.6979        \n##                                         \n##        'Positive' Class : reptile       \n## \nid <- strata(training_reptile, stratanames = \"type\", size = c(50, 100), method = \"srswr\")\ntraining_reptile_balanced <- training_reptile %>% slice(id$ID_unit)\ntable(training_reptile_balanced$type)## \n## nonreptile    reptile \n##         50        100\nfit <- training_reptile_balanced %>% train(type ~ .,\n  data = .,\n  method = \"rpart\",\n  trControl = trainControl(method = \"cv\"),\n  control = rpart.control(minsplit = 5))\n\nconfusionMatrix(data = predict(fit, testing_reptile),\n  ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         33       0\n##   reptile            15       2\n##                                         \n##                Accuracy : 0.7           \n##                  95% CI : (0.554, 0.821)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1.000000      \n##                                         \n##                   Kappa : 0.15          \n##                                         \n##  Mcnemar's Test P-Value : 0.000301      \n##                                         \n##             Sensitivity : 1.000         \n##             Specificity : 0.688         \n##          Pos Pred Value : 0.118         \n##          Neg Pred Value : 1.000         \n##              Prevalence : 0.040         \n##          Detection Rate : 0.040         \n##    Detection Prevalence : 0.340         \n##       Balanced Accuracy : 0.844         \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-basic-concepts-and-techniques.html","id":"option-3-build-a-larger-tree-and-use-predicted-probabilities","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7.3 Option 3: Build A Larger Tree and use Predicted Probabilities","text":"Increase complexity require less data splitting node.\nalso use AUC (area ROC) tuning metric.\nneed specify two class\nsummary function. Note tree still trying improve accuracy \ndata AUC! also enable class probabilities since want predict\nprobabilities later.Note: Accuracy high, close -information rate!","code":"\nfit <- training_reptile %>% train(type ~ .,\n  data = .,\n  method = \"rpart\",\n  tuneLength = 10,\n  trControl = trainControl(method = \"cv\",\n    classProbs = TRUE,                 ## necessary for predict with type=\"prob\"\n    summaryFunction=twoClassSummary),  ## necessary for ROC\n  metric = \"ROC\",\n  control = rpart.control(minsplit = 3))## Warning in nominalTrainWorkflow(x = x, y = y, wts =\n## weights, info = trainInfo, : There were missing values\n## in resampled performance measures.\nfit## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 47, 46, 46, 46, 45, ... \n## Resampling results:\n## \n##   ROC    Sens   Spec\n##   0.358  0.975  0   \n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n  ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         48       2\n##   reptile             0       0\n##                                         \n##                Accuracy : 0.96          \n##                  95% CI : (0.863, 0.995)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.677         \n##                                         \n##                   Kappa : 0             \n##                                         \n##  Mcnemar's Test P-Value : 0.480         \n##                                         \n##             Sensitivity : 0.00          \n##             Specificity : 1.00          \n##          Pos Pred Value :  NaN          \n##          Neg Pred Value : 0.96          \n##              Prevalence : 0.04          \n##          Detection Rate : 0.00          \n##    Detection Prevalence : 0.00          \n##       Balanced Accuracy : 0.50          \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-basic-concepts-and-techniques.html","id":"create-a-biased-classifier","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7.3.1 Create A Biased Classifier","text":"can create classifier detect reptiles\nexpense misclassifying non-reptiles. equivalent\nincreasing cost misclassifying reptile non-reptile.\nusual rule predict node\nmajority class test data node.\nbinary classification problem means probability >50%.\nfollowing, reduce threshold 1% .\nmeans new observation ends leaf node 1% \nreptiles training observation\nclassified reptile.\ndata set small works better data.Note accuracy goes information rate.\nHowever, measures based idea errors \ncost. important now able find \nreptiles.","code":"\nprob <- predict(fit, testing_reptile, type = \"prob\")\ntail(prob)##      nonreptile reptile\n## tuna      1.000  0.0000\n## vole      0.962  0.0385\n## wasp      0.500  0.5000\n## wolf      0.962  0.0385\n## worm      1.000  0.0000\n## wren      0.962  0.0385\npred <- as.factor(ifelse(prob[,\"reptile\"]>=0.01, \"reptile\", \"nonreptile\"))\n\nconfusionMatrix(data = pred,\n  ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         13       0\n##   reptile            35       2\n##                                         \n##                Accuracy : 0.3           \n##                  95% CI : (0.179, 0.446)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 1             \n##                                         \n##                   Kappa : 0.029         \n##                                         \n##  Mcnemar's Test P-Value : 9.08e-09      \n##                                         \n##             Sensitivity : 1.0000        \n##             Specificity : 0.2708        \n##          Pos Pred Value : 0.0541        \n##          Neg Pred Value : 1.0000        \n##              Prevalence : 0.0400        \n##          Detection Rate : 0.0400        \n##    Detection Prevalence : 0.7400        \n##       Balanced Accuracy : 0.6354        \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-basic-concepts-and-techniques.html","id":"plot-the-roc-curve","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7.3.2 Plot the ROC Curve","text":"Since binary classification problem classifier predicts\nprobability observation reptile, can also use \nreceiver operating characteristic (ROC)\ncurve. ROC curve different cutoff thresholds probability\nused connected line. area curve represents\nsingle number well classifier works (closer one, better).","code":"\nlibrary(\"pROC\")## Type 'citation(\"pROC\")' for a citation.## \n## Attaching package: 'pROC'## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\nr <- roc(testing_reptile$type == \"reptile\", prob[,\"reptile\"])## Setting levels: control = FALSE, case = TRUE## Setting direction: controls < cases\nr## \n## Call:\n## roc.default(response = testing_reptile$type == \"reptile\", predictor = prob[,     \"reptile\"])\n## \n## Data: prob[, \"reptile\"] in 48 controls (testing_reptile$type == \"reptile\" FALSE) < 2 cases (testing_reptile$type == \"reptile\" TRUE).\n## Area under the curve: 0.766\nggroc(r) + geom_abline(intercept = 1, slope = 1, color = \"darkgrey\")"},{"path":"classification-basic-concepts-and-techniques.html","id":"option-4-use-a-cost-sensitive-classifier","chapter":"3 Classification: Basic Concepts and Techniques","heading":"3.7.4 Option 4: Use a Cost-Sensitive Classifier","text":"implementation CART rpart can use cost matrix making splitting\ndecisions (parameter loss). matrix formTP FP\nFN TNTP TN 0. make FN expensive (100).warning “missing values resampled performance measures”\nmeans folds contain reptiles (class imbalance)\nthus performance measures calculates.high cost false negatives results classifier miss reptile.Note: Using cost-sensitive classifier often best option. Unfortunately, classification algorithms (implementation) ability consider misclassification cost.","code":"\ncost <- matrix(c(\n  0,   1,\n  100, 0\n), byrow = TRUE, nrow = 2)\ncost##      [,1] [,2]\n## [1,]    0    1\n## [2,]  100    0\nfit <- training_reptile %>% train(type ~ .,\n  data = .,\n  method = \"rpart\",\n  parms = list(loss = cost),\n  trControl = trainControl(method = \"cv\"))\nfit## CART \n## \n## 51 samples\n## 16 predictors\n##  2 classes: 'nonreptile', 'reptile' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 46, 46, 46, 45, 46, 45, ... \n## Resampling results:\n## \n##   Accuracy  Kappa  \n##   0.477     -0.0304\n## \n## Tuning parameter 'cp' was held constant at a value of 0\nrpart.plot(fit$finalModel, extra = 2)\nconfusionMatrix(data = predict(fit, testing_reptile),\n  ref = testing_reptile$type, positive = \"reptile\")## Confusion Matrix and Statistics\n## \n##             Reference\n## Prediction   nonreptile reptile\n##   nonreptile         39       0\n##   reptile             9       2\n##                                         \n##                Accuracy : 0.82          \n##                  95% CI : (0.686, 0.914)\n##     No Information Rate : 0.96          \n##     P-Value [Acc > NIR] : 0.99998       \n##                                         \n##                   Kappa : 0.257         \n##                                         \n##  Mcnemar's Test P-Value : 0.00766       \n##                                         \n##             Sensitivity : 1.000         \n##             Specificity : 0.812         \n##          Pos Pred Value : 0.182         \n##          Neg Pred Value : 1.000         \n##              Prevalence : 0.040         \n##          Detection Rate : 0.040         \n##    Detection Prevalence : 0.220         \n##       Balanced Accuracy : 0.906         \n##                                         \n##        'Positive' Class : reptile       \n## "},{"path":"classification-alternative-techniques.html","id":"classification-alternative-techniques","chapter":"4 Classification: Alternative Techniques","heading":"4 Classification: Alternative Techniques","text":"Packages used chapter: C50 (Kuhn Quinlan 2021), caret (Kuhn 2021), e1071 (Meyer et al. 2021), keras (Allaire Chollet 2021), lattice (Sarkar 2021), MASS (Ripley 2021a), mlbench (Leisch Dimitriadou. 2021), nnet (Ripley 2021b), randomForest (Breiman et al. 2018), rpart (Therneau Atkinson 2019), RWeka (Hornik 2020), scales (Wickham Seidel 2020), tidyverse (Wickham 2021c)use tidyverse prepare data.Show fewer digits","code":"\nlibrary(tidyverse)\noptions(digits=3)"},{"path":"classification-alternative-techniques.html","id":"training-and-test-data","chapter":"4 Classification: Alternative Techniques","heading":"4.1 Training and Test Data","text":"use Zoo dataset included R package mlbench (may install ).\nZoo dataset containing 17 (mostly logical) variables different 101 animals \ndata frame 17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize, type). convert data frame tidyverse tibble (optional).use package caret make preparing training sets building classification (regression) models easier. great cheat sheet can found .Use multi-core support cross-validation.\nNote: commented work rJava used RWeka .Test data used model building process needs set aside purely testing model completely built. use 80% training.","code":"\ndata(Zoo, package=\"mlbench\")\nZoo <- as_tibble(Zoo)\nZoo## # A tibble: 101 x 17\n##    hair  feathers eggs  milk  airborne aquatic predator\n##    <lgl> <lgl>    <lgl> <lgl> <lgl>    <lgl>   <lgl>   \n##  1 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE    \n##  2 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n##  3 FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE    \n##  4 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE    \n##  5 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   TRUE    \n##  6 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n##  7 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n##  8 FALSE FALSE    TRUE  FALSE FALSE    TRUE    FALSE   \n##  9 FALSE FALSE    TRUE  FALSE FALSE    TRUE    TRUE    \n## 10 TRUE  FALSE    FALSE TRUE  FALSE    FALSE   FALSE   \n## # … with 91 more rows, and 10 more variables:\n## #   toothed <lgl>, backbone <lgl>, breathes <lgl>,\n## #   venomous <lgl>, fins <lgl>, legs <int>,\n## #   tail <lgl>, domestic <lgl>, catsize <lgl>,\n## #   type <fct>\nlibrary(caret)\n##library(doMC, quietly = TRUE)\n##registerDoMC(cores = 4)\n##getDoParWorkers()\ninTrain <- createDataPartition(y = Zoo$type, p = .8, list = FALSE)\nZoo_train <- Zoo %>% slice(inTrain)\nZoo_test <- Zoo %>% slice(-inTrain)"},{"path":"classification-alternative-techniques.html","id":"fitting-different-classification-models-to-the-training-data","chapter":"4 Classification: Alternative Techniques","heading":"4.2 Fitting Different Classification Models to the Training Data","text":"Create fixed sampling scheme (10-folds) can compare fitted models\nlater.fixed folds used train() argument\ntrControl = trainControl(method = \"cv\", indexOut = train_index)). \ndon’t need fixed folds, remove indexOut = train_index code .help building models caret see: ? trainNote: careful many NA values data. train()\ncross-validation many fail cases. case \ncan remove features (columns) many NAs, omit NAs using\nna.omit() use imputation replace reasonable\nvalues (e.g., feature mean via kNN). Highly imbalanced datasets also problematic since chance fold \ncontain examples class leading hard understand error message.","code":"\ntrain_index <- createFolds(Zoo_train$type, k = 10)"},{"path":"classification-alternative-techniques.html","id":"conditional-inference-tree-decision-tree","chapter":"4 Classification: Alternative Techniques","heading":"4.2.1 Conditional Inference Tree (Decision Tree)","text":"final model can directly used predict()","code":"\nctreeFit <- Zoo_train %>% train(type ~ .,\n  method = \"ctree\",\n  data = .,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nctreeFit## Conditional Inference Tree \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 75, 76, 74, 74, 76, 74, ... \n## Resampling results across tuning parameters:\n## \n##   mincriterion  Accuracy  Kappa\n##   0.010         0.808     0.747\n##   0.255         0.808     0.747\n##   0.500         0.808     0.747\n##   0.745         0.808     0.747\n##   0.990         0.808     0.747\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was mincriterion\n##  = 0.99.\nplot(ctreeFit$finalModel)\npredict(ctreeFit, head(Zoo_test))## [1] mammal        mollusc.et.al bird         \n## [4] mammal        mollusc.et.al bird         \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al"},{"path":"classification-alternative-techniques.html","id":"c-4.5-decision-tree","chapter":"4 Classification: Alternative Techniques","heading":"4.2.2 C 4.5 Decision Tree","text":"","code":"\nlibrary(RWeka)\nC45Fit <- Zoo_train %>% train(type ~ .,\n  method = \"J48\",\n  data = .,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nC45Fit## C4.5-like Trees \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 76, 73, 74, 74, 76, 76, ... \n## Resampling results across tuning parameters:\n## \n##   C      M  Accuracy  Kappa\n##   0.010  1  0.978     0.971\n##   0.010  2  0.978     0.971\n##   0.010  3  0.978     0.971\n##   0.010  4  0.907     0.879\n##   0.010  5  0.918     0.893\n##   0.133  1  0.978     0.971\n##   0.133  2  0.978     0.971\n##   0.133  3  0.978     0.971\n##   0.133  4  0.907     0.879\n##   0.133  5  0.918     0.893\n##   0.255  1  0.989     0.985\n##   0.255  2  0.989     0.985\n##   0.255  3  0.978     0.971\n##   0.255  4  0.907     0.879\n##   0.255  5  0.918     0.893\n##   0.378  1  0.989     0.985\n##   0.378  2  0.989     0.985\n##   0.378  3  0.978     0.971\n##   0.378  4  0.907     0.879\n##   0.378  5  0.918     0.893\n##   0.500  1  0.989     0.985\n##   0.500  2  0.989     0.985\n##   0.500  3  0.978     0.971\n##   0.500  4  0.907     0.879\n##   0.500  5  0.918     0.893\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final values used for the model were C = 0.255\n##  and M = 1.\nC45Fit$finalModel## J48 pruned tree\n## ------------------\n## \n## feathersTRUE <= 0\n## |   milkTRUE <= 0\n## |   |   toothedTRUE <= 0\n## |   |   |   airborneTRUE <= 0\n## |   |   |   |   predatorTRUE <= 0\n## |   |   |   |   |   legs <= 2: mollusc.et.al (2.0)\n## |   |   |   |   |   legs > 2: insect (2.0)\n## |   |   |   |   predatorTRUE > 0: mollusc.et.al (6.0)\n## |   |   |   airborneTRUE > 0: insect (5.0)\n## |   |   toothedTRUE > 0\n## |   |   |   finsTRUE <= 0\n## |   |   |   |   aquaticTRUE <= 0: reptile (3.0)\n## |   |   |   |   aquaticTRUE > 0\n## |   |   |   |   |   eggsTRUE <= 0: reptile (1.0)\n## |   |   |   |   |   eggsTRUE > 0: amphibian (4.0)\n## |   |   |   finsTRUE > 0: fish (11.0)\n## |   milkTRUE > 0: mammal (33.0)\n## feathersTRUE > 0: bird (16.0)\n## \n## Number of Leaves  :  10\n## \n## Size of the tree :   19"},{"path":"classification-alternative-techniques.html","id":"k-nearest-neighbors","chapter":"4 Classification: Alternative Techniques","heading":"4.2.3 K-Nearest Neighbors","text":"Note: kNN uses Euclidean distance, data standardized (scaled) first.\nlegs measured 0 6 variables \n0 1. Scaling can directly performed preprocessing train using parameter\npreProcess = \"scale\".","code":"\nknnFit <- Zoo_train %>% train(type ~ .,\n  method = \"knn\",\n  data = .,\n  preProcess = \"scale\",\n    tuneLength = 5,\n  tuneGrid=data.frame(k = 1:10),\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nknnFit## k-Nearest Neighbors \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## Pre-processing: scaled (16) \n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 73, 76, 74, 74, 75, ... \n## Resampling results across tuning parameters:\n## \n##   k   Accuracy  Kappa\n##    1  1.000     1.000\n##    2  0.978     0.971\n##    3  0.967     0.957\n##    4  0.943     0.926\n##    5  0.965     0.954\n##    6  0.916     0.891\n##    7  0.883     0.850\n##    8  0.872     0.835\n##    9  0.883     0.848\n##   10  0.908     0.881\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was k = 1.\nknnFit$finalModel## 1-nearest neighbor model\n## Training set outcome distribution:\n## \n##        mammal          bird       reptile \n##            33            16             4 \n##          fish     amphibian        insect \n##            11             4             7 \n## mollusc.et.al \n##             8"},{"path":"classification-alternative-techniques.html","id":"part-rule-based-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.2.4 PART (Rule-based classifier)","text":"","code":"\nrulesFit <- Zoo_train %>% train(type ~ .,\n  method = \"PART\",\n  data = .,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\", indexOut = train_index))\nrulesFit## Rule-Based Classifier \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 77, 72, 77, 74, 74, 73, ... \n## Resampling results across tuning parameters:\n## \n##   threshold  pruned  Accuracy  Kappa\n##   0.010      yes     0.965     0.955\n##   0.010      no      0.988     0.984\n##   0.133      yes     0.965     0.955\n##   0.133      no      0.988     0.984\n##   0.255      yes     0.965     0.955\n##   0.255      no      0.988     0.984\n##   0.378      yes     0.965     0.955\n##   0.378      no      0.988     0.984\n##   0.500      yes     0.965     0.955\n##   0.500      no      0.988     0.984\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final values used for the model were threshold\n##  = 0.5 and pruned = no.\nrulesFit$finalModel## PART decision list\n## ------------------\n## \n## feathersTRUE <= 0 AND\n## milkTRUE > 0: mammal (33.0)\n## \n## feathersTRUE > 0: bird (16.0)\n## \n## toothedTRUE <= 0 AND\n## airborneTRUE <= 0 AND\n## predatorTRUE > 0: mollusc.et.al (6.0)\n## \n## toothedTRUE <= 0 AND\n## legs > 2: insect (7.0)\n## \n## finsTRUE > 0: fish (11.0)\n## \n## toothedTRUE > 0 AND\n## aquaticTRUE <= 0: reptile (3.0)\n## \n## aquaticTRUE > 0 AND\n## venomousTRUE <= 0: amphibian (3.0)\n## \n## aquaticTRUE <= 0: mollusc.et.al (2.0)\n## \n## : reptile (2.0/1.0)\n## \n## Number of Rules  :   9"},{"path":"classification-alternative-techniques.html","id":"linear-support-vector-machines","chapter":"4 Classification: Alternative Techniques","heading":"4.2.5 Linear Support Vector Machines","text":"","code":"\nsvmFit <- Zoo_train %>% train(type ~.,\n  method = \"svmLinear\",\n  data = .,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nsvmFit## Support Vector Machines with Linear Kernel \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 73, 75, 75, 74, 74, 76, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   1         1    \n## \n## Tuning parameter 'C' was held constant at a value of 1\nsvmFit$finalModel## Support Vector Machine object of class \"ksvm\" \n## \n## SV type: C-svc  (classification) \n##  parameter : cost C = 1 \n## \n## Linear (vanilla) kernel function. \n## \n## Number of Support Vectors : 44 \n## \n## Objective Function Value : -0.143 -0.198 -0.148 -0.175 -0.0945 -0.104 -0.19 -0.0814 -0.154 -0.0917 -0.115 -0.177 -0.568 -0.104 -0.15 -0.119 -0.0478 -0.083 -0.123 -0.148 -0.58 \n## Training error : 0"},{"path":"classification-alternative-techniques.html","id":"random-forest","chapter":"4 Classification: Alternative Techniques","heading":"4.2.6 Random Forest","text":"","code":"\nrandomForestFit <- Zoo_train %>% train(type ~ .,\n  method = \"rf\",\n  data = .,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index))\nrandomForestFit## Random Forest \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 74, 76, 75, 74, 73, 76, ... \n## Resampling results across tuning parameters:\n## \n##   mtry  Accuracy  Kappa\n##    2    0.976     0.968\n##    5    0.976     0.968\n##    9    0.976     0.968\n##   12    0.965     0.954\n##   16    0.976     0.969\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final value used for the model was mtry = 2.\nrandomForestFit$finalModel## \n## Call:\n##  randomForest(x = x, y = y, mtry = min(param$mtry, ncol(x))) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 2\n## \n##         OOB estimate of  error rate: 3.61%\n## Confusion matrix:\n##               mammal bird reptile fish amphibian\n## mammal            33    0       0    0         0\n## bird               0   16       0    0         0\n## reptile            0    0       2    1         1\n## fish               0    0       0   11         0\n## amphibian          0    0       0    0         4\n## insect             0    0       0    0         0\n## mollusc.et.al      0    0       0    0         0\n##               insect mollusc.et.al class.error\n## mammal             0             0       0.000\n## bird               0             0       0.000\n## reptile            0             0       0.500\n## fish               0             0       0.000\n## amphibian          0             0       0.000\n## insect             7             0       0.000\n## mollusc.et.al      1             7       0.125"},{"path":"classification-alternative-techniques.html","id":"gradient-boosted-decision-trees-xgboost","chapter":"4 Classification: Alternative Techniques","heading":"4.2.7 Gradient Boosted Decision Trees (xgboost)","text":"","code":"\nxgboostFit <- Zoo_train %>% train(type ~ .,\n  method = \"xgbTree\",\n  data = .,\n  tuneLength = 5,\n  trControl = trainControl(method = \"cv\", indexOut = train_index),\n  tuneGrid = expand.grid(\n    nrounds = 20,\n    max_depth = 3,\n    colsample_bytree = .6,\n    eta = 0.1,\n    gamma=0,\n    min_child_weight = 1,\n    subsample = .5\n  ))\nxgboostFit## eXtreme Gradient Boosting \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 76, 75, 75, 74, 76, 74, ... \n## Resampling results:\n## \n##   Accuracy  Kappa\n##   0.976     0.969\n## \n## Tuning parameter 'nrounds' was held constant at\n##  a value of 1\n## Tuning parameter 'subsample' was\n##  held constant at a value of 0.5\nxgboostFit$finalModel## ##### xgb.Booster\n## raw: 83.5 Kb \n## call:\n##   xgboost::xgb.train(params = list(eta = param$eta, max_depth = param$max_depth, \n##     gamma = param$gamma, colsample_bytree = param$colsample_bytree, \n##     min_child_weight = param$min_child_weight, subsample = param$subsample), \n##     data = x, nrounds = param$nrounds, num_class = length(lev), \n##     objective = \"multi:softprob\")\n## params (as set within xgb.train):\n##   eta = \"0.1\", max_depth = \"3\", gamma = \"0\", colsample_bytree = \"0.6\", min_child_weight = \"1\", subsample = \"0.5\", num_class = \"7\", objective = \"multi:softprob\", validate_parameters = \"TRUE\"\n## xgb.attributes:\n##   niter\n## callbacks:\n##   cb.print.evaluation(period = print_every_n)\n## # of features: 16 \n## niter: 20\n## nfeatures : 16 \n## xNames : hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \n## problemType : Classification \n## tuneValue :\n##    nrounds max_depth eta gamma colsample_bytree\n## 1      20         3 0.1     0              0.6\n##   min_child_weight subsample\n## 1                1       0.5\n## obsLevels : mammal bird reptile fish amphibian insect mollusc.et.al \n## param :\n##  list()"},{"path":"classification-alternative-techniques.html","id":"artificial-neural-network","chapter":"4 Classification: Alternative Techniques","heading":"4.2.8 Artificial Neural Network","text":"","code":"\nnnetFit <- Zoo_train %>% train(type ~ .,\n  method = \"nnet\",\n  data = .,\n    tuneLength = 5,\n    trControl = trainControl(method = \"cv\", indexOut = train_index),\n  trace = FALSE)\nnnetFit## Neural Network \n## \n## 83 samples\n## 16 predictors\n##  7 classes: 'mammal', 'bird', 'reptile', 'fish', 'amphibian', 'insect', 'mollusc.et.al' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold) \n## Summary of sample sizes: 75, 77, 72, 75, 75, 76, ... \n## Resampling results across tuning parameters:\n## \n##   size  decay  Accuracy  Kappa\n##   1     0e+00  0.694     0.558\n##   1     1e-04  0.807     0.728\n##   1     1e-03  0.892     0.852\n##   1     1e-02  0.825     0.766\n##   1     1e-01  0.727     0.633\n##   3     0e+00  0.954     0.939\n##   3     1e-04  0.989     0.986\n##   3     1e-03  0.989     0.986\n##   3     1e-02  0.989     0.986\n##   3     1e-01  0.989     0.986\n##   5     0e+00  0.939     0.917\n##   5     1e-04  0.965     0.954\n##   5     1e-03  0.989     0.986\n##   5     1e-02  0.989     0.986\n##   5     1e-01  0.989     0.986\n##   7     0e+00  0.989     0.986\n##   7     1e-04  0.989     0.986\n##   7     1e-03  0.989     0.986\n##   7     1e-02  1.000     1.000\n##   7     1e-01  0.989     0.986\n##   9     0e+00  0.989     0.986\n##   9     1e-04  0.989     0.986\n##   9     1e-03  0.989     0.986\n##   9     1e-02  0.989     0.986\n##   9     1e-01  1.000     1.000\n## \n## Accuracy was used to select the optimal model\n##  using the largest value.\n## The final values used for the model were size = 7\n##  and decay = 0.01.\nnnetFit$finalModel## a 16-7-7 network with 175 weights\n## inputs: hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE \n## output(s): .outcome \n## options were - softmax modelling  decay=0.01"},{"path":"classification-alternative-techniques.html","id":"comparing-models","chapter":"4 Classification: Alternative Techniques","heading":"4.3 Comparing Models","text":"Collect performance metrics models trained data.Calculate summary statisticsPerform inference differences models. metric, pair-wise differences computed tested assess difference equal zero. default Bonferroni correction multiple comparison used. Differences shown upper triangle p-values lower triangle.perform similarly well except ctree (differences first row negative p-values first column <.05 indicating null-hypothesis difference 0 can rejected).","code":"\nresamps <- resamples(list(\n  ctree = ctreeFit,\n  C45 = C45Fit,\n  SVM = svmFit,\n  KNN = knnFit,\n  rules = rulesFit,\n  randomForest = randomForestFit,\n  xgboost = xgboostFit,\n  NeuralNet = nnetFit\n    ))\nresamps## \n## Call:\n## resamples.default(x = list(ctree = ctreeFit, C45\n##  = rulesFit, randomForest = randomForestFit, xgboost\n##  = xgboostFit, NeuralNet = nnetFit))\n## \n## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet \n## Number of resamples: 10 \n## Performance metrics: Accuracy, Kappa \n## Time estimates for: everything, final model fit\nsummary(resamps)## \n## Call:\n## summary.resamples(object = resamps)\n## \n## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet \n## Number of resamples: 10 \n## \n## Accuracy \n##               Min. 1st Qu. Median  Mean 3rd Qu.  Max.\n## ctree        0.750   0.778  0.778 0.808   0.851 0.889\n## C45          0.889   1.000  1.000 0.989   1.000 1.000\n## SVM          1.000   1.000  1.000 1.000   1.000 1.000\n## KNN          1.000   1.000  1.000 1.000   1.000 1.000\n## rules        0.875   1.000  1.000 0.988   1.000 1.000\n## randomForest 0.875   1.000  1.000 0.976   1.000 1.000\n## xgboost      0.875   1.000  1.000 0.976   1.000 1.000\n## NeuralNet    1.000   1.000  1.000 1.000   1.000 1.000\n##              NA's\n## ctree           0\n## C45             0\n## SVM             0\n## KNN             0\n## rules           0\n## randomForest    0\n## xgboost         0\n## NeuralNet       0\n## \n## Kappa \n##               Min. 1st Qu. Median  Mean 3rd Qu.  Max.\n## ctree        0.673   0.701  0.723 0.747   0.798 0.852\n## C45          0.850   1.000  1.000 0.985   1.000 1.000\n## SVM          1.000   1.000  1.000 1.000   1.000 1.000\n## KNN          1.000   1.000  1.000 1.000   1.000 1.000\n## rules        0.837   1.000  1.000 0.984   1.000 1.000\n## randomForest 0.833   1.000  1.000 0.968   1.000 1.000\n## xgboost      0.833   1.000  1.000 0.969   1.000 1.000\n## NeuralNet    1.000   1.000  1.000 1.000   1.000 1.000\n##              NA's\n## ctree           0\n## C45             0\n## SVM             0\n## KNN             0\n## rules           0\n## randomForest    0\n## xgboost         0\n## NeuralNet       0\nlibrary(lattice)\nbwplot(resamps, layout = c(3, 1))\ndifs <- diff(resamps)\ndifs## \n## Call:\n## diff.resamples(x = resamps)\n## \n## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet \n## Metrics: Accuracy, Kappa \n## Number of differences: 28 \n## p-value adjustment: bonferroni\nsummary(difs)## \n## Call:\n## summary.diff.resamples(object = difs)\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## Accuracy \n##              ctree    C45      SVM      KNN     \n## ctree                 -0.18095 -0.19206 -0.19206\n## C45          0.000109          -0.01111 -0.01111\n## SVM          3.49e-05 1.000000           0.00000\n## KNN          3.49e-05 1.000000 NA               \n## rules        5.75e-05 1.000000 1.000000 1.000000\n## randomForest 0.000126 1.000000 1.000000 1.000000\n## xgboost      0.001617 1.000000 1.000000 1.000000\n## NeuralNet    3.49e-05 1.000000 NA       NA      \n##              rules    randomForest xgboost  NeuralNet\n## ctree        -0.17956 -0.16845     -0.16845 -0.19206 \n## C45           0.00139  0.01250      0.01250 -0.01111 \n## SVM           0.01250  0.02361      0.02361  0.00000 \n## KNN           0.01250  0.02361      0.02361  0.00000 \n## rules                  0.01111      0.01111 -0.01250 \n## randomForest 1.000000               0.00000 -0.02361 \n## xgboost      1.000000 1.000000              -0.02361 \n## NeuralNet    1.000000 1.000000     1.000000          \n## \n## Kappa \n##              ctree    C45       SVM       KNN      \n## ctree                 -0.238389 -0.253389 -0.253389\n## C45          6.36e-05           -0.015000 -0.015000\n## SVM          2.08e-05 1.00000              0.000000\n## KNN          2.08e-05 1.00000   NA                 \n## rules        3.70e-05 1.00000   1.00000   1.00000  \n## randomForest 7.76e-05 1.00000   1.00000   1.00000  \n## xgboost      0.00124  1.00000   1.00000   1.00000  \n## NeuralNet    2.08e-05 1.00000   NA        NA       \n##              rules     randomForest xgboost  \n## ctree        -0.237063 -0.221723    -0.222437\n## C45           0.001327  0.016667     0.015952\n## SVM           0.016327  0.031667     0.030952\n## KNN           0.016327  0.031667     0.030952\n## rules                   0.015340     0.014626\n## randomForest 1.00000                -0.000714\n## xgboost      1.00000   1.00000               \n## NeuralNet    1.00000   1.00000      1.00000  \n##              NeuralNet\n## ctree        -0.253389\n## C45          -0.015000\n## SVM           0.000000\n## KNN           0.000000\n## rules        -0.016327\n## randomForest -0.031667\n## xgboost      -0.030952\n## NeuralNet"},{"path":"classification-alternative-techniques.html","id":"applying-the-chosen-model-to-the-test-data","chapter":"4 Classification: Alternative Techniques","heading":"4.4 Applying the Chosen Model to the Test Data","text":"models similarly well data. choose random forest model.Calculate confusion matrix held-test data.","code":"\npr <- predict(randomForestFit, Zoo_test)\npr##  [1] mammal        mollusc.et.al bird         \n##  [4] mammal        insect        bird         \n##  [7] mammal        mollusc.et.al mammal       \n## [10] mammal        bird          bird         \n## [13] fish          mammal        fish         \n## [16] mammal        bird          mammal       \n## 7 Levels: mammal bird reptile fish ... mollusc.et.al\nconfusionMatrix(pr, reference = Zoo_test$type)## Confusion Matrix and Statistics\n## \n##                Reference\n## Prediction      mammal bird reptile fish amphibian\n##   mammal             8    0       0    0         0\n##   bird               0    4       1    0         0\n##   reptile            0    0       0    0         0\n##   fish               0    0       0    2         0\n##   amphibian          0    0       0    0         0\n##   insect             0    0       0    0         0\n##   mollusc.et.al      0    0       0    0         0\n##                Reference\n## Prediction      insect mollusc.et.al\n##   mammal             0             0\n##   bird               0             0\n##   reptile            0             0\n##   fish               0             0\n##   amphibian          0             0\n##   insect             1             0\n##   mollusc.et.al      0             2\n## \n## Overall Statistics\n##                                         \n##                Accuracy : 0.944         \n##                  95% CI : (0.727, 0.999)\n##     No Information Rate : 0.444         \n##     P-Value [Acc > NIR] : 1.08e-05      \n##                                         \n##                   Kappa : 0.922         \n##                                         \n##  Mcnemar's Test P-Value : NA            \n## \n## Statistics by Class:\n## \n##                      Class: mammal Class: bird\n## Sensitivity                  1.000       1.000\n## Specificity                  1.000       0.929\n## Pos Pred Value               1.000       0.800\n## Neg Pred Value               1.000       1.000\n## Prevalence                   0.444       0.222\n## Detection Rate               0.444       0.222\n## Detection Prevalence         0.444       0.278\n## Balanced Accuracy            1.000       0.964\n##                      Class: reptile Class: fish\n## Sensitivity                  0.0000       1.000\n## Specificity                  1.0000       1.000\n## Pos Pred Value                  NaN       1.000\n## Neg Pred Value               0.9444       1.000\n## Prevalence                   0.0556       0.111\n## Detection Rate               0.0000       0.111\n## Detection Prevalence         0.0000       0.111\n## Balanced Accuracy            0.5000       1.000\n##                      Class: amphibian Class: insect\n## Sensitivity                        NA        1.0000\n## Specificity                         1        1.0000\n## Pos Pred Value                     NA        1.0000\n## Neg Pred Value                     NA        1.0000\n## Prevalence                          0        0.0556\n## Detection Rate                      0        0.0556\n## Detection Prevalence                0        0.0556\n## Balanced Accuracy                  NA        1.0000\n##                      Class: mollusc.et.al\n## Sensitivity                         1.000\n## Specificity                         1.000\n## Pos Pred Value                      1.000\n## Neg Pred Value                      1.000\n## Prevalence                          0.111\n## Detection Rate                      0.111\n## Detection Prevalence                0.111\n## Balanced Accuracy                   1.000"},{"path":"classification-alternative-techniques.html","id":"decision-boundaries","chapter":"4 Classification: Alternative Techniques","heading":"4.5 Decision Boundaries","text":"Classifiers create decision boundaries discriminate classes.\nDifferent classifiers able create different shapes decision\nboundaries (e.g., strictly linear) thus classifiers\nmay perform better certain datasets. page visualizes decision\nboundaries found several popular classification methods.following plot adds decision boundary evaluating classifier\nevenly spaced grid points. Note low resolution\n(make evaluation faster) make\ndecision boundary look like small steps even \n(straight) line.","code":"\nlibrary(scales)## \n## Attaching package: 'scales'## The following object is masked from 'package:purrr':\n## \n##     discard## The following object is masked from 'package:readr':\n## \n##     col_factor\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(caret)\n\ndecisionplot <- function(model, x, cl = NULL, predict_type = \"class\",\n  resolution = 100) {\n\n  if(!is.null(cl)) {\n    x_data <- x %>% dplyr::select(-all_of(cl))\n    cl <- x %>% pull(cl)\n  } else cl <- 1\n  k <- length(unique(cl))\n\n  # resubstitution accuracy\n  prediction <- predict(model, x_data, type = predict_type)\n  if(is.list(prediction)) prediction <- prediction$class\n  if(is.numeric(prediction))\n    prediction <-  factor(prediction, labels = levels(cl))\n  else\n    prediction <- factor(prediction, levels = levels(cl))\n\n  cm <- confusionMatrix(data = prediction, reference = cl)\n  acc <- cm$overall[\"Accuracy\"]\n\n  # evaluate model on a grid\n  r <- sapply(x[, 1:2], range, na.rm = TRUE)\n  xs <- seq(r[1,1], r[2,1], length.out = resolution)\n  ys <- seq(r[1,2], r[2,2], length.out = resolution)\n  g <- cbind(rep(xs, each = resolution), rep(ys, time = resolution))\n  colnames(g) <- colnames(r)\n  g <- as_tibble(g)\n\n  ### guess how to get class labels from predict\n  ### (unfortunately not very consistent between models)\n  prediction <- predict(model, g, type = predict_type)\n  if(is.list(prediction)) prediction <- prediction$class\n  if(is.numeric(prediction))\n    prediction <-  factor(prediction, labels = levels(cl))\n  else\n    prediction <- factor(prediction, levels = levels(cl))\n\n  g <- g %>% add_column(prediction)\n\n  ggplot(g, mapping = aes_string(\n    x = colnames(g)[1],\n    y = colnames(g)[2])) +\n    geom_tile(mapping = aes(fill = prediction)) +\n    geom_point(data = x, mapping =  aes_string(\n      x = colnames(x)[1],\n      y = colnames(x)[2],\n      shape = colnames(x)[3]), alpha = .5) +\n    labs(subtitle = paste(\"Training accuracy:\", round(acc, 2)))\n}"},{"path":"classification-alternative-techniques.html","id":"iris-dataset","chapter":"4 Classification: Alternative Techniques","heading":"4.5.1 Iris Dataset","text":"easier visualization, use two dimensions Iris dataset.Note: overplotting use geom_jitter() instead geom_point().","code":"\nset.seed(1000)\ndata(iris)\niris <- as_tibble(iris)\n\n### Three classes (MASS also has a select function)\nx <- iris %>% dplyr::select(Sepal.Length, Sepal.Width, Species)\nx## # A tibble: 150 x 3\n##    Sepal.Length Sepal.Width Species\n##           <dbl>       <dbl> <fct>  \n##  1          5.1         3.5 setosa \n##  2          4.9         3   setosa \n##  3          4.7         3.2 setosa \n##  4          4.6         3.1 setosa \n##  5          5           3.6 setosa \n##  6          5.4         3.9 setosa \n##  7          4.6         3.4 setosa \n##  8          5           3.4 setosa \n##  9          4.4         2.9 setosa \n## 10          4.9         3.1 setosa \n## # … with 140 more rows\nggplot(x, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point()"},{"path":"classification-alternative-techniques.html","id":"k-nearest-neighbors-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.5.1.1 K-Nearest Neighbors Classifier","text":"","code":"\nlibrary(caret)\nmodel <- x %>% knn3(Species ~ ., data = ., k = 1)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"kNN (1 neighbor)\")\nmodel <- x %>% knn3(Species ~ ., data = ., k = 10)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"kNN (10 neighbor)\")"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier","chapter":"4 Classification: Alternative Techniques","heading":"4.5.1.2 Naive Bayes Classifier","text":"","code":"\nlibrary(e1071)\nmodel <- x %>% naiveBayes(Species ~ ., data = .)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"Naive Bayes\")"},{"path":"classification-alternative-techniques.html","id":"linear-discriminant-analysis","chapter":"4 Classification: Alternative Techniques","heading":"4.5.1.3 Linear Discriminant Analysis","text":"","code":"\nlibrary(MASS)## \n## Attaching package: 'MASS'## The following object is masked from 'package:dplyr':\n## \n##     select\nmodel <- x %>% lda(Species ~ ., data = .)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"LDA\")"},{"path":"classification-alternative-techniques.html","id":"multinomial-logistic-regression-implemented-in-nnet","chapter":"4 Classification: Alternative Techniques","heading":"4.5.1.4 Multinomial Logistic Regression (implemented in nnet)","text":"Multinomial logistic regression extension logistic regression problems two classes.","code":"\nlibrary(nnet)\nmodel <- x %>% multinom(Species ~., data = .)## # weights:  12 (6 variable)\n## initial  value 164.791843 \n## iter  10 value 62.715967\n## iter  20 value 59.808291\n## iter  30 value 55.445984\n## iter  40 value 55.375704\n## iter  50 value 55.346472\n## iter  60 value 55.301707\n## iter  70 value 55.253532\n## iter  80 value 55.243230\n## iter  90 value 55.230241\n## iter 100 value 55.212479\n## final  value 55.212479 \n## stopped after 100 iterations\ndecisionplot(model, x, cl = \"Species\") + labs(titel = \"Multinomial Logistic Regression\")"},{"path":"classification-alternative-techniques.html","id":"decision-trees-1","chapter":"4 Classification: Alternative Techniques","heading":"4.5.1.5 Decision Trees","text":"","code":"\nlibrary(\"rpart\")\nmodel <- x %>% rpart(Species ~ ., data = .)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"CART\")\nmodel <- x %>% rpart(Species ~ ., data = .,\n  control = rpart.control(cp = 0.001, minsplit = 1))\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"CART (overfitting)\")\nlibrary(C50)\nmodel <- x %>% C5.0(Species ~ ., data = .)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"C5.0\")\nlibrary(randomForest)## randomForest 4.6-14## Type rfNews() to see new features/changes/bug fixes.## \n## Attaching package: 'randomForest'## The following object is masked from 'package:dplyr':\n## \n##     combine## The following object is masked from 'package:ggplot2':\n## \n##     margin\nmodel <- x %>% randomForest(Species ~ ., data = .)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"Random Forest\")"},{"path":"classification-alternative-techniques.html","id":"svm","chapter":"4 Classification: Alternative Techniques","heading":"4.5.1.6 SVM","text":"","code":"\nlibrary(e1071)\nmodel <- x %>% svm(Species ~ ., data = ., kernel = \"linear\")\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"SVM (linear kernel)\")\nmodel <- x %>% svm(Species ~ ., data = ., kernel = \"radial\")\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"SVM (radial kernel)\")\nmodel <- x %>% svm(Species ~ ., data = ., kernel = \"polynomial\")\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"SVM (polynomial kernel)\")\nmodel <- x %>% svm(Species ~ ., data = ., kernel = \"sigmoid\")\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"SVM (sigmoid kernel)\")"},{"path":"classification-alternative-techniques.html","id":"single-layer-feed-forward-neural-networks","chapter":"4 Classification: Alternative Techniques","heading":"4.5.1.7 Single Layer Feed-forward Neural Networks","text":"","code":"\nlibrary(nnet)\nmodel <-x %>% nnet(Species ~ ., data = ., size = 1, maxit = 1000, trace = FALSE)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"NN (1 neuron)\")\nmodel <-x %>% nnet(Species ~ ., data = ., size = 2, maxit = 1000, trace = FALSE)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"NN (2 neurons)\")\nmodel <-x %>% nnet(Species ~ ., data = ., size = 4, maxit = 1000, trace = FALSE)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"NN (4 neurons)\")\nmodel <-x %>% nnet(Species ~ ., data = ., size = 10, maxit = 1000, trace = FALSE)\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"NN (10 neurons)\")"},{"path":"classification-alternative-techniques.html","id":"deep-learning-with-keras","chapter":"4 Classification: Alternative Techniques","heading":"4.5.1.8 Deep Learning with keras","text":"define predict works decision plotChoices activation function, number layers, number units per layer optimizer.\nL2 regularizer used dense layer weights reduce overfitting. output \ncategorical class value, therefore output layer uses softmax activation function,\nloss categorical crossentropy, metric accuracy.","code":"\nlibrary(keras)\npredict.keras.engine.training.Model <- function(object, newdata, ...)\n  predict_classes(object, as.matrix(newdata))\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 10, activation = 'relu', input_shape = c(2),\n    kernel_regularizer=regularizer_l2(l=0.01)) %>%\n  layer_dense(units = 4, activation = 'softmax') %>%\n  compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n\nhistory <- model %>% fit(\n  as.matrix(x[,1:2]),\n  x %>% pull(3) %>% as.integer %>% to_categorical(),\n  epochs = 100,\n  batch_size = 10\n)\n\nhistory## \n## Final epoch (plot to see history):\n##     loss: 0.7034\n## accuracy: 0.6933\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"keras (relu activation)\")\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 10, activation = 'tanh', input_shape = c(2),\n    kernel_regularizer = regularizer_l2(l = 0.01)) %>%\n  layer_dense(units = 4, activation = 'softmax') %>%\n  compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n\nhistory <- model %>% fit(\n  as.matrix(x[,1:2]),\n  x %>% pull(3) %>% as.integer %>% to_categorical(),\n  epochs = 100,\n  batch_size = 10\n)\n\nhistory## \n## Final epoch (plot to see history):\n##     loss: 0.57\n## accuracy: 0.78\ndecisionplot(model, x, cl = \"Species\") + labs(title = \"keras (tanh activation)\")"},{"path":"classification-alternative-techniques.html","id":"circle-dataset","chapter":"4 Classification: Alternative Techniques","heading":"4.5.2 Circle Dataset","text":"set linearly separable!","code":"\nset.seed(1000)\n\nlibrary(mlbench)\nx <- mlbench.circle(500)\n###x <- mlbench.cassini(500)\n###x <- mlbench.spirals(500, sd = .1)\n###x <- mlbench.smiley(500)\nx <- cbind(as.data.frame(x$x), factor(x$classes))\ncolnames(x) <- c(\"x\", \"y\", \"class\")\nx <- as_tibble(x)\nx## # A tibble: 500 x 3\n##          x       y class\n##      <dbl>   <dbl> <fct>\n##  1 -0.344   0.448  1    \n##  2  0.518   0.915  2    \n##  3 -0.772  -0.0913 1    \n##  4  0.382   0.412  1    \n##  5  0.0328  0.438  1    \n##  6 -0.865  -0.354  2    \n##  7  0.477   0.640  2    \n##  8  0.167  -0.809  2    \n##  9 -0.568  -0.281  1    \n## 10 -0.488   0.638  2    \n## # … with 490 more rows\nggplot(x, aes(x = x, y = y, color = class)) + geom_point()"},{"path":"classification-alternative-techniques.html","id":"k-nearest-neighbors-classifier-1","chapter":"4 Classification: Alternative Techniques","heading":"4.5.2.1 K-Nearest Neighbors Classifier","text":"","code":"\nlibrary(caret)\nmodel <- x %>% knn3(class ~ ., data = ., k = 1)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"kNN (1 neighbor)\")\nmodel <- x %>% knn3(class ~ ., data = ., k = 10)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"kNN (10 neighbor)\")"},{"path":"classification-alternative-techniques.html","id":"naive-bayes-classifier-1","chapter":"4 Classification: Alternative Techniques","heading":"4.5.2.2 Naive Bayes Classifier","text":"","code":"\nlibrary(e1071)\nmodel <- x %>% naiveBayes(class ~ ., data = .)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"naive Bayes\")"},{"path":"classification-alternative-techniques.html","id":"linear-discriminant-analysis-1","chapter":"4 Classification: Alternative Techniques","heading":"4.5.2.3 Linear Discriminant Analysis","text":"","code":"\nlibrary(MASS)\nmodel <- x %>% lda(class ~ ., data = .)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"LDA\")"},{"path":"classification-alternative-techniques.html","id":"multinomial-logistic-regression-implemented-in-nnet-1","chapter":"4 Classification: Alternative Techniques","heading":"4.5.2.4 Multinomial Logistic Regression (implemented in nnet)","text":"Multinomial logistic regression extension logistic regression problems two classes.","code":"\nlibrary(nnet)\nmodel <- x %>% multinom(class ~., data = .)## # weights:  4 (3 variable)\n## initial  value 346.573590 \n## final  value 346.308371 \n## converged\ndecisionplot(model, x, cl = \"class\") + labs(titel = \"Multinomial Logistic Regression\")"},{"path":"classification-alternative-techniques.html","id":"decision-trees-2","chapter":"4 Classification: Alternative Techniques","heading":"4.5.2.5 Decision Trees","text":"","code":"\nlibrary(\"rpart\")\nmodel <- x %>% rpart(class ~ ., data = .)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"CART\")\nmodel <- x %>% rpart(class ~ ., data = .,\n  control = rpart.control(cp = 0.001, minsplit = 1))\ndecisionplot(model, x, cl = \"class\") + labs(title = \"CART (overfitting)\")\nlibrary(C50)\nmodel <- x %>% C5.0(class ~ ., data = .)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"C5.0\")\nlibrary(randomForest)\nmodel <- x %>% randomForest(class ~ ., data = .)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"Random Forest\")"},{"path":"classification-alternative-techniques.html","id":"svm-1","chapter":"4 Classification: Alternative Techniques","heading":"4.5.2.6 SVM","text":"","code":"\nlibrary(e1071)\nmodel <- x %>% svm(class ~ ., data = ., kernel = \"linear\")\ndecisionplot(model, x, cl = \"class\") + labs(title = \"SVM (linear kernel)\")\nmodel <- x %>% svm(class ~ ., data = ., kernel = \"radial\")\ndecisionplot(model, x, cl = \"class\") + labs(title = \"SVM (radial kernel)\")\nmodel <- x %>% svm(class ~ ., data = ., kernel = \"polynomial\")\ndecisionplot(model, x, cl = \"class\") + labs(title = \"SVM (polynomial kernel)\")\nmodel <- x %>% svm(class ~ ., data = ., kernel = \"sigmoid\")\ndecisionplot(model, x, cl = \"class\") + labs(title = \"SVM (sigmoid kernel)\")"},{"path":"classification-alternative-techniques.html","id":"single-layer-feed-forward-neural-networks-1","chapter":"4 Classification: Alternative Techniques","heading":"4.5.2.7 Single Layer Feed-forward Neural Networks","text":"","code":"\nlibrary(nnet)\nmodel <-x %>% nnet(class ~ ., data = ., size = 1, maxit = 1000, trace = FALSE)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"NN (1 neuron)\")\nmodel <-x %>% nnet(class ~ ., data = ., size = 2, maxit = 1000, trace = FALSE)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"NN (2 neurons)\")\nmodel <-x %>% nnet(class ~ ., data = ., size = 4, maxit = 1000, trace = FALSE)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"NN (4 neurons)\")\nmodel <-x %>% nnet(class ~ ., data = ., size = 10, maxit = 1000, trace = FALSE)\ndecisionplot(model, x, cl = \"class\") + labs(title = \"NN (10 neurons)\")"},{"path":"classification-alternative-techniques.html","id":"deep-learning-with-keras-1","chapter":"4 Classification: Alternative Techniques","heading":"4.5.2.8 Deep Learning with keras","text":"redefine predict works decision plotChoices activation function, number layers, number units per layer optimizer.\nL2 regularizer used dense layer weights reduce overfitting. output \ncategorical class value, therefore output layer uses softmax activation function,\nloss categorical crossentropy, metric accuracy.","code":"\nlibrary(keras)\npredict.keras.engine.training.Model <- function(object, newdata, ...)\n  predict_classes(object, as.matrix(newdata))\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 10, activation = 'relu', input_shape = c(2),\n    kernel_regularizer=regularizer_l2(l = 0.0001)) %>%\n  layer_dense(units = 3, activation = 'softmax') %>%\n  compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n\nhistory <- model %>% fit(\n  as.matrix(x[,1:2]),\n  x %>% pull(3) %>% as.integer %>% to_categorical(),\n  epochs = 100,\n  batch_size = 10\n)\n\nhistory## \n## Final epoch (plot to see history):\n##     loss: 0.1891\n## accuracy: 0.958\ndecisionplot(model, x, cl = \"class\") + labs(title = \"keras (relu activation)\")\nmodel <- keras_model_sequential() %>%\n  layer_dense(units = 10, activation = 'tanh', input_shape = c(2),\n    kernel_regularizer = regularizer_l2(l = 0.0001)) %>%\n  layer_dense(units = 3, activation = 'softmax') %>%\n  compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n\nhistory <- model %>% fit(\n  as.matrix(x[,1:2]),\n  x %>% pull(3) %>% as.integer %>% to_categorical(),\n  epochs = 100,\n  batch_size = 10\n)\n\nhistory## \n## Final epoch (plot to see history):\n##     loss: 0.3642\n## accuracy: 0.936\ndecisionplot(model, x, cl = \"class\") + labs(title = \"keras (tanh activation)\")"},{"path":"classification-alternative-techniques.html","id":"more-information","chapter":"4 Classification: Alternative Techniques","heading":"4.6 More Information","text":"Example using deep learning keras.Package caret: http://topepo.github.io/caret/index.htmlTidymodels (machine learning tidyverse): https://www.tidymodels.org/R taskview machine learning: http://cran.r-project.org/web/views/MachineLearning.html","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"association-analysis-basic-concepts-and-algorithms","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5 Association Analysis: Basic Concepts and Algorithms","text":"Packages used chapter: arules (Hahsler et al. 2021), arulesViz (Hahsler 2021a), mlbench (Leisch Dimitriadou. 2021), tidyverse (Wickham 2021c)can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 5. Association Analysis: Basic Concepts Algorithms","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"the-arules-package","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.1 The arules Package","text":"Association rule mining R implemented package arules.information arules package try:\nhelp(package=\"arules\")\n\nvignette(\"arules\") (also available CRAN)arules uses S4 object system implement classes methods.\nStandard R objects use S3 object system\nuse formal class definitions usually implemented\nlist class attribute.\narules many R packages use \nS4 object system based \nformal class definitions member variables methods\n(similar object-oriented programming languages like Java C++).\nimportant differences using S4 objects compared usual S3\nobjects :coercion (casting): (, \"class_name\")help classes: class? class_name","code":"\nlibrary(tidyverse)\nlibrary(arules)\nlibrary(arulesViz)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"transactions","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.2 Transactions","text":"","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"create-transactions","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.2.1 Create Transactions","text":"use Zoo dataset mlbench.data data.frame need converted set transactions row represents transaction column translated items.\ndone using contructor transactions().\nZoo data set means consider animals transactions\ndifferent traits (features) become items animal . \nexample animal antelope item hair transaction.conversion gives warning discrete features (factor logical) can \ndirectly translated items. Continuous features need discretized first.column 13?Possible solution: Make legs /legsAlternatives:use unique value item:discretize (see ? discretize \ndiscretization code Chapter 2):Convert data set transactions","code":"\ndata(Zoo, package = \"mlbench\")\nhead(Zoo)##           hair feathers  eggs  milk airborne aquatic\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE\n## bear      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## boar      TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## buffalo   TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n##          predator toothed backbone breathes venomous\n## aardvark     TRUE    TRUE     TRUE     TRUE    FALSE\n## antelope    FALSE    TRUE     TRUE     TRUE    FALSE\n## bass         TRUE    TRUE     TRUE    FALSE    FALSE\n## bear         TRUE    TRUE     TRUE     TRUE    FALSE\n## boar         TRUE    TRUE     TRUE     TRUE    FALSE\n## buffalo     FALSE    TRUE     TRUE     TRUE    FALSE\n##           fins legs  tail domestic catsize   type\n## aardvark FALSE    4 FALSE    FALSE    TRUE mammal\n## antelope FALSE    4  TRUE    FALSE    TRUE mammal\n## bass      TRUE    0  TRUE    FALSE   FALSE   fish\n## bear     FALSE    4 FALSE    FALSE    TRUE mammal\n## boar     FALSE    4  TRUE    FALSE    TRUE mammal\n## buffalo  FALSE    4  TRUE    FALSE    TRUE mammal\ntrans <- transactions(Zoo)## Warning: Column(s) 13 not logical or factor. Applying\n## default discretization (see '? discretizeDF').\nsummary(Zoo[13])##       legs     \n##  Min.   :0.00  \n##  1st Qu.:2.00  \n##  Median :4.00  \n##  Mean   :2.84  \n##  3rd Qu.:4.00  \n##  Max.   :8.00\nggplot(Zoo, aes(legs)) + geom_histogram()## `stat_bin()` using `bins = 30`. Pick better value\n## with `binwidth`.\ntable(Zoo$legs)## \n##  0  2  4  5  6  8 \n## 23 27 38  1 10  2\nZoo_has_legs <- Zoo %>% mutate(legs = legs > 0)\nggplot(Zoo_has_legs, aes(legs)) + geom_bar()\ntable(Zoo_has_legs$legs)## \n## FALSE  TRUE \n##    23    78\nZoo_unique_leg_values <- Zoo %>% mutate(legs = factor(legs))\nhead(Zoo_unique_leg_values$legs)## [1] 4 4 0 4 4 4\n## Levels: 0 2 4 5 6 8\nZoo_discretized_legs <- Zoo %>% mutate(\n  legs = discretize(legs, breaks = 2, method=\"interval\")\n)\ntable(Zoo_discretized_legs$legs)## \n## [0,4) [4,8] \n##    50    51\ntrans <- transactions(Zoo_has_legs)\ntrans## transactions in sparse format with\n##  101 transactions (rows) and\n##  23 items (columns)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"inspect-transactions","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.2.2 Inspect Transactions","text":"Look created items. still called column names since transactions actually stored large sparse logical matrix (see ).Compare original features (column names) ZooLook (first) transactions matrix. 1 indicates presence item.Look transactions sets itemsPlot binary matrix. Dark dots represent 1s.Look relative frequency (=support) items data set. look 10 frequent items.Alternative encoding: Also create items FALSE (use factor)","code":"\nsummary(trans)## transactions as itemMatrix in sparse format with\n##  101 rows (elements/itemsets/transactions) and\n##  23 columns (items) and a density of 0.361 \n## \n## most frequent items:\n## backbone breathes     legs     tail  toothed  (Other) \n##       83       80       78       75       61      462 \n## \n## element (itemset/transaction) length distribution:\n## sizes\n##  3  4  5  6  7  8  9 10 11 12 \n##  3  2  6  5  8 21 27 25  3  1 \n## \n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    3.00    8.00    9.00    8.31   10.00   12.00 \n## \n## includes extended item information - examples:\n##     labels variables levels\n## 1     hair      hair   TRUE\n## 2 feathers  feathers   TRUE\n## 3     eggs      eggs   TRUE\n## \n## includes extended transaction information - examples:\n##   transactionID\n## 1      aardvark\n## 2      antelope\n## 3          bass\ncolnames(trans)##  [1] \"hair\"               \"feathers\"          \n##  [3] \"eggs\"               \"milk\"              \n##  [5] \"airborne\"           \"aquatic\"           \n##  [7] \"predator\"           \"toothed\"           \n##  [9] \"backbone\"           \"breathes\"          \n## [11] \"venomous\"           \"fins\"              \n## [13] \"legs\"               \"tail\"              \n## [15] \"domestic\"           \"catsize\"           \n## [17] \"type=mammal\"        \"type=bird\"         \n## [19] \"type=reptile\"       \"type=fish\"         \n## [21] \"type=amphibian\"     \"type=insect\"       \n## [23] \"type=mollusc.et.al\"\ncolnames(Zoo)##  [1] \"hair\"     \"feathers\" \"eggs\"     \"milk\"    \n##  [5] \"airborne\" \"aquatic\"  \"predator\" \"toothed\" \n##  [9] \"backbone\" \"breathes\" \"venomous\" \"fins\"    \n## [13] \"legs\"     \"tail\"     \"domestic\" \"catsize\" \n## [17] \"type\"\nas(trans, \"matrix\")[1:3,]##           hair feathers  eggs  milk airborne aquatic\n## aardvark  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## antelope  TRUE    FALSE FALSE  TRUE    FALSE   FALSE\n## bass     FALSE    FALSE  TRUE FALSE    FALSE    TRUE\n##          predator toothed backbone breathes venomous\n## aardvark     TRUE    TRUE     TRUE     TRUE    FALSE\n## antelope    FALSE    TRUE     TRUE     TRUE    FALSE\n## bass         TRUE    TRUE     TRUE    FALSE    FALSE\n##           fins  legs  tail domestic catsize\n## aardvark FALSE  TRUE FALSE    FALSE    TRUE\n## antelope FALSE  TRUE  TRUE    FALSE    TRUE\n## bass      TRUE FALSE  TRUE    FALSE   FALSE\n##          type=mammal type=bird type=reptile type=fish\n## aardvark        TRUE     FALSE        FALSE     FALSE\n## antelope        TRUE     FALSE        FALSE     FALSE\n## bass           FALSE     FALSE        FALSE      TRUE\n##          type=amphibian type=insect type=mollusc.et.al\n## aardvark          FALSE       FALSE              FALSE\n## antelope          FALSE       FALSE              FALSE\n## bass              FALSE       FALSE              FALSE\ninspect(trans[1:3])##     items         transactionID\n## [1] {hair,                     \n##      milk,                     \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      catsize,                  \n##      type=mammal}      aardvark\n## [2] {hair,                     \n##      milk,                     \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      tail,                     \n##      catsize,                  \n##      type=mammal}      antelope\n## [3] {eggs,                     \n##      aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      type=fish}        bass\nimage(trans)\nitemFrequencyPlot(trans,topN = 20)\nggplot(\n  tibble(\n    Support = sort(itemFrequency(trans, type = \"absolute\"), decreasing = TRUE),\n    Item = seq_len(ncol(trans))\n  ), aes(x = Item, y = Support)) + geom_line()\nsapply(Zoo_has_legs, class)##      hair  feathers      eggs      milk  airborne \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##   aquatic  predator   toothed  backbone  breathes \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##  venomous      fins      legs      tail  domestic \n## \"logical\" \"logical\" \"logical\" \"logical\" \"logical\" \n##   catsize      type \n## \"logical\"  \"factor\"\nZoo_factors <- Zoo_has_legs %>% mutate_if(is.logical, factor)\nsapply(Zoo_factors, class)##     hair feathers     eggs     milk airborne  aquatic \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \n## predator  toothed backbone breathes venomous     fins \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \"factor\" \n##     legs     tail domestic  catsize     type \n## \"factor\" \"factor\" \"factor\" \"factor\" \"factor\"\nsummary(Zoo_factors)##     hair     feathers     eggs       milk   \n##  FALSE:58   FALSE:81   FALSE:42   FALSE:60  \n##  TRUE :43   TRUE :20   TRUE :59   TRUE :41  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##   airborne   aquatic    predator   toothed  \n##  FALSE:77   FALSE:65   FALSE:45   FALSE:40  \n##  TRUE :24   TRUE :36   TRUE :56   TRUE :61  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##   backbone   breathes   venomous     fins   \n##  FALSE:18   FALSE:21   FALSE:93   FALSE:84  \n##  TRUE :83   TRUE :80   TRUE : 8   TRUE :17  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##     legs       tail     domestic   catsize  \n##  FALSE:23   FALSE:26   FALSE:88   FALSE:57  \n##  TRUE :78   TRUE :75   TRUE :13   TRUE :44  \n##                                             \n##                                             \n##                                             \n##                                             \n##                                             \n##             type   \n##  mammal       :41  \n##  bird         :20  \n##  reptile      : 5  \n##  fish         :13  \n##  amphibian    : 4  \n##  insect       : 8  \n##  mollusc.et.al:10\ntrans_factors <- transactions(Zoo_factors)\ntrans_factors## transactions in sparse format with\n##  101 transactions (rows) and\n##  39 items (columns)\nitemFrequencyPlot(trans_factors, topN = 20)\n## Select transactions that contain a certain item\ntrans_insects <- trans_factors[trans %in% \"type=insect\"]\ntrans_insects## transactions in sparse format with\n##  8 transactions (rows) and\n##  39 items (columns)\ninspect(trans_insects)##     items             transactionID\n## [1] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=FALSE,               \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          flea    \n## [2] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          gnat    \n## [3] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=TRUE,                \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=TRUE,                \n##      catsize=FALSE,                \n##      type=insect}          honeybee\n## [4] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          housefly\n## [5] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=TRUE,                \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          ladybird\n## [6] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          moth    \n## [7] {hair=FALSE,                   \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=FALSE,               \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=FALSE,               \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          termite \n## [8] {hair=TRUE,                    \n##      feathers=FALSE,               \n##      eggs=TRUE,                    \n##      milk=FALSE,                   \n##      airborne=TRUE,                \n##      aquatic=FALSE,                \n##      predator=FALSE,               \n##      toothed=FALSE,                \n##      backbone=FALSE,               \n##      breathes=TRUE,                \n##      venomous=TRUE,                \n##      fins=FALSE,                   \n##      legs=TRUE,                    \n##      tail=FALSE,                   \n##      domestic=FALSE,               \n##      catsize=FALSE,                \n##      type=insect}          wasp"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"vertical-layout-transaction-id-lists","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.2.3 Vertical Layout (Transaction ID Lists)","text":"default layout transactions horizontal layout (.e. transaction row).\nvertical layout represents transaction data list transaction IDs item (= transaction ID lists).","code":"\nvertical <- as(trans, \"tidLists\")\nas(vertical, \"matrix\")[1:10, 1:5]##          aardvark antelope  bass  bear  boar\n## hair         TRUE     TRUE FALSE  TRUE  TRUE\n## feathers    FALSE    FALSE FALSE FALSE FALSE\n## eggs        FALSE    FALSE  TRUE FALSE FALSE\n## milk         TRUE     TRUE FALSE  TRUE  TRUE\n## airborne    FALSE    FALSE FALSE FALSE FALSE\n## aquatic     FALSE    FALSE  TRUE FALSE FALSE\n## predator     TRUE    FALSE  TRUE  TRUE  TRUE\n## toothed      TRUE     TRUE  TRUE  TRUE  TRUE\n## backbone     TRUE     TRUE  TRUE  TRUE  TRUE\n## breathes     TRUE     TRUE FALSE  TRUE  TRUE"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"frequent-itemsets","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3 Frequent Itemsets","text":"","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"mine-frequent-itemsets","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3.1 Mine Frequent Itemsets","text":"dataset already huge number possible itemsetsFind frequent itemsets (target=“frequent”) default settings.Default minimum support .1 (10%).\nNote: use small data set. larger datasets\ndefault minimum support might low may run memory. probably want start higher minimum support like\n.5 (50%) work way .order find itemsets effect 5 animals need go \nsupport 5%.Sort supportLook frequent itemsets many items (set breaks manually since\nAutomatically chosen breaks look bad)","code":"\n2^ncol(trans)## [1] 8388608\nits <- apriori(trans, parameter=list(target = \"frequent\"))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##          NA    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen            target  ext\n##        5     0.1      1     10 frequent itemsets TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [18 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans, parameter = list(target =\n## \"frequent\")): Mining stopped (maxlen reached). Only\n## patterns up to a length of 10 returned!##  done [0.00s].\n## sorting transactions ... done [0.00s].\n## writing ... [1465 set(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nits## set of 1465 itemsets\n5/nrow(trans)## [1] 0.0495\nits <- apriori(trans, parameter=list(target = \"frequent\", support = 0.05))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##          NA    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen            target  ext\n##        5    0.05      1     10 frequent itemsets TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 5 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [21 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans, parameter = list(target =\n## \"frequent\", support = 0.05)): Mining stopped (maxlen\n## reached). Only patterns up to a length of 10 returned!##  done [0.00s].\n## sorting transactions ... done [0.00s].\n## writing ... [2537 set(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nits## set of 2537 itemsets\nits <- sort(its, by = \"support\")\ninspect(head(its, n = 10))##      items                      support count\n## [1]  {backbone}                 0.822   83   \n## [2]  {breathes}                 0.792   80   \n## [3]  {legs}                     0.772   78   \n## [4]  {tail}                     0.743   75   \n## [5]  {backbone, tail}           0.733   74   \n## [6]  {breathes, legs}           0.723   73   \n## [7]  {backbone, breathes}       0.683   69   \n## [8]  {backbone, legs}           0.634   64   \n## [9]  {backbone, breathes, legs} 0.634   64   \n## [10] {toothed}                  0.604   61\nggplot(tibble(`Itemset Size` = factor(size(its))), aes(`Itemset Size`)) + geom_bar()\ninspect(its[size(its) > 8])##      items         support count\n## [1]  {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.2376    24\n## [2]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       catsize,                  \n##       type=mammal}  0.1584    16\n## [3]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       type=mammal}  0.1485    15\n## [4]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1386    14\n## [5]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [6]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [7]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [8]  {milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [9]  {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize}      0.1287    13\n## [10] {hair,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [11] {hair,                     \n##       milk,                     \n##       predator,                 \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       catsize,                  \n##       type=mammal}  0.1287    13\n## [12] {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       domestic,                 \n##       catsize,                  \n##       type=mammal}  0.0594     6\n## [13] {hair,                     \n##       milk,                     \n##       toothed,                  \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       domestic,                 \n##       type=mammal}  0.0594     6\n## [14] {feathers,                 \n##       eggs,                     \n##       airborne,                 \n##       predator,                 \n##       backbone,                 \n##       breathes,                 \n##       legs,                     \n##       tail,                     \n##       type=bird}    0.0594     6"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"concise-representation-of-itemsets","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.3.2 Concise Representation of Itemsets","text":"Find maximal frequent itemsets (superset frequent)Find closed frequent itemsets (superset frequent)","code":"\nits_max <- its[is.maximal(its)]\nits_max## set of 22 itemsets\ninspect(head(its_max, by = \"support\"))##     items         support count\n## [1] {hair,                     \n##      milk,                     \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes,                 \n##      legs,                     \n##      tail,                     \n##      catsize,                  \n##      type=mammal}  0.1287    13\n## [2] {eggs,                     \n##      aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      type=fish}    0.0891     9\n## [3] {aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      breathes}     0.0792     8\n## [4] {aquatic,                  \n##      predator,                 \n##      toothed,                  \n##      backbone,                 \n##      fins,                     \n##      tail,                     \n##      catsize}      0.0693     7\n## [5] {eggs,                     \n##      venomous}     0.0594     6\n## [6] {predator,                 \n##      venomous}     0.0594     6\nits_closed <- its[is.closed(its)]\nits_closed## set of 230 itemsets\ninspect(head(its_closed, by = \"support\"))##     items            support count\n## [1] {backbone}       0.822   83   \n## [2] {breathes}       0.792   80   \n## [3] {legs}           0.772   78   \n## [4] {tail}           0.743   75   \n## [5] {backbone, tail} 0.733   74   \n## [6] {breathes, legs} 0.723   73\ncounts <- c(\n  frequent=length(its),\n  closed=length(its_closed),\n  maximal=length(its_max)\n)\n\nggplot(as_tibble(counts, rownames = \"Itemsets\"),\n  aes(Itemsets, counts)) + geom_bar(stat = \"identity\")"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"association-rules","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.4 Association Rules","text":"","code":""},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"mine-association-rules","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.4.1 Mine Association Rules","text":"use APRIORI algorithm (see ? apriori)Look rules highest liftCreate rules using alternative encoding (“FALSE” item)","code":"\nrules <- apriori(trans, parameter = list(support = 0.05, confidence = 0.9))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##         0.9    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen target  ext\n##        5    0.05      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 5 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [21 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans, parameter = list(support\n## = 0.05, confidence = 0.9)): Mining stopped (maxlen\n## reached). Only patterns up to a length of 10 returned!##  done [0.00s].\n## writing ... [7174 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nlength(rules)## [1] 7174\ninspect(head(rules))##     lhs                     rhs        support\n## [1] {type=insect}        => {eggs}     0.0792 \n## [2] {type=insect}        => {legs}     0.0792 \n## [3] {type=insect}        => {breathes} 0.0792 \n## [4] {type=mollusc.et.al} => {eggs}     0.0891 \n## [5] {type=fish}          => {fins}     0.1287 \n## [6] {type=fish}          => {aquatic}  0.1287 \n##     confidence coverage lift count\n## [1] 1.0        0.0792   1.71  8   \n## [2] 1.0        0.0792   1.29  8   \n## [3] 1.0        0.0792   1.26  8   \n## [4] 0.9        0.0990   1.54  9   \n## [5] 1.0        0.1287   5.94 13   \n## [6] 1.0        0.1287   2.81 13\nquality(head(rules))##   support confidence coverage lift count\n## 1  0.0792        1.0   0.0792 1.71     8\n## 2  0.0792        1.0   0.0792 1.29     8\n## 3  0.0792        1.0   0.0792 1.26     8\n## 4  0.0891        0.9   0.0990 1.54     9\n## 5  0.1287        1.0   0.1287 5.94    13\n## 6  0.1287        1.0   0.1287 2.81    13\nrules <- sort(rules, by = \"lift\")\ninspect(head(rules, n = 10))##      lhs            rhs         support confidence coverage lift count\n## [1]  {eggs,                                                           \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [2]  {eggs,                                                           \n##       aquatic,                                                        \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [3]  {eggs,                                                           \n##       predator,                                                       \n##       fins}      => {type=fish}  0.0891          1   0.0891 7.77     9\n## [4]  {eggs,                                                           \n##       toothed,                                                        \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [5]  {eggs,                                                           \n##       fins,                                                           \n##       tail}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [6]  {eggs,                                                           \n##       backbone,                                                       \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [7]  {eggs,                                                           \n##       aquatic,                                                        \n##       predator,                                                       \n##       fins}      => {type=fish}  0.0891          1   0.0891 7.77     9\n## [8]  {eggs,                                                           \n##       aquatic,                                                        \n##       toothed,                                                        \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [9]  {eggs,                                                           \n##       aquatic,                                                        \n##       fins,                                                           \n##       tail}      => {type=fish}  0.1287          1   0.1287 7.77    13\n## [10] {eggs,                                                           \n##       aquatic,                                                        \n##       backbone,                                                       \n##       fins}      => {type=fish}  0.1287          1   0.1287 7.77    13\nr <- apriori(trans_factors)## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##         0.8    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen target  ext\n##        5     0.1      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[39 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [34 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans_factors): Mining stopped\n## (maxlen reached). Only patterns up to a length of 10\n## returned!##  done [0.10s].\n## writing ... [1517191 rule(s)] done [0.26s].\n## creating S4 object  ... done [0.73s].\nr## set of 1517191 rules\nprint(object.size(r), unit = \"Mb\")## 110.2 Mb\ninspect(r[1:10])##      lhs                rhs              support\n## [1]  {}              => {feathers=FALSE} 0.802  \n## [2]  {}              => {backbone=TRUE}  0.822  \n## [3]  {}              => {fins=FALSE}     0.832  \n## [4]  {}              => {domestic=FALSE} 0.871  \n## [5]  {}              => {venomous=FALSE} 0.921  \n## [6]  {domestic=TRUE} => {predator=FALSE} 0.109  \n## [7]  {domestic=TRUE} => {aquatic=FALSE}  0.119  \n## [8]  {domestic=TRUE} => {legs=TRUE}      0.119  \n## [9]  {domestic=TRUE} => {breathes=TRUE}  0.119  \n## [10] {domestic=TRUE} => {backbone=TRUE}  0.119  \n##      confidence coverage lift count\n## [1]  0.802      1.000    1.00 81   \n## [2]  0.822      1.000    1.00 83   \n## [3]  0.832      1.000    1.00 84   \n## [4]  0.871      1.000    1.00 88   \n## [5]  0.921      1.000    1.00 93   \n## [6]  0.846      0.129    1.90 11   \n## [7]  0.923      0.129    1.43 12   \n## [8]  0.923      0.129    1.20 12   \n## [9]  0.923      0.129    1.17 12   \n## [10] 0.923      0.129    1.12 12\ninspect(head(r, n = 10, by = \"lift\"))##      lhs                  rhs         support confidence coverage lift count\n## [1]  {breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [2]  {eggs=TRUE,                                                            \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [3]  {milk=FALSE,                                                           \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [4]  {breathes=FALSE,                                                       \n##       fins=TRUE,                                                            \n##       legs=FALSE}      => {type=fish}   0.129          1    0.129 7.77    13\n## [5]  {aquatic=TRUE,                                                         \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [6]  {hair=FALSE,                                                           \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [7]  {eggs=TRUE,                                                            \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [8]  {milk=FALSE,                                                           \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [9]  {toothed=TRUE,                                                         \n##       breathes=FALSE,                                                       \n##       fins=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13\n## [10] {breathes=FALSE,                                                       \n##       fins=TRUE,                                                            \n##       tail=TRUE}       => {type=fish}   0.129          1    0.129 7.77    13"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"calculate-additional-interest-measures","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.4.2 Calculate Additional Interest Measures","text":"Add measures rulesFind rules score high Phi correlation","code":"\ninterestMeasure(rules[1:10], measure = c(\"phi\", \"gini\"),\n  trans = trans)##      phi  gini\n## 1  1.000 0.224\n## 2  1.000 0.224\n## 3  0.814 0.149\n## 4  1.000 0.224\n## 5  1.000 0.224\n## 6  1.000 0.224\n## 7  0.814 0.149\n## 8  1.000 0.224\n## 9  1.000 0.224\n## 10 1.000 0.224\nquality(rules) <- cbind(quality(rules),\n  interestMeasure(rules, measure = c(\"phi\", \"gini\"),\n    trans = trans))\ninspect(head(rules, by = \"phi\"))##     lhs            rhs         support confidence coverage lift count phi  gini\n## [1] {eggs,                                                                     \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [2] {eggs,                                                                     \n##      aquatic,                                                                  \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [3] {eggs,                                                                     \n##      toothed,                                                                  \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [4] {eggs,                                                                     \n##      fins,                                                                     \n##      tail}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [5] {eggs,                                                                     \n##      backbone,                                                                 \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224\n## [6] {eggs,                                                                     \n##      aquatic,                                                                  \n##      toothed,                                                                  \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13   1 0.224"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"mine-using-templates","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.4.3 Mine Using Templates","text":"Sometimes beneficial specify items rule. apriori can use parameter appearance specify (see ? APappearance). \nfollowing restrict rules animal type RHS item \nLHS.Saving rules CSV-file opened Excel tools.write(rules, file = \"rules.csv\", quote = TRUE)","code":"\ntype <- grep(\"type=\", itemLabels(trans), value = TRUE)\ntype## [1] \"type=mammal\"        \"type=bird\"         \n## [3] \"type=reptile\"       \"type=fish\"         \n## [5] \"type=amphibian\"     \"type=insect\"       \n## [7] \"type=mollusc.et.al\"\nrules_type <- apriori(trans, appearance= list(rhs = type))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##         0.8    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen target  ext\n##        5     0.1      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 10 \n## \n## set item appearances ...[7 item(s)] done [0.00s].\n## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s].\n## sorting and recoding items ... [18 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 6 7 8 9 10## Warning in apriori(trans, appearance = list(rhs =\n## type)): Mining stopped (maxlen reached). Only patterns\n## up to a length of 10 returned!##  done [0.00s].\n## writing ... [571 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\ninspect(head(sort(rules_type, by = \"lift\")))##     lhs            rhs         support confidence coverage lift count\n## [1] {eggs,                                                           \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13\n## [2] {eggs,                                                           \n##      aquatic,                                                        \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13\n## [3] {eggs,                                                           \n##      toothed,                                                        \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13\n## [4] {eggs,                                                           \n##      fins,                                                           \n##      tail}      => {type=fish}   0.129          1    0.129 7.77    13\n## [5] {eggs,                                                           \n##      backbone,                                                       \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13\n## [6] {eggs,                                                           \n##      aquatic,                                                        \n##      toothed,                                                        \n##      fins}      => {type=fish}   0.129          1    0.129 7.77    13"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"association-rule-visualization","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.5 Association Rule Visualization","text":"Default scatterplotNote jitter (randomly move points) added show many rules \nconfidence support value. Without jitter:Grouped plotAs graph","code":"\nlibrary(arulesViz)\nplot(rules)## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\nplot(rules, control = list(jitter = 0))\nplot(rules, shading = \"order\")## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter.\n##plot(rules, interactive = TRUE)\nplot(rules, method = \"grouped\")\n##plot(rules, method = \"grouped\", engine = \"interactive\")\nplot(rules, method = \"graph\")## Warning: Too many rules supplied. Only plotting the\n## best 100 rules using lift (change control parameter max\n## if needed)\nplot(head(rules, by = \"phi\", n = 100), method = \"graph\")"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"interactive-visualizations","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.6 Interactive Visualizations","text":"use association rules mined Iris dataset following examples.Convert data transactions. Note features numeric need discretized.\nconversion automatically applies frequency-based discretization 3 classes numeric\nfeature (warning).Next, mine association rules.","code":"\ndata(iris)\nsummary(iris)##   Sepal.Length   Sepal.Width    Petal.Length \n##  Min.   :4.30   Min.   :2.00   Min.   :1.00  \n##  1st Qu.:5.10   1st Qu.:2.80   1st Qu.:1.60  \n##  Median :5.80   Median :3.00   Median :4.35  \n##  Mean   :5.84   Mean   :3.06   Mean   :3.76  \n##  3rd Qu.:6.40   3rd Qu.:3.30   3rd Qu.:5.10  \n##  Max.   :7.90   Max.   :4.40   Max.   :6.90  \n##   Petal.Width        Species  \n##  Min.   :0.1   setosa    :50  \n##  1st Qu.:0.3   versicolor:50  \n##  Median :1.3   virginica :50  \n##  Mean   :1.2                  \n##  3rd Qu.:1.8                  \n##  Max.   :2.5\niris_trans <- transactions(iris)## Warning: Column(s) 1, 2, 3, 4 not logical or factor.\n## Applying default discretization (see '? discretizeDF').\ninspect(head(iris_trans))##     items                      transactionID\n## [1] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       1\n## [2] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[2.9,3.2),                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       2\n## [3] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       3\n## [4] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[2.9,3.2),                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       4\n## [5] {Sepal.Length=[4.3,5.4),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       5\n## [6] {Sepal.Length=[5.4,6.3),                \n##      Sepal.Width=[3.2,4.4],                 \n##      Petal.Length=[1,2.63),                 \n##      Petal.Width=[0.1,0.867),               \n##      Species=setosa}                       6\nrules <- apriori(iris_trans, parameter = list(support = 0.1, confidence = 0.8))## Apriori\n## \n## Parameter specification:\n##  confidence minval smax arem  aval originalSupport\n##         0.8    0.1    1 none FALSE            TRUE\n##  maxtime support minlen maxlen target  ext\n##        5     0.1      1     10  rules TRUE\n## \n## Algorithmic control:\n##  filter tree heap memopt load sort verbose\n##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n## \n## Absolute minimum support count: 15 \n## \n## set item appearances ...[0 item(s)] done [0.00s].\n## set transactions ...[15 item(s), 150 transaction(s)] done [0.00s].\n## sorting and recoding items ... [15 item(s)] done [0.00s].\n## creating transaction tree ... done [0.00s].\n## checking subsets of size 1 2 3 4 5 done [0.00s].\n## writing ... [144 rule(s)] done [0.00s].\n## creating S4 object  ... done [0.00s].\nrules## set of 144 rules"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"interactive-inspect-with-sorting-filtering-and-paging","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.6.1 Interactive Inspect With Sorting, Filtering and Paging","text":"","code":"\ninspectDT(rules)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"scatter-plot-1","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.6.2 Scatter Plot","text":"Plot rules scatter plot using interactive html widget. avoid overplotting,\njitter added automatically. Set jitter = 0 disable jitter. Hovering rules shows\nrule information.\nNote: plotly/javascript well many points, plot\nselects top 1000 rules warning rules supplied.","code":"\nplot(rules, engine = \"html\")## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter."},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"matrix-visualization","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.6.3 Matrix Visualization","text":"Plot rules matrix using interactive html widget.","code":"\nplot(rules, method = \"matrix\", engine = \"html\") "},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"visualization-as-graph","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.6.4 Visualization as Graph","text":"Plot rules graph using interactive html widget.\nNote: used javascript library well many graph nodes, plot selects top 100 rules (warning).","code":"\nplot(rules, method = \"graph\", engine = \"html\")## Warning: Too many rules supplied. Only plotting the\n## best 100 rules using lift (change control parameter max\n## if needed)"},{"path":"association-analysis-basic-concepts-and-algorithms.html","id":"interactive-rule-explorer","chapter":"5 Association Analysis: Basic Concepts and Algorithms","heading":"5.6.5 Interactive Rule Explorer","text":"can specify rule set dataset. explore rules can mined iris, use:\nruleExplorer(iris)rule explorer creates interactive Shiny application can used locally \ndeployed server sharing. deployed version ruleExplorer available\n(using shinyapps.io).","code":""},{"path":"association-analysis-advanced-concepts.html","id":"association-analysis-advanced-concepts","chapter":"6 Association Analysis: Advanced Concepts","heading":"6 Association Analysis: Advanced Concepts","text":"code available Chapter. topics already covered code previous chapter.","code":""},{"path":"clustering-analysis.html","id":"clustering-analysis","chapter":"7 Clustering Analysis","heading":"7 Clustering Analysis","text":"Packages used chapter: cluster (Maechler et al. 2021), dbscan (Hahsler Piekenbrock 2021), e1071 (Meyer et al. 2021), factoextra (Kassambara Mundt 2020), fpc (Hennig 2020), GGally (Schloerke et al. 2021), kernlab (Karatzoglou, Smola, Hornik 2019), mclust (Fraley, Raftery, Scrucca 2020), mlbench (Leisch Dimitriadou. 2021), scatterpie (Yu 2021), seriation (Hahsler, Buchta, Hornik 2021), tidyverse (Wickham 2021c)can read free sample chapter textbook (Tan, Steinbach, Kumar 2005):\nChapter 7. Cluster Analysis: Basic Concepts Algorithms","code":""},{"path":"clustering-analysis.html","id":"data-preparation","chapter":"7 Clustering Analysis","heading":"7.1 Data Preparation","text":"use small clean dataset called Ruspini included R package cluster.Ruspini data set, consisting 75 points four groups popular illustrating clustering techniques. simple data set well separated clusters.\noriginal dataset points ordered group. can shuffle data (rows) using sample_frac samples default 100%.","code":"\nlibrary(tidyverse)\ndata(ruspini, package = \"cluster\")\nruspini <- as_tibble(ruspini) %>% sample_frac()\nruspini## # A tibble: 75 x 2\n##        x     y\n##    <int> <int>\n##  1    38   143\n##  2    30    52\n##  3    22    74\n##  4    70     4\n##  5    77    12\n##  6    18    61\n##  7    85   115\n##  8    34   141\n##  9    53   144\n## 10    35   153\n## # … with 65 more rows"},{"path":"clustering-analysis.html","id":"data-cleaning","chapter":"7 Clustering Analysis","heading":"7.1.1 Data cleaning","text":"clustering algorithms necessary handle missing values outliers (e.g., remove observations). details see Section “Outlier removal” .\ndata set missing values strong outlier looks like clear groups.","code":"\nggplot(ruspini, aes(x = x, y = y)) + geom_point()\nsummary(ruspini)##        x               y        \n##  Min.   :  4.0   Min.   :  4.0  \n##  1st Qu.: 31.5   1st Qu.: 56.5  \n##  Median : 52.0   Median : 96.0  \n##  Mean   : 54.9   Mean   : 92.0  \n##  3rd Qu.: 76.5   3rd Qu.:141.5  \n##  Max.   :117.0   Max.   :156.0"},{"path":"clustering-analysis.html","id":"scale-data","chapter":"7 Clustering Analysis","heading":"7.1.2 Scale data","text":"Clustering algorithms use distances variables largest number range dominate distance calculation. summary shows issue Ruspini dataset \n, x y, roughly 0 150. data analysts still\nscale column data zero mean unit standard deviation (z-scores).\nNote: standard scale() function scales whole data matrix implement function single vector apply numeric columns.scaling, z-scores fall range \\([-3,3]\\) (z-scores measured standard deviations mean), \n\\(0\\) means average.","code":"\n## I use this till tidyverse implements a scale function\nscale_numeric <- function(x) x %>% mutate_if(is.numeric, function(y) as.vector(scale(y)))\n\nruspini_scaled <- ruspini %>% scale_numeric()\nsummary(ruspini_scaled)##        x                y         \n##  Min.   :-1.668   Min.   :-1.807  \n##  1st Qu.:-0.766   1st Qu.:-0.729  \n##  Median :-0.094   Median : 0.082  \n##  Mean   : 0.000   Mean   : 0.000  \n##  3rd Qu.: 0.709   3rd Qu.: 1.016  \n##  Max.   : 2.037   Max.   : 1.314"},{"path":"clustering-analysis.html","id":"clustering-methods","chapter":"7 Clustering Analysis","heading":"7.2 Clustering methods","text":"","code":""},{"path":"clustering-analysis.html","id":"k-means-clustering","chapter":"7 Clustering Analysis","heading":"7.2.1 k-means Clustering","text":"k-means implicitly assumes Euclidean distances. use \\(k = 4\\) clusters run algorithm 10 times random initialized centroids. best result returned.km R object implemented list. clustering vector contains \ncluster assignment data row can accessed using km$cluster. add \ncluster assignment column scaled dataset (make factor since represents \nnominal label).Add centroids plot.Use factoextra package visualization","code":"\nkm <- kmeans(ruspini_scaled, centers = 4, nstart = 10)\nkm## K-means clustering with 4 clusters of sizes 17, 23, 15, 20\n## \n## Cluster means:\n##        x      y\n## 1  1.419  0.469\n## 2 -0.360  1.109\n## 3  0.461 -1.491\n## 4 -1.139 -0.556\n## \n## Clustering vector:\n##  [1] 2 4 4 3 3 4 1 2 2 2 3 2 3 4 1 3 4 4 1 4 2 3 1 2 1\n## [26] 2 3 1 3 2 1 4 3 4 4 1 1 2 1 4 2 1 2 2 4 3 3 2 2 4\n## [51] 1 2 4 2 3 4 2 3 2 4 1 4 4 1 1 2 4 3 1 2 2 3 4 1 2\n## \n## Within cluster sum of squares by cluster:\n## [1] 3.64 2.66 1.08 2.71\n##  (between_SS / total_SS =  93.2 %)\n## \n## Available components:\n## \n## [1] \"cluster\"      \"centers\"      \"totss\"       \n## [4] \"withinss\"     \"tot.withinss\" \"betweenss\"   \n## [7] \"size\"         \"iter\"         \"ifault\"\nruspini_clustered <- ruspini_scaled %>% add_column(cluster = factor(km$cluster))\nruspini_clustered## # A tibble: 75 x 3\n##          x      y cluster\n##      <dbl>  <dbl> <fct>  \n##  1 -0.553   1.05  2      \n##  2 -0.816  -0.822 4      \n##  3 -1.08   -0.370 4      \n##  4  0.496  -1.81  3      \n##  5  0.725  -1.64  3      \n##  6 -1.21   -0.637 4      \n##  7  0.987   0.472 1      \n##  8 -0.685   1.01  2      \n##  9 -0.0616  1.07  2      \n## 10 -0.652   1.25  2      \n## # … with 65 more rows\nggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point()\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\ncentroids## # A tibble: 4 x 3\n##   cluster      x      y\n##   <chr>    <dbl>  <dbl>\n## 1 1        1.42   0.469\n## 2 2       -0.360  1.11 \n## 3 3        0.461 -1.49 \n## 4 4       -1.14  -0.556\nggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)\nlibrary(factoextra)\nfviz_cluster(km, data = ruspini_scaled, centroids = TRUE, repel = TRUE, ellipse.type = \"norm\")## Warning: ggrepel: 10 unlabeled data points (too many\n## overlaps). Consider increasing max.overlaps"},{"path":"clustering-analysis.html","id":"inspect-clusters","chapter":"7 Clustering Analysis","heading":"7.2.1.1 Inspect clusters","text":"inspect clusters created 4-cluster k-means solution. following code can adapted used clustering methods.","code":""},{"path":"clustering-analysis.html","id":"cluster-profiles","chapter":"7 Clustering Analysis","heading":"7.2.1.1.1 Cluster Profiles","text":"Inspect centroids horizontal bar charts organized cluster. group plots cluster, change data format “long”-format using pivot operation. use colors match clusters scatter plots.","code":"\nggplot(pivot_longer(centroids, cols = c(x, y), names_to = \"feature\"),\n  aes(x = value, y = feature, fill = cluster)) +\n  geom_bar(stat = \"identity\") +\n  facet_grid(rows = vars(cluster))"},{"path":"clustering-analysis.html","id":"extract-a-single-cluster","chapter":"7 Clustering Analysis","heading":"7.2.1.1.2 Extract a single cluster","text":"need filter rows corresponding cluster index. next\nexample calculates summary statistics plots data points cluster 1.happens try cluster 8 centers?","code":"\ncluster1 <- ruspini_clustered %>% filter(cluster == 1)\ncluster1## # A tibble: 17 x 3\n##        x      y cluster\n##    <dbl>  <dbl> <fct>  \n##  1 0.987 0.472  1      \n##  2 1.74  0.492  1      \n##  3 1.38  0.615  1      \n##  4 0.758 0.0405 1      \n##  5 1.97  0.513  1      \n##  6 1.45  0.554  1      \n##  7 1.51  0.472  1      \n##  8 0.987 0.0816 1      \n##  9 0.627 0.0816 1      \n## 10 1.74  0.390  1      \n## 11 2.04  0.472  1      \n## 12 1.45  0.739  1      \n## 13 1.84  0.698  1      \n## 14 1.81  0.390  1      \n## 15 1.02  0.821  1      \n## 16 1.41  0.657  1      \n## 17 1.41  0.492  1\nsummary(cluster1)##        x               y         cluster\n##  Min.   :0.627   Min.   :0.041   1:17   \n##  1st Qu.:1.020   1st Qu.:0.390   2: 0   \n##  Median :1.446   Median :0.492   3: 0   \n##  Mean   :1.419   Mean   :0.469   4: 0   \n##  3rd Qu.:1.741   3rd Qu.:0.615          \n##  Max.   :2.037   Max.   :0.821\nggplot(cluster1, aes(x = x, y = y)) + geom_point() +\n  coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2))\nfviz_cluster(kmeans(ruspini_scaled, centers = 8), data = ruspini_scaled,\n  centroids = TRUE,  geom = \"point\", ellipse.type = \"norm\")## Too few points to calculate an ellipse"},{"path":"clustering-analysis.html","id":"hierarchical-clustering","chapter":"7 Clustering Analysis","heading":"7.2.2 Hierarchical Clustering","text":"Hierarchical clustering starts distance matrix. dist() defaults method=“Euclidean.” Note: Distance matrices become large quickly (size time complexity \\(O(n^2)\\) \\(n\\) number data points). possible calculate store matrix small data sets (maybe hundred thousand data points) main memory. data large can use sampling.hclust() implements agglomerative hierarchical clustering. cluster using complete link.Hierarchical clustering return cluster assignments dendrogram. standard plot\nfunction plots dendrogram.Use factoextra (ggplot version). can specify number clusters visualize dendrogram cut clusters.plotting options dendrograms, including plotting\nparts large dendrograms can found .Extract cluster assignments cutting dendrogram four parts add cluster id data.Try 8 clusters (Note: fviz_cluster needs list data cluster labels hclust)Clustering single link","code":"\nd <- dist(ruspini_scaled)\nhc <- hclust(d, method = \"complete\")\nplot(hc)\nfviz_dend(hc, k = 4)## Warning: `guides(<scale> = FALSE)` is deprecated.\n## Please use `guides(<scale> = \"none\")` instead.\nclusters <- cutree(hc, k = 4)\ncluster_complete <- ruspini_scaled %>%\n  add_column(cluster = factor(clusters))\ncluster_complete## # A tibble: 75 x 3\n##          x      y cluster\n##      <dbl>  <dbl> <fct>  \n##  1 -0.553   1.05  1      \n##  2 -0.816  -0.822 2      \n##  3 -1.08   -0.370 2      \n##  4  0.496  -1.81  3      \n##  5  0.725  -1.64  3      \n##  6 -1.21   -0.637 2      \n##  7  0.987   0.472 4      \n##  8 -0.685   1.01  1      \n##  9 -0.0616  1.07  1      \n## 10 -0.652   1.25  1      \n## # … with 65 more rows\nggplot(cluster_complete, aes(x, y, color = cluster)) +\n  geom_point()\nfviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc, k = 8)), geom = \"point\")\nhc_single <- hclust(d, method = \"single\")\nfviz_dend(hc_single, k = 4)## Warning: `guides(<scale> = FALSE)` is deprecated.\n## Please use `guides(<scale> = \"none\")` instead.\nfviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc_single, k = 4)), geom = \"point\")"},{"path":"clustering-analysis.html","id":"density-based-clustering-with-dbscan","chapter":"7 Clustering Analysis","heading":"7.2.3 Density-based clustering with DBSCAN","text":"DBSCAN stands “Density-Based Spatial Clustering Applications Noise.” groups together points closely packed together treats points low-density regions outliers.Parameters: minPts defines many points epsilon neighborhood needed make point\ncore point. often chosen smoothing parameter. use minPts = 4.decide epsilon, knee kNN distance plot often used. Note minPts contains point , k-nearest neighbor . therefore use k = minPts - 1!\nknee around eps = .32.run dbscanNote: Cluster 0 represents outliers).Play eps (neighborhood size) MinPts (minimum points needed core cluster)","code":"\nlibrary(dbscan)\nkNNdistplot(ruspini_scaled, k = 3)\nabline(h = .32, col = \"red\")\ndb <- dbscan(ruspini_scaled, eps = .32, minPts = 4)\ndb## DBSCAN clustering for 75 objects.\n## Parameters: eps = 0.32, minPts = 4\n## The clustering contains 4 cluster(s) and 5 noise points.\n## \n##  0  1  2  3  4 \n##  5 23 20 15 12 \n## \n## Available fields: cluster, eps, minPts\nstr(db)## List of 3\n##  $ cluster: int [1:75] 1 2 2 3 3 2 0 1 1 1 ...\n##  $ eps    : num 0.32\n##  $ minPts : num 4\n##  - attr(*, \"class\")= chr [1:2] \"dbscan_fast\" \"dbscan\"\nggplot(ruspini_scaled %>% add_column(cluster = factor(db$cluster)),\n  aes(x, y, color = cluster)) + geom_point()\nfviz_cluster(db, ruspini_scaled, geom = \"point\")"},{"path":"clustering-analysis.html","id":"partitioning-around-medoids-pam","chapter":"7 Clustering Analysis","heading":"7.2.4 Partitioning Around Medoids (PAM)","text":"PAM tries solve \n\\(k\\)-medoids problem.\nproblem similar \\(k\\)-means, uses medoids instead centroids represent clusters. Like hierarchical clustering, typically works precomputed distance matrix.\nadvantage can use distance metric just Euclidean distances.\nNote: medoid central data point middle cluster.","code":"\nlibrary(cluster)## \n## Attaching package: 'cluster'## The following object is masked _by_ '.GlobalEnv':\n## \n##     ruspini\nd <- dist(ruspini_scaled)\nstr(d)##  'dist' num [1:2775] 1.89 1.51 3.04 2.98 1.81 ...\n##  - attr(*, \"Size\")= int 75\n##  - attr(*, \"Diag\")= logi FALSE\n##  - attr(*, \"Upper\")= logi FALSE\n##  - attr(*, \"method\")= chr \"Euclidean\"\n##  - attr(*, \"call\")= language dist(x = ruspini_scaled)\np <- pam(d, k = 4)\np## Medoids:\n##      ID   \n## [1,] 66 66\n## [2,] 56 56\n## [3,] 33 33\n## [4,] 28 28\n## Clustering vector:\n##  [1] 1 2 2 3 3 2 4 1 1 1 3 1 3 2 4 3 2 2 4 2 1 3 4 1 4\n## [26] 1 3 4 3 1 4 2 3 2 2 4 4 1 4 2 1 4 1 1 2 3 3 1 1 2\n## [51] 4 1 2 1 3 2 1 3 1 2 4 2 2 4 4 1 2 3 4 1 1 3 2 4 1\n## Objective function:\n## build  swap \n## 0.442 0.319 \n## \n## Available components:\n## [1] \"medoids\"    \"id.med\"     \"clustering\" \"objective\" \n## [5] \"isolation\"  \"clusinfo\"   \"silinfo\"    \"diss\"      \n## [9] \"call\"\nruspini_clustered <- ruspini_scaled %>% add_column(cluster = factor(p$cluster))\n\nmedoids <- as_tibble(ruspini_scaled[p$medoids, ], rownames = \"cluster\")\nmedoids## # A tibble: 4 x 3\n##   cluster      x      y\n##   <chr>    <dbl>  <dbl>\n## 1 1       -0.357  1.17 \n## 2 2       -1.18  -0.555\n## 3 3        0.463 -1.46 \n## 4 4        1.45   0.554\nggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() +\n  geom_point(data = medoids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)\n## __Note:__ `fviz_cluster` needs the original data.\nfviz_cluster(c(p, list(data = ruspini_scaled)), geom = \"point\", ellipse.type = \"norm\")"},{"path":"clustering-analysis.html","id":"gaussian-mixture-models","chapter":"7 Clustering Analysis","heading":"7.2.5 Gaussian Mixture Models","text":"Gaussian mixture models assume data set \nresult drawing data set \nGaussian distributions distribution represents cluster. Estimation algorithms try identify location parameters distributions thus can used find clusters.\nMclust() uses Bayesian Information Criterion (BIC) find \nnumber clusters (model selection). BIC uses likelihood \npenalty term guard overfitting.Rerun fixed number 4 clusters","code":"\nlibrary(mclust)## Package 'mclust' version 5.4.7\n## Type 'citation(\"mclust\")' for citing this R package in publications.## \n## Attaching package: 'mclust'## The following object is masked from 'package:purrr':\n## \n##     map\nm <- Mclust(ruspini_scaled)\nsummary(m)## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEI (diagonal, equal volume and shape) model\n## with 5 components: \n## \n##  log-likelihood  n df  BIC  ICL\n##           -91.3 75 16 -252 -252\n## \n## Clustering table:\n##  1  2  3  4  5 \n## 23 20 15  3 14\nplot(m, what = \"classification\")\nm <- Mclust(ruspini_scaled, G=4)\nsummary(m)## ---------------------------------------------------- \n## Gaussian finite mixture model fitted by EM algorithm \n## ---------------------------------------------------- \n## \n## Mclust EEI (diagonal, equal volume and shape) model\n## with 4 components: \n## \n##  log-likelihood  n df  BIC  ICL\n##            -102 75 13 -259 -259\n## \n## Clustering table:\n##  1  2  3  4 \n## 23 20 15 17\nplot(m, what = \"classification\")"},{"path":"clustering-analysis.html","id":"spectral-clustering","chapter":"7 Clustering Analysis","heading":"7.2.6 Spectral clustering","text":"Spectral clustering works embedding data points partitioning problem subspace k largest eigenvectors normalized affinity/kernel matrix. uses simple clustering method like k-means.","code":"\nlibrary(\"kernlab\")## \n## Attaching package: 'kernlab'## The following object is masked from 'package:scales':\n## \n##     alpha## The following object is masked from 'package:arules':\n## \n##     size## The following object is masked from 'package:purrr':\n## \n##     cross## The following object is masked from 'package:ggplot2':\n## \n##     alpha\ncluster_spec <- specc(as.matrix(ruspini_scaled), centers = 4)\ncluster_spec## Spectral Clustering object of class \"specc\" \n## \n##  Cluster memberships: \n##  \n## 1 3 3 2 2 3 4 1 1 1 2 1 2 3 4 2 3 3 4 3 1 2 4 1 4 1 2 4 2 1 4 3 2 3 3 4 4 1 4 3 1 4 1 1 3 2 2 1 1 3 4 1 3 1 2 3 1 2 1 3 4 3 3 4 4 1 3 2 4 1 1 2 3 4 1 \n##  \n## Gaussian Radial Basis kernel function. \n##  Hyperparameter : sigma =  41.7670067458421 \n## \n## Centers:  \n##        [,1]   [,2]\n## [1,] -0.360  1.109\n## [2,]  0.461 -1.491\n## [3,] -1.139 -0.556\n## [4,]  1.419  0.469\n## \n## Cluster size:  \n## [1] 23 15 20 17\n## \n## Within-cluster sum of squares:  \n## [1] 53.27 53.27  8.81 18.84\nggplot(ruspini_scaled %>% add_column(cluster = factor(cluster_spec)),\n  aes(x, y, color = cluster)) + geom_point()"},{"path":"clustering-analysis.html","id":"fuzzy-c-means-clustering","chapter":"7 Clustering Analysis","heading":"7.2.7 Fuzzy C-Means Clustering","text":"fuzzy clustering version k-means clustering problem. data point\ndegree membership cluster.Plot membership (shown small pie charts)","code":"\nlibrary(\"e1071\")\n\ncluster_cmeans <- cmeans(as.matrix(ruspini_scaled), centers = 4)\ncluster_cmeans## Fuzzy c-means clustering with 4 clusters\n## \n## Cluster centers:\n##        x      y\n## 1 -1.137 -0.555\n## 2  0.455 -1.476\n## 3  1.505  0.516\n## 4 -0.376  1.114\n## \n## Memberships:\n##              1        2        3        4\n##  [1,] 0.012065 0.004750 7.76e-03 9.75e-01\n##  [2,] 0.866509 0.074035 2.11e-02 3.84e-02\n##  [3,] 0.971282 0.010239 4.91e-03 1.36e-02\n##  [4,] 0.024935 0.947252 1.65e-02 1.14e-02\n##  [5,] 0.020593 0.950361 1.82e-02 1.09e-02\n##  [6,] 0.992095 0.003402 1.36e-03 3.14e-03\n##  [7,] 0.039260 0.053619 8.11e-01 9.62e-02\n##  [8,] 0.037605 0.013313 1.97e-02 9.29e-01\n##  [9,] 0.024784 0.013940 3.40e-02 9.27e-01\n## [10,] 0.025639 0.010355 1.73e-02 9.47e-01\n## [11,] 0.008241 0.983990 4.42e-03 3.35e-03\n## [12,] 0.001560 0.000705 1.32e-03 9.96e-01\n## [13,] 0.003861 0.992177 2.30e-03 1.66e-03\n## [14,] 0.768380 0.097124 4.14e-02 9.31e-02\n## [15,] 0.005870 0.009963 9.73e-01 1.13e-02\n## [16,] 0.024150 0.952363 1.34e-02 1.01e-02\n## [17,] 0.828839 0.045276 2.77e-02 9.82e-02\n## [18,] 0.904502 0.033979 1.64e-02 4.51e-02\n## [19,] 0.003221 0.004747 9.85e-01 7.44e-03\n## [20,] 0.934346 0.027260 1.15e-02 2.69e-02\n## [21,] 0.003385 0.001497 2.77e-03 9.92e-01\n## [22,] 0.020387 0.949234 1.93e-02 1.11e-02\n## [23,] 0.107506 0.177387 5.41e-01 1.74e-01\n## [24,] 0.011470 0.004817 8.41e-03 9.75e-01\n## [25,] 0.018433 0.031839 9.16e-01 3.39e-02\n## [26,] 0.004627 0.002182 4.27e-03 9.89e-01\n## [27,] 0.003167 0.993633 1.85e-03 1.35e-03\n## [28,] 0.000609 0.000943 9.97e-01 1.32e-03\n## [29,] 0.028738 0.947019 1.34e-02 1.08e-02\n## [30,] 0.071388 0.050971 1.76e-01 7.02e-01\n## [31,] 0.000250 0.000411 9.99e-01 5.07e-04\n## [32,] 0.939767 0.029086 1.01e-02 2.10e-02\n## [33,] 0.000110 0.999766 7.43e-05 5.05e-05\n## [34,] 0.860429 0.059383 2.50e-02 5.52e-02\n## [35,] 0.895316 0.033633 1.80e-02 5.31e-02\n## [36,] 0.065465 0.118857 7.06e-01 1.10e-01\n## [37,] 0.128305 0.183755 4.70e-01 2.18e-01\n## [38,] 0.011252 0.005928 1.35e-02 9.69e-01\n## [39,] 0.007575 0.013540 9.65e-01 1.39e-02\n## [40,] 0.890088 0.054964 1.83e-02 3.66e-02\n## [41,] 0.067223 0.044821 1.33e-01 7.55e-01\n## [42,] 0.022924 0.040523 8.96e-01 4.09e-02\n## [43,] 0.009541 0.004635 9.54e-03 9.76e-01\n## [44,] 0.048384 0.016805 2.45e-02 9.10e-01\n## [45,] 0.914871 0.040505 1.46e-02 3.00e-02\n## [46,] 0.049811 0.912543 2.04e-02 1.73e-02\n## [47,] 0.038484 0.892180 4.59e-02 2.34e-02\n## [48,] 0.004484 0.002237 4.75e-03 9.89e-01\n## [49,] 0.015164 0.007890 1.73e-02 9.60e-01\n## [50,] 0.872757 0.063345 2.13e-02 4.26e-02\n## [51,] 0.006153 0.008725 9.70e-01 1.48e-02\n## [52,] 0.075851 0.025668 3.63e-02 8.62e-01\n## [53,] 0.942647 0.022073 9.90e-03 2.54e-02\n## [54,] 0.041983 0.015519 2.38e-02 9.19e-01\n## [55,] 0.017339 0.959100 1.45e-02 9.02e-03\n## [56,] 0.998933 0.000436 1.84e-04 4.47e-04\n## [57,] 0.020461 0.011470 2.85e-02 9.40e-01\n## [58,] 0.018343 0.953743 1.78e-02 1.02e-02\n## [59,] 0.037153 0.014629 2.37e-02 9.25e-01\n## [60,] 0.962608 0.013809 6.49e-03 1.71e-02\n## [61,] 0.013081 0.020545 9.40e-01 2.68e-02\n## [62,] 0.930263 0.035820 1.14e-02 2.25e-02\n## [63,] 0.954076 0.015519 7.84e-03 2.26e-02\n## [64,] 0.010680 0.019237 9.51e-01 1.93e-02\n## [65,] 0.039416 0.046127 7.88e-01 1.27e-01\n## [66,] 0.000964 0.000451 8.88e-04 9.98e-01\n## [67,] 0.973167 0.012776 4.51e-03 9.55e-03\n## [68,] 0.025463 0.953144 1.19e-02 9.53e-03\n## [69,] 0.003456 0.005041 9.83e-01 8.07e-03\n## [70,] 0.010326 0.004135 6.88e-03 9.79e-01\n## [71,] 0.033362 0.019994 5.51e-02 8.92e-01\n## [72,] 0.003079 0.993497 2.03e-03 1.40e-03\n## [73,] 0.887734 0.043108 2.00e-02 4.92e-02\n## [74,] 0.001160 0.001840 9.95e-01 2.46e-03\n## [75,] 0.092067 0.051905 1.05e-01 7.51e-01\n## \n## Closest hard clustering:\n##  [1] 4 1 1 2 2 1 3 4 4 4 2 4 2 1 3 2 1 1 3 1 4 2 3 4 3\n## [26] 4 2 3 2 4 3 1 2 1 1 3 3 4 3 1 4 3 4 4 1 2 2 4 4 1\n## [51] 3 4 1 4 2 1 4 2 4 1 3 1 1 3 3 4 1 2 3 4 4 2 1 3 4\n## \n## Available components:\n## [1] \"centers\"     \"size\"        \"cluster\"    \n## [4] \"membership\"  \"iter\"        \"withinerror\"\n## [7] \"call\"\nlibrary(\"scatterpie\")\nggplot()  +\n  geom_scatterpie(data = cbind(ruspini_scaled, cluster_cmeans$membership),\n    aes(x = x, y = y), cols = colnames(cluster_cmeans$membership), legend_name = \"Membership\") + coord_equal()"},{"path":"clustering-analysis.html","id":"internal-cluster-validation","chapter":"7 Clustering Analysis","heading":"7.3 Internal Cluster Validation","text":"","code":""},{"path":"clustering-analysis.html","id":"compare-the-clustering-quality","chapter":"7 Clustering Analysis","heading":"7.3.1 Compare the Clustering Quality","text":"two popular quality metrics within-cluster sum squares (WCSS) used\n\\(k\\)-means \naverage silhouette width.\nLook within.cluster.ss avg.silwidth .Notes:\n* load fpc since NAMESPACE overwrites dbscan.\n* clustering (second argument ) supplied vector numbers (cluster IDs) factor (use .integer() convert factor ID).Read ? cluster.stats explanation available indices.","code":"\n##library(fpc)\nfpc::cluster.stats(d, km$cluster)## $n\n## [1] 75\n## \n## $cluster.number\n## [1] 4\n## \n## $cluster.size\n## [1] 17 23 15 20\n## \n## $min.cluster.size\n## [1] 15\n## \n## $noisen\n## [1] 0\n## \n## $diameter\n## [1] 1.463 1.159 0.836 1.119\n## \n## $average.distance\n## [1] 0.581 0.429 0.356 0.482\n## \n## $median.distance\n## [1] 0.502 0.393 0.338 0.449\n## \n## $separation\n## [1] 0.768 0.768 1.158 1.158\n## \n## $average.toother\n## [1] 2.29 2.15 2.31 2.16\n## \n## $separation.matrix\n##       [,1]  [,2] [,3] [,4]\n## [1,] 0.000 0.768 1.31 1.34\n## [2,] 0.768 0.000 1.96 1.22\n## [3,] 1.308 1.958 0.00 1.16\n## [4,] 1.340 1.220 1.16 0.00\n## \n## $ave.between.matrix\n##      [,1] [,2] [,3] [,4]\n## [1,] 0.00 1.92 2.22 2.77\n## [2,] 1.92 0.00 2.75 1.89\n## [3,] 2.22 2.75 0.00 1.87\n## [4,] 2.77 1.89 1.87 0.00\n## \n## $average.between\n## [1] 2.22\n## \n## $average.within\n## [1] 0.463\n## \n## $n.between\n## [1] 2091\n## \n## $n.within\n## [1] 684\n## \n## $max.diameter\n## [1] 1.46\n## \n## $min.separation\n## [1] 0.768\n## \n## $within.cluster.ss\n## [1] 10.1\n## \n## $clus.avg.silwidths\n##     1     2     3     4 \n## 0.681 0.745 0.807 0.721 \n## \n## $avg.silwidth\n## [1] 0.737\n## \n## $g2\n## NULL\n## \n## $g3\n## NULL\n## \n## $pearsongamma\n## [1] 0.842\n## \n## $dunn\n## [1] 0.525\n## \n## $dunn2\n## [1] 3.23\n## \n## $entropy\n## [1] 1.37\n## \n## $wb.ratio\n## [1] 0.209\n## \n## $ch\n## [1] 324\n## \n## $cwidegap\n## [1] 0.415 0.315 0.235 0.261\n## \n## $widestgap\n## [1] 0.415\n## \n## $sindex\n## [1] 0.858\n## \n## $corrected.rand\n## NULL\n## \n## $vi\n## NULL\nsapply(\n  list(\n    km = km$cluster,\n    hc_compl = cutree(hc, k = 4),\n    hc_single = cutree(hc_single, k = 4)\n  ),\n  FUN = function(x)\n    fpc::cluster.stats(d, x))[c(\"within.cluster.ss\", \"avg.silwidth\"), ]##                   km    hc_compl hc_single\n## within.cluster.ss 10.1  10.1     10.1     \n## avg.silwidth      0.737 0.737    0.737"},{"path":"clustering-analysis.html","id":"silhouette-plot","chapter":"7 Clustering Analysis","heading":"7.3.2 Silhouette plot","text":"Note: silhouette plot show correctly R Studio many objects (bars missing). work open new plotting device windows(), x11() quartz().ggplot visualization using factoextra","code":"\nlibrary(cluster)\nplot(silhouette(km$cluster, d))\nfviz_silhouette(silhouette(km$cluster, d))##   cluster size ave.sil.width\n## 1       1   17          0.68\n## 2       2   23          0.75\n## 3       3   15          0.81\n## 4       4   20          0.72"},{"path":"clustering-analysis.html","id":"find-optimal-number-of-clusters-for-k-means","chapter":"7 Clustering Analysis","heading":"7.3.3 Find Optimal Number of Clusters for k-means","text":"","code":"\nggplot(ruspini_scaled, aes(x, y)) + geom_point()\n## We will use different methods and try 1-10 clusters.\nset.seed(1234)\nks <- 2:10"},{"path":"clustering-analysis.html","id":"elbow-method-within-cluster-sum-of-squares","chapter":"7 Clustering Analysis","heading":"7.3.3.1 Elbow Method: Within-Cluster Sum of Squares","text":"Calculate within-cluster sum squares different numbers clusters look knee elbow plot.\n(nstart = 5 just repeats k-means 5 times returns best solution)","code":"\nWCSS <- sapply(ks, FUN = function(k) {\n  kmeans(ruspini_scaled, centers = k, nstart = 5)$tot.withinss\n  })\n\nggplot(as_tibble(ks, WCSS), aes(ks, WCSS)) + geom_line() +\n  geom_vline(xintercept = 4, color = \"red\", linetype = 2)"},{"path":"clustering-analysis.html","id":"average-silhouette-width","chapter":"7 Clustering Analysis","heading":"7.3.3.2 Average Silhouette Width","text":"Plot average silhouette width different number clusters look maximum plot.","code":"\nASW <- sapply(ks, FUN=function(k) {\n  fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart = 5)$cluster)$avg.silwidth\n  })\n\nbest_k <- ks[which.max(ASW)]\nbest_k## [1] 4\nggplot(as_tibble(ks, ASW), aes(ks, ASW)) + geom_line() +\n  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"},{"path":"clustering-analysis.html","id":"dunn-index","chapter":"7 Clustering Analysis","heading":"7.3.3.3 Dunn Index","text":"Use Dunn index (another internal measure given min. separation/ max. diameter)","code":"\nDI <- sapply(ks, FUN=function(k) {\n  fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart=5)$cluster)$dunn\n})\n\nbest_k <- ks[which.max(DI)]\nggplot(as_tibble(ks, DI), aes(ks, DI)) + geom_line() +\n  geom_vline(xintercept = best_k, color = \"red\", linetype = 2)"},{"path":"clustering-analysis.html","id":"gap-statistic","chapter":"7 Clustering Analysis","heading":"7.3.3.4 Gap Statistic","text":"Compares change within-cluster dispersion expected\nnull model (see ? clusGap).\ndefault method \nchoose smallest k value Gap(k) \n1 standard error away first local maximum.Note: methods can also used hierarchical clustering.many methods indices proposed determine\nnumber clusters.\nSee, e.g., package NbClust.","code":"\nlibrary(cluster)\nk <- clusGap(ruspini_scaled, FUN = kmeans,  nstart = 10, K.max = 10)\nk## Clustering Gap statistic [\"clusGap\"] from call:\n## clusGap(x = ruspini_scaled, FUNcluster = kmeans, K.max = 10,     nstart = 10)\n## B=100 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n##  --> Number of clusters (method 'firstSEmax', SE.factor=1): 4\n##       logW E.logW     gap SE.sim\n##  [1,] 3.50   3.47 -0.0308 0.0357\n##  [2,] 3.07   3.15  0.0762 0.0374\n##  [3,] 2.68   2.90  0.2247 0.0380\n##  [4,] 2.11   2.70  0.5971 0.0363\n##  [5,] 1.99   2.57  0.5827 0.0347\n##  [6,] 1.86   2.45  0.5871 0.0365\n##  [7,] 1.73   2.35  0.6156 0.0395\n##  [8,] 1.66   2.26  0.5987 0.0413\n##  [9,] 1.61   2.17  0.5630 0.0409\n## [10,] 1.50   2.09  0.5910 0.0393\nplot(k)"},{"path":"clustering-analysis.html","id":"visualizing-the-distance-matrix","chapter":"7 Clustering Analysis","heading":"7.3.4 Visualizing the Distance Matrix","text":"Inspect distance matrix first 5 objects.false-color image visualizes value matrix pixel color representing value.Rows columns objects ordered data set. diagonal represents distance object definition distance 0 (dark line).\nVisualizing unordered distance matrix show much structure, can reorder\nmatrix (rows columns) using k-means cluster labels cluster 1 4. clear block structure representing clusters becomes visible.Plot function dissplot package seriation rearranges matrix adds lines cluster labels. lower half plot, shows average dissimilarities clusters. function\norganizes objects cluster reorders clusters objects within clusters similar objects closer together.reordering dissplot makes misspecification k visible blocks.Using factoextra","code":"\nggplot(ruspini_scaled, aes(x, y, color = factor(km$cluster))) + geom_point()\nd <- dist(ruspini_scaled)\nas.matrix(d)[1:5, 1:5]##      1     2     3     4     5\n## 1 0.00 1.887 1.511 3.041 2.978\n## 2 1.89 0.000 0.522 1.640 1.746\n## 3 1.51 0.522 0.000 2.131 2.207\n## 4 3.04 1.640 2.131 0.000 0.282\n## 5 2.98 1.746 2.207 0.282 0.000\nlibrary(seriation)\npimage(d, col = bluered(100))\npimage(d, order=order(km$cluster), col = bluered(100))\ndissplot(d, labels = km$cluster, options=list(main=\"k-means with k=4\"))\ndissplot(d, labels = kmeans(ruspini_scaled, centers = 3)$cluster, col = bluered(100))\ndissplot(d, labels = kmeans(ruspini_scaled, centers = 9)$cluster, col = bluered(100))\nfviz_dist(d)"},{"path":"clustering-analysis.html","id":"external-cluster-validation","chapter":"7 Clustering Analysis","heading":"7.4 External Cluster Validation","text":"External cluster validation uses ground truth information. ,\nuser idea data grouped. known\nclass label provided clustering algorithm.use artificial data set known groups.Prepare dataFind optimal number Clusters k-meansUse within sum squares (look knee)Looks like 7 clustersHierarchical clustering: use single-link mouth non-convex chaining may help.Find optimal number clustersThe maximum clearly 4 clusters.Compare ground truth corrected (=adjusted) Rand index (ARI),\nvariation information (VI) index, entropy purity.cluster_stats computes ARI VI comparative measures. define functions \nentropy purity :calculate measures (comparison also use random “clusterings”\n4 6 clusters)Notes:Hierarchical clustering found perfect clustering.Entropy purity heavily impacted number clusters (clusters improve metric).corrected rand index shows clearly random clusterings relationship ground truth (close 0). helpful property.Read ? cluster.stats explanation available indices.","code":"\nlibrary(mlbench)\nset.seed(1234)\nshapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)\nplot(shapes)\ntruth <- as.integer(shapes$class)\nshapes <- scale(shapes$x)\ncolnames(shapes) <- c(\"x\", \"y\")\nshapes <- as_tibble(shapes)\n\nggplot(shapes, aes(x, y)) + geom_point()\nks <- 2:20\nWCSS <- sapply(ks, FUN = function(k) {\n  kmeans(shapes, centers = k, nstart = 10)$tot.withinss\n})\n\nggplot(as_tibble(ks, WCSS), aes(ks, WCSS)) + geom_line()\nkm <- kmeans(shapes, centers = 7, nstart = 10)\n\nggplot(shapes %>% add_column(cluster = factor(km$cluster)), aes(x, y, color = cluster)) +\n  geom_point()\nd <- dist(shapes)\nhc <- hclust(d, method = \"single\")\nASW <- sapply(ks, FUN = function(k) {\n  fpc::cluster.stats(d, cutree(hc, k))$avg.silwidth\n})\n\nggplot(as_tibble(ks, ASW), aes(ks, ASW)) + geom_line()\nhc_4 <- cutree(hc, 4)\n\nggplot(shapes %>% add_column(cluster = factor(hc_4)), aes(x, y, color = cluster)) +\n  geom_point()\nentropy <- function(cluster, truth) {\n  k <- max(cluster, truth)\n  cluster <- factor(cluster, levels = 1:k)\n  truth <- factor(truth, levels = 1:k)\n  w <- table(cluster)/length(cluster)\n\n  cnts <- sapply(split(truth, cluster), table)\n  p <- sweep(cnts, 1, rowSums(cnts), \"/\")\n  p[is.nan(p)] <- 0\n  e <- -p * log(p, 2)\n\n  sum(w * rowSums(e, na.rm = TRUE))\n}\n\npurity <- function(cluster, truth) {\n  k <- max(cluster, truth)\n  cluster <- factor(cluster, levels = 1:k)\n  truth <- factor(truth, levels = 1:k)\n  w <- table(cluster)/length(cluster)\n\n  cnts <- sapply(split(truth, cluster), table)\n  p <- sweep(cnts, 1, rowSums(cnts), \"/\")\n  p[is.nan(p)] <- 0\n\n  sum(w * apply(p, 1, max))\n}\nrandom_4 <- sample(1:4, nrow(shapes), replace = TRUE)\nrandom_6 <- sample(1:6, nrow(shapes), replace = TRUE)\n\nr <- rbind(\n  kmeans_7 = c(\n    unlist(fpc::cluster.stats(d, km$cluster, truth, compareonly = TRUE)),\n    entropy = entropy(km$cluster, truth),\n    purity = purity(km$cluster, truth)\n    ),\n  hc_4 = c(\n    unlist(fpc::cluster.stats(d, hc_4, truth, compareonly = TRUE)),\n    entropy = entropy(hc_4, truth),\n    purity = purity(hc_4, truth)\n    ),\n  random_4 = c(\n    unlist(fpc::cluster.stats(d, random_4, truth, compareonly = TRUE)),\n    entropy = entropy(random_4, truth),\n    purity = purity(random_4, truth)\n    ),\n  random_6 = c(\n    unlist(fpc::cluster.stats(d, random_6, truth, compareonly = TRUE)),\n    entropy = entropy(random_6, truth),\n    purity = purity(random_6, truth)\n    )\n  )\nr##          corrected.rand    vi entropy purity\n## kmeans_7        0.63823 0.571   0.229  0.464\n## hc_4            1.00000 0.000   0.000  1.000\n## random_4       -0.00324 2.683   1.988  0.288\n## random_6       -0.00213 3.076   1.728  0.144"},{"path":"clustering-analysis.html","id":"advanced-data-preparation-for-clustering","chapter":"7 Clustering Analysis","heading":"7.5 Advanced Data Preparation for Clustering","text":"","code":""},{"path":"clustering-analysis.html","id":"outlier-removal","chapter":"7 Clustering Analysis","heading":"7.5.1 Outlier Removal","text":"clustering algorithms perform complete assignment (.e., data points need assigned cluster). Outliers affect clustering. useful identify outliers remove strong outliers prior clustering.\ndensity based method identify outlier LOF (Local Outlier Factor).\nrelated dbscan compares density around point \ndensities around neighbors (specify neighborhood size \\(k\\)).\nLOF value regular data point 1.\nlarger LOF value gets, likely point outlier.Add clear outlier scaled Ruspini dataset 10 standard deviations average x axis.","code":"\nlibrary(dbscan)\nruspini_scaled_outlier <- ruspini_scaled %>% add_case(x=10,y=0)"},{"path":"clustering-analysis.html","id":"visual-inspection-of-the-data","chapter":"7 Clustering Analysis","heading":"7.5.1.1 Visual inspection of the data","text":"Outliers can identified using summary statistics, histograms, scatterplots (pairs plots), boxplots, etc. use pairs plot (diagonal contains smoothed histograms). outlier visible single separate point scatter plot long tail smoothed histogram x (expect observations fall range [-3,3] normalized data).outlier problem k-meansThis problem can fixed increasing number clusters removing small clusters post-processing step identifying removing outliers clustering.","code":"\nlibrary(\"GGally\")\nggpairs(ruspini_scaled_outlier)\nkm <- kmeans(ruspini_scaled_outlier, centers = 4, nstart = 10)\nruspini_scaled_outlier_km <- ruspini_scaled_outlier%>%\n  add_column(cluster = factor(km$cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\n\nggplot(ruspini_scaled_outlier_km, aes(x = x, y = y, color = cluster)) + geom_point() +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)"},{"path":"clustering-analysis.html","id":"local-outlier-factor-lof","chapter":"7 Clustering Analysis","heading":"7.5.1.2 Local Outlier Factor (LOF)","text":"Local Outlier Factor related concepts DBSCAN can help identify potential outliers.\nCalculate LOF (choose neighborhood size 10 density estimation),Plot points sorted increasing LOF look knee.Choose threshold 1.many outlier removal strategies available. See, e.g., package\noutliers.","code":"\nlof <- lof(ruspini_scaled_outlier, k = 10)## Warning in lof(ruspini_scaled_outlier, k = 10): lof: k\n## is now deprecated. use minPts = 11 instead .\nlof##  [1]  0.978  1.044  0.926  1.024  1.022  0.966  1.154\n##  [8]  1.071  1.124  1.058  1.008  0.933  1.000  1.074\n## [15]  1.008  0.987  1.184  0.989  0.984  1.080  0.911\n## [22]  1.019  1.524  0.979  1.045  0.958  1.022  0.934\n## [29]  0.979  1.470  0.964  0.988  0.973  1.236  1.082\n## [36]  1.326  1.566  1.018  0.998  1.029  1.378  1.107\n## [43]  0.952  1.083  1.091  1.029  1.181  1.009  1.031\n## [50]  1.030  1.002  1.201  1.001  1.071  0.968  0.954\n## [57]  1.046  0.970  1.066  1.045  0.989  0.966  1.028\n## [64]  0.991  1.152  0.942  0.977  1.000  0.984  0.998\n## [71]  1.174  0.996  1.116  0.934  1.588 17.027\nggplot(ruspini_scaled_outlier %>% add_column(lof = lof), aes(x, y, color = lof)) +\n    geom_point() + scale_color_gradient(low = \"gray\", high = \"red\")\nggplot(tibble(index = seq_len(length(lof)), lof = sort(lof)), aes(index, lof)) +\n  geom_line() +\n  geom_hline(yintercept = 1, color = \"red\", linetype = 2)\nggplot(ruspini_scaled_outlier %>% add_column(outlier = lof >= 2), aes(x, y, color = outlier)) +\n  geom_point()\n## Analyze the found outliers (they might be interesting data points) and then cluster the data without them.\nruspini_scaled_clean <- ruspini_scaled_outlier  %>% filter(lof < 2)\n\nkm <- kmeans(ruspini_scaled_clean, centers = 4, nstart = 10)\nruspini_scaled_clean_km <- ruspini_scaled_clean%>%\n  add_column(cluster = factor(km$cluster))\ncentroids <- as_tibble(km$centers, rownames = \"cluster\")\n\nggplot(ruspini_scaled_clean_km, aes(x = x, y = y, color = cluster)) + geom_point() +\n  geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10)"},{"path":"clustering-analysis.html","id":"clustering-tendency","chapter":"7 Clustering Analysis","heading":"7.5.2 Clustering Tendency","text":"clustering algorithms always produce clustering, even \ndata contain cluster structure. typically good check\ncluster tendency attempting cluster data.use smiley data.","code":"\nlibrary(mlbench)\nshapes <- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)$x\ncolnames(shapes) <- c(\"x\", \"y\")\nshapes <- as_tibble(shapes)"},{"path":"clustering-analysis.html","id":"scatter-plots","chapter":"7 Clustering Analysis","heading":"7.5.2.1 Scatter plots","text":"first step visual inspection using scatter plots.Cluster tendency typically indicated several separated point clouds. Often appropriate number clusters can also visually obtained counting number point clouds. see four clusters, mouth convex/spherical thus pose problems algorithms like k-means.data two features can use pairs plot (scatterplot matrix) look scatterplot first two principal components using PCA.\n#### Visual Analysis Cluster Tendency Assessment (VAT)VAT reorders \nobjects show potential clustering tendency block structure\n(dark blocks along main diagonal). scale data using Euclidean distance.iVAT uses largest distances possible paths two objects\ninstead direct distances make block structure better visible.","code":"\nggplot(shapes, aes(x = x, y = y)) + geom_point()\nlibrary(seriation)\n\nd_shapes <- dist(scale(shapes))\nVAT(d_shapes, col = bluered(100))\niVAT(d_shapes, col = bluered(100))"},{"path":"clustering-analysis.html","id":"hopkins-statistic","chapter":"7 Clustering Analysis","heading":"7.5.2.2 Hopkins statistic","text":"factoextra can also create VAT plot calculate Hopkins statistic assess clustering tendency. Hopkins statistic, sample size \\(n\\) drawn data compares nearest neighbor distribution simulated dataset drawn random uniform distribution (see detailed explanation). values >.5 indicates usually clustering tendency.plots show strong cluster structure 4 clusters.","code":"\nget_clust_tendency(shapes, n = 10)## $hopkins_stat\n## [1] 0.907\n## \n## $plot"},{"path":"clustering-analysis.html","id":"data-without-clustering-tendency","chapter":"7 Clustering Analysis","heading":"7.5.2.3 Data Without Clustering Tendency","text":"point clouds visible, just noise.little clustering structure visible indicating low clustering tendency clustering performed data. However, k-means can used partition data \\(k\\) regions roughly equivalent size. can used data-driven discretization space.","code":"\ndata_random <- tibble(x = runif(500), y = runif(500))\nggplot(data_random, aes(x, y)) + geom_point()\nd_random <- dist(data_random)\nVAT(d_random, col = bluered(100))\niVAT(d_random, col = bluered(100))\nget_clust_tendency(data_random, n = 10, graph = FALSE)## $hopkins_stat\n## [1] 0.464\n## \n## $plot\n## NULL"},{"path":"clustering-analysis.html","id":"k-means-on-data-without-clustering-tendency","chapter":"7 Clustering Analysis","heading":"7.5.2.4 k-means on Data Without Clustering Tendency","text":"happens perform k-means data inherent clustering structure?k-means discretizes space similarly sized regions.","code":"\nkm <- kmeans(data_random, centers = 4)\nrandom_clustered<- data_random %>% add_column(cluster = factor(km$cluster))\nggplot(random_clustered, aes(x = x, y = y, color = cluster)) + geom_point()"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
