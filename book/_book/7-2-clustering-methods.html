<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="7.2 Clustering methods | R Code Companion for the Textbook Introduction to Data Mining" />
<meta property="og:type" content="book" />


<meta property="og:description" content="This book contains documented R examples to accompany several chapters of the popular data mining text book Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach and Vipin Kumar (2006 or 2017 edition)." />


<meta name="author" content="Michael Hahsler" />

<meta name="date" content="2021-07-09" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="This book contains documented R examples to accompany several chapters of the popular data mining text book Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach and Vipin Kumar (2006 or 2017 edition).">

<title>7.2 Clustering methods | R Code Companion for the Textbook Introduction to Data Mining</title>

<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap/css/united.min.css" rel="stylesheet" />
<script src="libs/bootstrap/js/bootstrap.min.js"></script>
<script src="libs/bootstrap/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation/tabsets.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/plotly-binding/plotly.js"></script>
<script src="libs/typedarray/typedarray.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main/plotly-latest.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider/jquery.nouislider.min.js"></script>
<link href="libs/selectize/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize/selectize.min.js"></script>
<link href="libs/vis/vis.css" rel="stylesheet" />
<script src="libs/vis/vis.min.js"></script>
<script src="libs/visNetwork-binding/visNetwork.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="1-introduction.html#introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="1-1-data-manipulation-with-tidyverse.html#data-manipulation-with-tidyverse"><span class="toc-section-number">1.1</span> Data Manipulation with Tidyverse</a></li>
<li><a href="1-2-visualization-with-ggplot2.html#visualization-with-ggplot2"><span class="toc-section-number">1.2</span> Visualization with ggplot2</a></li>
</ul></li>
<li><a href="2-data.html#data"><span class="toc-section-number">2</span> Data</a>
<ul>
<li><a href="2-1-the-iris-dataset.html#the-iris-dataset"><span class="toc-section-number">2.1</span> The Iris Dataset</a></li>
<li><a href="2-2-data-quality.html#data-quality"><span class="toc-section-number">2.2</span> Data Quality</a></li>
<li><a href="2-3-aggregation.html#aggregation"><span class="toc-section-number">2.3</span> Aggregation</a></li>
<li><a href="2-4-sampling.html#sampling"><span class="toc-section-number">2.4</span> Sampling</a>
<ul>
<li><a href="2-4-sampling.html#random-sampling"><span class="toc-section-number">2.4.1</span> Random Sampling</a></li>
<li><a href="2-4-sampling.html#stratified-sampling"><span class="toc-section-number">2.4.2</span> Stratified Sampling</a></li>
</ul></li>
<li><a href="2-5-features.html#features"><span class="toc-section-number">2.5</span> Features</a>
<ul>
<li><a href="2-5-features.html#dimensionality-reduction"><span class="toc-section-number">2.5.1</span> Dimensionality Reduction</a></li>
<li><a href="2-5-features.html#feature-selection"><span class="toc-section-number">2.5.2</span> Feature Selection</a></li>
<li><a href="2-5-features.html#discretize-features"><span class="toc-section-number">2.5.3</span> Discretize Features</a></li>
<li><a href="2-5-features.html#standardize-data-z-scores"><span class="toc-section-number">2.5.4</span> Standardize Data (Z-Scores)</a></li>
</ul></li>
<li><a href="2-6-proximities-similarities-and-distances.html#proximities-similarities-and-distances"><span class="toc-section-number">2.6</span> Proximities: Similarities and Distances</a>
<ul>
<li><a href="2-6-proximities-similarities-and-distances.html#minkowsky-distances"><span class="toc-section-number">2.6.1</span> Minkowsky Distances</a></li>
<li><a href="2-6-proximities-similarities-and-distances.html#distances-for-binary-data"><span class="toc-section-number">2.6.2</span> Distances for Binary Data</a></li>
<li><a href="2-6-proximities-similarities-and-distances.html#distances-for-mixed-data"><span class="toc-section-number">2.6.3</span> Distances for Mixed Data</a></li>
<li><a href="2-6-proximities-similarities-and-distances.html#additional-proximity-measures-available-in-package-proxy"><span class="toc-section-number">2.6.4</span> Additional proximity Measures Available in Package proxy</a></li>
</ul></li>
<li><a href="2-7-relationships-between-features.html#relationships-between-features"><span class="toc-section-number">2.7</span> Relationships Between Features</a>
<ul>
<li><a href="2-7-relationships-between-features.html#correlation"><span class="toc-section-number">2.7.1</span> Correlation</a></li>
<li><a href="2-7-relationships-between-features.html#rank-correlation"><span class="toc-section-number">2.7.2</span> Rank Correlation</a></li>
<li><a href="2-7-relationships-between-features.html#relationship-between-nominal-and-ordinal-features"><span class="toc-section-number">2.7.3</span> Relationship Between Nominal and Ordinal Features</a></li>
</ul></li>
<li><a href="2-8-density-estimation.html#density-estimation"><span class="toc-section-number">2.8</span> Density Estimation</a></li>
<li><a href="2-9-exploring-data.html#exploring-data"><span class="toc-section-number">2.9</span> Exploring Data</a>
<ul>
<li><a href="2-9-exploring-data.html#basic-statistics"><span class="toc-section-number">2.9.1</span> Basic statistics</a></li>
<li><a href="2-9-exploring-data.html#tabulate-data"><span class="toc-section-number">2.9.2</span> Tabulate data</a></li>
<li><a href="2-9-exploring-data.html#percentiles-quantiles"><span class="toc-section-number">2.9.3</span> Percentiles (Quantiles)</a></li>
</ul></li>
<li><a href="2-10-visualization.html#visualization"><span class="toc-section-number">2.10</span> Visualization</a>
<ul>
<li><a href="2-10-visualization.html#histogram"><span class="toc-section-number">2.10.1</span> Histogram</a></li>
<li><a href="2-10-visualization.html#boxplot"><span class="toc-section-number">2.10.2</span> Boxplot</a></li>
<li><a href="2-10-visualization.html#scatter-plot"><span class="toc-section-number">2.10.3</span> Scatter plot</a></li>
<li><a href="2-10-visualization.html#scatter-plot-matrix"><span class="toc-section-number">2.10.4</span> Scatter Plot Matrix</a></li>
<li><a href="2-10-visualization.html#data-matrix-visualization"><span class="toc-section-number">2.10.5</span> Data Matrix Visualization</a></li>
<li><a href="2-10-visualization.html#correlation-matrix"><span class="toc-section-number">2.10.6</span> Correlation Matrix</a></li>
<li><a href="2-10-visualization.html#parallel-coordinates-plot"><span class="toc-section-number">2.10.7</span> Parallel Coordinates Plot</a></li>
</ul></li>
</ul></li>
<li><a href="3-classification-basic-concepts-and-techniques.html#classification-basic-concepts-and-techniques"><span class="toc-section-number">3</span> Classification: Basic Concepts and Techniques</a>
<ul>
<li><a href="3-1-the-zoo-dataset.html#the-zoo-dataset"><span class="toc-section-number">3.1</span> The Zoo Dataset</a></li>
<li><a href="3-2-decision-trees.html#decision-trees"><span class="toc-section-number">3.2</span> Decision Trees</a>
<ul>
<li><a href="3-2-decision-trees.html#create-tree-with-default-settings-uses-pre-pruning"><span class="toc-section-number">3.2.1</span> Create Tree With Default Settings (uses pre-pruning)</a></li>
<li><a href="3-2-decision-trees.html#create-a-full-tree"><span class="toc-section-number">3.2.2</span> Create a Full Tree</a></li>
<li><a href="3-2-decision-trees.html#make-predictions-for-new-data"><span class="toc-section-number">3.2.3</span> Make Predictions for New Data</a></li>
</ul></li>
<li><a href="3-3-model-evaluation-with-caret.html#model-evaluation-with-caret"><span class="toc-section-number">3.3</span> Model Evaluation with Caret</a>
<ul>
<li><a href="3-3-model-evaluation-with-caret.html#hold-out-test-data"><span class="toc-section-number">3.3.1</span> Hold out Test Data</a></li>
<li><a href="3-3-model-evaluation-with-caret.html#learn-a-model-and-tune-hyperparameters-on-the-training-data"><span class="toc-section-number">3.3.2</span> Learn a Model and Tune Hyperparameters on the Training Data</a></li>
</ul></li>
<li><a href="3-4-testing-confusion-matrix-and-confidence-interval-for-accuracy.html#testing-confusion-matrix-and-confidence-interval-for-accuracy"><span class="toc-section-number">3.4</span> Testing: Confusion Matrix and Confidence Interval for Accuracy</a></li>
<li><a href="3-5-model-comparison.html#model-comparison"><span class="toc-section-number">3.5</span> Model Comparison</a></li>
<li><a href="3-6-feature-selection-and-feature-preparation.html#feature-selection-and-feature-preparation"><span class="toc-section-number">3.6</span> Feature Selection and Feature Preparation</a>
<ul>
<li><a href="3-6-feature-selection-and-feature-preparation.html#univariate-feature-importance-score"><span class="toc-section-number">3.6.1</span> Univariate Feature Importance Score</a></li>
<li><a href="3-6-feature-selection-and-feature-preparation.html#feature-subset-selection"><span class="toc-section-number">3.6.2</span> Feature Subset Selection</a></li>
<li><a href="3-6-feature-selection-and-feature-preparation.html#using-dummy-variables-for-factors"><span class="toc-section-number">3.6.3</span> Using Dummy Variables for Factors</a></li>
</ul></li>
<li><a href="3-7-class-imbalance.html#class-imbalance"><span class="toc-section-number">3.7</span> Class Imbalance</a>
<ul>
<li><a href="3-7-class-imbalance.html#option-1-use-the-data-as-is-and-hope-for-the-best"><span class="toc-section-number">3.7.1</span> Option 1: Use the Data As Is and Hope For The Best</a></li>
<li><a href="3-7-class-imbalance.html#option-2-balance-data-with-resampling"><span class="toc-section-number">3.7.2</span> Option 2: Balance Data With Resampling</a></li>
<li><a href="3-7-class-imbalance.html#option-3-build-a-larger-tree-and-use-predicted-probabilities"><span class="toc-section-number">3.7.3</span> Option 3: Build A Larger Tree and use Predicted Probabilities</a></li>
<li><a href="3-7-class-imbalance.html#option-4-use-a-cost-sensitive-classifier"><span class="toc-section-number">3.7.4</span> Option 4: Use a Cost-Sensitive Classifier</a></li>
</ul></li>
</ul></li>
<li><a href="4-classification-alternative-techniques.html#classification-alternative-techniques"><span class="toc-section-number">4</span> Classification: Alternative Techniques</a>
<ul>
<li><a href="4-1-training-and-test-data.html#training-and-test-data"><span class="toc-section-number">4.1</span> Training and Test Data</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#fitting-different-classification-models-to-the-training-data"><span class="toc-section-number">4.2</span> Fitting Different Classification Models to the Training Data</a>
<ul>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#conditional-inference-tree-decision-tree"><span class="toc-section-number">4.2.1</span> Conditional Inference Tree (Decision Tree)</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#c-4.5-decision-tree"><span class="toc-section-number">4.2.2</span> C 4.5 Decision Tree</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#k-nearest-neighbors"><span class="toc-section-number">4.2.3</span> K-Nearest Neighbors</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#part-rule-based-classifier"><span class="toc-section-number">4.2.4</span> PART (Rule-based classifier)</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#linear-support-vector-machines"><span class="toc-section-number">4.2.5</span> Linear Support Vector Machines</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#random-forest"><span class="toc-section-number">4.2.6</span> Random Forest</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#gradient-boosted-decision-trees-xgboost"><span class="toc-section-number">4.2.7</span> Gradient Boosted Decision Trees (xgboost)</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#artificial-neural-network"><span class="toc-section-number">4.2.8</span> Artificial Neural Network</a></li>
</ul></li>
<li><a href="4-3-comparing-models.html#comparing-models"><span class="toc-section-number">4.3</span> Comparing Models</a></li>
<li><a href="4-4-applying-the-chosen-model-to-the-test-data.html#applying-the-chosen-model-to-the-test-data"><span class="toc-section-number">4.4</span> Applying the Chosen Model to the Test Data</a></li>
<li><a href="4-5-decision-boundaries.html#decision-boundaries"><span class="toc-section-number">4.5</span> Decision Boundaries</a>
<ul>
<li><a href="4-5-decision-boundaries.html#iris-dataset"><span class="toc-section-number">4.5.1</span> Iris Dataset</a></li>
<li><a href="4-5-decision-boundaries.html#circle-dataset"><span class="toc-section-number">4.5.2</span> Circle Dataset</a></li>
</ul></li>
<li><a href="4-6-more-information.html#more-information"><span class="toc-section-number">4.6</span> More Information</a></li>
</ul></li>
<li><a href="5-association-analysis-basic-concepts-and-algorithms.html#association-analysis-basic-concepts-and-algorithms"><span class="toc-section-number">5</span> Association Analysis: Basic Concepts and Algorithms</a>
<ul>
<li><a href="5-1-the-arules-package.html#the-arules-package"><span class="toc-section-number">5.1</span> The arules Package</a></li>
<li><a href="5-2-transactions.html#transactions"><span class="toc-section-number">5.2</span> Transactions</a>
<ul>
<li><a href="5-2-transactions.html#create-transactions"><span class="toc-section-number">5.2.1</span> Create Transactions</a></li>
<li><a href="5-2-transactions.html#inspect-transactions"><span class="toc-section-number">5.2.2</span> Inspect Transactions</a></li>
<li><a href="5-2-transactions.html#vertical-layout-transaction-id-lists"><span class="toc-section-number">5.2.3</span> Vertical Layout (Transaction ID Lists)</a></li>
</ul></li>
<li><a href="5-3-frequent-itemsets.html#frequent-itemsets"><span class="toc-section-number">5.3</span> Frequent Itemsets</a>
<ul>
<li><a href="5-3-frequent-itemsets.html#mine-frequent-itemsets"><span class="toc-section-number">5.3.1</span> Mine Frequent Itemsets</a></li>
<li><a href="5-3-frequent-itemsets.html#concise-representation-of-itemsets"><span class="toc-section-number">5.3.2</span> Concise Representation of Itemsets</a></li>
</ul></li>
<li><a href="5-4-association-rules.html#association-rules"><span class="toc-section-number">5.4</span> Association Rules</a>
<ul>
<li><a href="5-4-association-rules.html#mine-association-rules"><span class="toc-section-number">5.4.1</span> Mine Association Rules</a></li>
<li><a href="5-4-association-rules.html#calculate-additional-interest-measures"><span class="toc-section-number">5.4.2</span> Calculate Additional Interest Measures</a></li>
<li><a href="5-4-association-rules.html#mine-using-templates"><span class="toc-section-number">5.4.3</span> Mine Using Templates</a></li>
</ul></li>
<li><a href="5-5-association-rule-visualization.html#association-rule-visualization"><span class="toc-section-number">5.5</span> Association Rule Visualization</a></li>
<li><a href="5-6-interactive-visualizations.html#interactive-visualizations"><span class="toc-section-number">5.6</span> Interactive Visualizations</a>
<ul>
<li><a href="5-6-interactive-visualizations.html#interactive-inspect-with-sorting-filtering-and-paging"><span class="toc-section-number">5.6.1</span> Interactive Inspect With Sorting, Filtering and Paging</a></li>
<li><a href="5-6-interactive-visualizations.html#scatter-plot-1"><span class="toc-section-number">5.6.2</span> Scatter Plot</a></li>
<li><a href="5-6-interactive-visualizations.html#matrix-visualization"><span class="toc-section-number">5.6.3</span> Matrix Visualization</a></li>
<li><a href="5-6-interactive-visualizations.html#visualization-as-graph"><span class="toc-section-number">5.6.4</span> Visualization as Graph</a></li>
<li><a href="5-6-interactive-visualizations.html#interactive-rule-explorer"><span class="toc-section-number">5.6.5</span> Interactive Rule Explorer</a></li>
</ul></li>
</ul></li>
<li><a href="6-association-analysis-advanced-concepts.html#association-analysis-advanced-concepts"><span class="toc-section-number">6</span> Association Analysis: Advanced Concepts</a></li>
<li><a href="7-clustering-analysis.html#clustering-analysis"><span class="toc-section-number">7</span> Clustering Analysis</a>
<ul>
<li><a href="7-1-data-preparation.html#data-preparation"><span class="toc-section-number">7.1</span> Data Preparation</a>
<ul>
<li><a href="7-1-data-preparation.html#data-cleaning"><span class="toc-section-number">7.1.1</span> Data cleaning</a></li>
<li><a href="7-1-data-preparation.html#scale-data"><span class="toc-section-number">7.1.2</span> Scale data</a></li>
</ul></li>
<li><a href="7-2-clustering-methods.html#clustering-methods"><span class="toc-section-number">7.2</span> Clustering methods</a>
<ul>
<li><a href="7-2-clustering-methods.html#k-means-clustering"><span class="toc-section-number">7.2.1</span> k-means Clustering</a></li>
<li><a href="7-2-clustering-methods.html#hierarchical-clustering"><span class="toc-section-number">7.2.2</span> Hierarchical Clustering</a></li>
<li><a href="7-2-clustering-methods.html#density-based-clustering-with-dbscan"><span class="toc-section-number">7.2.3</span> Density-based clustering with DBSCAN</a></li>
<li><a href="7-2-clustering-methods.html#partitioning-around-medoids-pam"><span class="toc-section-number">7.2.4</span> Partitioning Around Medoids (PAM)</a></li>
<li><a href="7-2-clustering-methods.html#gaussian-mixture-models"><span class="toc-section-number">7.2.5</span> Gaussian Mixture Models</a></li>
<li><a href="7-2-clustering-methods.html#spectral-clustering"><span class="toc-section-number">7.2.6</span> Spectral clustering</a></li>
<li><a href="7-2-clustering-methods.html#fuzzy-c-means-clustering"><span class="toc-section-number">7.2.7</span> Fuzzy C-Means Clustering</a></li>
</ul></li>
<li><a href="7-3-internal-cluster-validation.html#internal-cluster-validation"><span class="toc-section-number">7.3</span> Internal Cluster Validation</a>
<ul>
<li><a href="7-3-internal-cluster-validation.html#compare-the-clustering-quality"><span class="toc-section-number">7.3.1</span> Compare the Clustering Quality</a></li>
<li><a href="7-3-internal-cluster-validation.html#silhouette-plot"><span class="toc-section-number">7.3.2</span> Silhouette plot</a></li>
<li><a href="7-3-internal-cluster-validation.html#find-optimal-number-of-clusters-for-k-means"><span class="toc-section-number">7.3.3</span> Find Optimal Number of Clusters for k-means</a></li>
<li><a href="7-3-internal-cluster-validation.html#visualizing-the-distance-matrix"><span class="toc-section-number">7.3.4</span> Visualizing the Distance Matrix</a></li>
</ul></li>
<li><a href="7-4-external-cluster-validation.html#external-cluster-validation"><span class="toc-section-number">7.4</span> External Cluster Validation</a></li>
<li><a href="7-5-advanced-data-preparation-for-clustering.html#advanced-data-preparation-for-clustering"><span class="toc-section-number">7.5</span> Advanced Data Preparation for Clustering</a>
<ul>
<li><a href="7-5-advanced-data-preparation-for-clustering.html#outlier-removal"><span class="toc-section-number">7.5.1</span> Outlier Removal</a></li>
<li><a href="7-5-advanced-data-preparation-for-clustering.html#clustering-tendency"><span class="toc-section-number">7.5.2</span> Clustering Tendency</a></li>
</ul></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="clustering-methods" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Clustering methods</h2>
<div id="k-means-clustering" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> k-means Clustering</h3>
<p><a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means</a> implicitly assumes Euclidean distances. We use <span class="math inline">\(k = 4\)</span> clusters and run the algorithm 10 times with random initialized centroids. The best result is returned.</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb657-1"><a href="7-2-clustering-methods.html#cb657-1" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(ruspini_scaled, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb657-2"><a href="7-2-clustering-methods.html#cb657-2" aria-hidden="true" tabindex="-1"></a>km</span></code></pre></div>
<pre><code>## K-means clustering with 4 clusters of sizes 17, 23, 15, 20
## 
## Cluster means:
##        x      y
## 1  1.419  0.469
## 2 -0.360  1.109
## 3  0.461 -1.491
## 4 -1.139 -0.556
## 
## Clustering vector:
##  [1] 2 4 4 3 3 4 1 2 2 2 3 2 3 4 1 3 4 4 1 4 2 3 1 2 1
## [26] 2 3 1 3 2 1 4 3 4 4 1 1 2 1 4 2 1 2 2 4 3 3 2 2 4
## [51] 1 2 4 2 3 4 2 3 2 4 1 4 4 1 1 2 4 3 1 2 2 3 4 1 2
## 
## Within cluster sum of squares by cluster:
## [1] 3.64 2.66 1.08 2.71
##  (between_SS / total_SS =  93.2 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;       
## [4] &quot;withinss&quot;     &quot;tot.withinss&quot; &quot;betweenss&quot;   
## [7] &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<p><code>km</code> is an R object implemented as a list. The clustering vector contains the
cluster assignment for each data row and can be accessed using <code>km$cluster</code>. I add the
cluster assignment as a column to the scaled dataset (I make it a factor since it represents a
nominal label).</p>
<div class="sourceCode" id="cb659"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb659-1"><a href="7-2-clustering-methods.html#cb659-1" aria-hidden="true" tabindex="-1"></a>ruspini_clustered <span class="ot">&lt;-</span> ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster))</span>
<span id="cb659-2"><a href="7-2-clustering-methods.html#cb659-2" aria-hidden="true" tabindex="-1"></a>ruspini_clustered</span></code></pre></div>
<pre><code>## # A tibble: 75 x 3
##          x      y cluster
##      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  
##  1 -0.553   1.05  2      
##  2 -0.816  -0.822 4      
##  3 -1.08   -0.370 4      
##  4  0.496  -1.81  3      
##  5  0.725  -1.64  3      
##  6 -1.21   -0.637 4      
##  7  0.987   0.472 1      
##  8 -0.685   1.01  2      
##  9 -0.0616  1.07  2      
## 10 -0.652   1.25  2      
## # … with 65 more rows</code></pre>
<div class="sourceCode" id="cb661"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb661-1"><a href="7-2-clustering-methods.html#cb661-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_clustered, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-237-1.png" width="672" /></p>
<p>Add the centroids to the plot.</p>
<div class="sourceCode" id="cb662"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb662-1"><a href="7-2-clustering-methods.html#cb662-1" aria-hidden="true" tabindex="-1"></a>centroids <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(km<span class="sc">$</span>centers, <span class="at">rownames =</span> <span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb662-2"><a href="7-2-clustering-methods.html#cb662-2" aria-hidden="true" tabindex="-1"></a>centroids</span></code></pre></div>
<pre><code>## # A tibble: 4 x 3
##   cluster      x      y
##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 1        1.42   0.469
## 2 2       -0.360  1.11 
## 3 3        0.461 -1.49 
## 4 4       -1.14  -0.556</code></pre>
<div class="sourceCode" id="cb664"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb664-1"><a href="7-2-clustering-methods.html#cb664-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_clustered, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb664-2"><a href="7-2-clustering-methods.html#cb664-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> centroids, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster), <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">size =</span> <span class="dv">10</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-238-1.png" width="672" /></p>
<p>Use the <code>factoextra</code> package for visualization</p>
<div class="sourceCode" id="cb665"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb665-1"><a href="7-2-clustering-methods.html#cb665-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb665-2"><a href="7-2-clustering-methods.html#cb665-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(km, <span class="at">data =</span> ruspini_scaled, <span class="at">centroids =</span> <span class="cn">TRUE</span>, <span class="at">repel =</span> <span class="cn">TRUE</span>, <span class="at">ellipse.type =</span> <span class="st">&quot;norm&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: ggrepel: 10 unlabeled data points (too many
## overlaps). Consider increasing max.overlaps</code></pre>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-239-1.png" width="672" /></p>
<div id="inspect-clusters" class="section level4" number="7.2.1.1">
<h4><span class="header-section-number">7.2.1.1</span> Inspect clusters</h4>
<p>We inspect the clusters created by the 4-cluster k-means solution. The following code can be adapted to be used for other clustering methods.</p>
<div id="cluster-profiles" class="section level5" number="7.2.1.1.1">
<h5><span class="header-section-number">7.2.1.1.1</span> Cluster Profiles</h5>
<p>Inspect the centroids with horizontal bar charts organized by cluster. To group the plots by cluster, we have to change the data format to the “long”-format using a pivot operation. I use colors to match the clusters in the scatter plots.</p>
<div class="sourceCode" id="cb667"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb667-1"><a href="7-2-clustering-methods.html#cb667-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">pivot_longer</span>(centroids, <span class="at">cols =</span> <span class="fu">c</span>(x, y), <span class="at">names_to =</span> <span class="st">&quot;feature&quot;</span>),</span>
<span id="cb667-2"><a href="7-2-clustering-methods.html#cb667-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> feature, <span class="at">fill =</span> cluster)) <span class="sc">+</span></span>
<span id="cb667-3"><a href="7-2-clustering-methods.html#cb667-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb667-4"><a href="7-2-clustering-methods.html#cb667-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="at">rows =</span> <span class="fu">vars</span>(cluster))</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-240-1.png" width="672" /></p>
</div>
<div id="extract-a-single-cluster" class="section level5" number="7.2.1.1.2">
<h5><span class="header-section-number">7.2.1.1.2</span> Extract a single cluster</h5>
<p>You need is to filter the rows corresponding to the cluster index. The next
example calculates summary statistics and then plots all data points of cluster 1.</p>
<div class="sourceCode" id="cb668"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb668-1"><a href="7-2-clustering-methods.html#cb668-1" aria-hidden="true" tabindex="-1"></a>cluster1 <span class="ot">&lt;-</span> ruspini_clustered <span class="sc">%&gt;%</span> <span class="fu">filter</span>(cluster <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb668-2"><a href="7-2-clustering-methods.html#cb668-2" aria-hidden="true" tabindex="-1"></a>cluster1</span></code></pre></div>
<pre><code>## # A tibble: 17 x 3
##        x      y cluster
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  
##  1 0.987 0.472  1      
##  2 1.74  0.492  1      
##  3 1.38  0.615  1      
##  4 0.758 0.0405 1      
##  5 1.97  0.513  1      
##  6 1.45  0.554  1      
##  7 1.51  0.472  1      
##  8 0.987 0.0816 1      
##  9 0.627 0.0816 1      
## 10 1.74  0.390  1      
## 11 2.04  0.472  1      
## 12 1.45  0.739  1      
## 13 1.84  0.698  1      
## 14 1.81  0.390  1      
## 15 1.02  0.821  1      
## 16 1.41  0.657  1      
## 17 1.41  0.492  1</code></pre>
<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb670-1"><a href="7-2-clustering-methods.html#cb670-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cluster1)</span></code></pre></div>
<pre><code>##        x               y         cluster
##  Min.   :0.627   Min.   :0.041   1:17   
##  1st Qu.:1.020   1st Qu.:0.390   2: 0   
##  Median :1.446   Median :0.492   3: 0   
##  Mean   :1.419   Mean   :0.469   4: 0   
##  3rd Qu.:1.741   3rd Qu.:0.615          
##  Max.   :2.037   Max.   :0.821</code></pre>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb672-1"><a href="7-2-clustering-methods.html#cb672-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(cluster1, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb672-2"><a href="7-2-clustering-methods.html#cb672-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>))</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-241-1.png" width="672" /></p>
<p>What happens if we try to cluster with 8 centers?</p>
<div class="sourceCode" id="cb673"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb673-1"><a href="7-2-clustering-methods.html#cb673-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">kmeans</span>(ruspini_scaled, <span class="at">centers =</span> <span class="dv">8</span>), <span class="at">data =</span> ruspini_scaled,</span>
<span id="cb673-2"><a href="7-2-clustering-methods.html#cb673-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">centroids =</span> <span class="cn">TRUE</span>,  <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="at">ellipse.type =</span> <span class="st">&quot;norm&quot;</span>)</span></code></pre></div>
<pre><code>## Too few points to calculate an ellipse</code></pre>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-242-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="hierarchical-clustering" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Hierarchical Clustering</h3>
<p>Hierarchical clustering starts with a distance matrix. <code>dist()</code> defaults to method=“Euclidean.” <strong>Note:</strong> Distance matrices become very large quickly (size and time complexity is <span class="math inline">\(O(n^2)\)</span> where <span class="math inline">\(n\)</span> is the number if data points). It is only possible to calculate and store the matrix for small data sets (maybe a few hundred thousand data points) in main memory. If your data is too large then you can use sampling.</p>
<div class="sourceCode" id="cb675"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb675-1"><a href="7-2-clustering-methods.html#cb675-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">dist</span>(ruspini_scaled)</span></code></pre></div>
<p><code>hclust()</code> implements <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">agglomerative hierarchical clustering</a>. We cluster using complete link.</p>
<div class="sourceCode" id="cb676"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb676-1"><a href="7-2-clustering-methods.html#cb676-1" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> <span class="st">&quot;complete&quot;</span>)</span></code></pre></div>
<p>Hierarchical clustering does not return cluster assignments but a dendrogram. The standard plot
function plots the dendrogram.</p>
<div class="sourceCode" id="cb677"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb677-1"><a href="7-2-clustering-methods.html#cb677-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-245-1.png" width="672" /></p>
<p>Use <code>factoextra</code> (ggplot version). We can specify the number of clusters to visualize how the dendrogram will be cut into clusters.</p>
<div class="sourceCode" id="cb678"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb678-1"><a href="7-2-clustering-methods.html#cb678-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_dend</span>(hc, <span class="at">k =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated.
## Please use `guides(&lt;scale&gt; = &quot;none&quot;)` instead.</code></pre>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-246-1.png" width="672" /></p>
<p>More plotting options for dendrograms, including plotting
parts of large dendrograms can be found <a href="https://rpubs.com/gaston/dendrograms">here.</a></p>
<p>Extract cluster assignments by cutting the dendrogram into four parts and add the cluster id to the data.</p>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb680-1"><a href="7-2-clustering-methods.html#cb680-1" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc, <span class="at">k =</span> <span class="dv">4</span>)</span>
<span id="cb680-2"><a href="7-2-clustering-methods.html#cb680-2" aria-hidden="true" tabindex="-1"></a>cluster_complete <span class="ot">&lt;-</span> ruspini_scaled <span class="sc">%&gt;%</span></span>
<span id="cb680-3"><a href="7-2-clustering-methods.html#cb680-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(clusters))</span>
<span id="cb680-4"><a href="7-2-clustering-methods.html#cb680-4" aria-hidden="true" tabindex="-1"></a>cluster_complete</span></code></pre></div>
<pre><code>## # A tibble: 75 x 3
##          x      y cluster
##      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  
##  1 -0.553   1.05  1      
##  2 -0.816  -0.822 2      
##  3 -1.08   -0.370 2      
##  4  0.496  -1.81  3      
##  5  0.725  -1.64  3      
##  6 -1.21   -0.637 2      
##  7  0.987   0.472 4      
##  8 -0.685   1.01  1      
##  9 -0.0616  1.07  1      
## 10 -0.652   1.25  1      
## # … with 65 more rows</code></pre>
<div class="sourceCode" id="cb682"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb682-1"><a href="7-2-clustering-methods.html#cb682-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(cluster_complete, <span class="fu">aes</span>(x, y, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb682-2"><a href="7-2-clustering-methods.html#cb682-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-247-1.png" width="672" /></p>
<p>Try 8 clusters (Note: <code>fviz_cluster</code> needs a list with data and the cluster labels for hclust)</p>
<div class="sourceCode" id="cb683"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb683-1"><a href="7-2-clustering-methods.html#cb683-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">list</span>(<span class="at">data =</span> ruspini_scaled, <span class="at">cluster =</span> <span class="fu">cutree</span>(hc, <span class="at">k =</span> <span class="dv">8</span>)), <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-248-1.png" width="672" /></p>
<p>Clustering with single link</p>
<div class="sourceCode" id="cb684"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb684-1"><a href="7-2-clustering-methods.html#cb684-1" aria-hidden="true" tabindex="-1"></a>hc_single <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> <span class="st">&quot;single&quot;</span>)</span>
<span id="cb684-2"><a href="7-2-clustering-methods.html#cb684-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_dend</span>(hc_single, <span class="at">k =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated.
## Please use `guides(&lt;scale&gt; = &quot;none&quot;)` instead.</code></pre>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-249-1.png" width="672" /></p>
<div class="sourceCode" id="cb686"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb686-1"><a href="7-2-clustering-methods.html#cb686-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">list</span>(<span class="at">data =</span> ruspini_scaled, <span class="at">cluster =</span> <span class="fu">cutree</span>(hc_single, <span class="at">k =</span> <span class="dv">4</span>)), <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-249-2.png" width="672" /></p>
</div>
<div id="density-based-clustering-with-dbscan" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Density-based clustering with DBSCAN</h3>
<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb687-1"><a href="7-2-clustering-methods.html#cb687-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dbscan)</span></code></pre></div>
<p><a href="https://en.wikipedia.org/wiki/DBSCAN">DBSCAN</a> stands for “Density-Based Spatial Clustering of Applications with Noise.” It groups together points that are closely packed together and treats points in low-density regions as outliers.</p>
<p><strong>Parameters:</strong> minPts defines how many points in the epsilon neighborhood are needed to make a point
a core point. It is often chosen as a smoothing parameter. I use here minPts = 4.</p>
<p>To decide on epsilon, the knee in the kNN distance plot is often used. Note that minPts contains the point itself, while the k-nearest neighbor does not. We therefore have to use k = minPts - 1!
The knee is around eps = .32.</p>
<div class="sourceCode" id="cb688"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb688-1"><a href="7-2-clustering-methods.html#cb688-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kNNdistplot</span>(ruspini_scaled, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb688-2"><a href="7-2-clustering-methods.html#cb688-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> .<span class="dv">32</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-251-1.png" width="672" /></p>
<p>run dbscan</p>
<div class="sourceCode" id="cb689"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb689-1"><a href="7-2-clustering-methods.html#cb689-1" aria-hidden="true" tabindex="-1"></a>db <span class="ot">&lt;-</span> <span class="fu">dbscan</span>(ruspini_scaled, <span class="at">eps =</span> .<span class="dv">32</span>, <span class="at">minPts =</span> <span class="dv">4</span>)</span>
<span id="cb689-2"><a href="7-2-clustering-methods.html#cb689-2" aria-hidden="true" tabindex="-1"></a>db</span></code></pre></div>
<pre><code>## DBSCAN clustering for 75 objects.
## Parameters: eps = 0.32, minPts = 4
## The clustering contains 4 cluster(s) and 5 noise points.
## 
##  0  1  2  3  4 
##  5 23 20 15 12 
## 
## Available fields: cluster, eps, minPts</code></pre>
<div class="sourceCode" id="cb691"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb691-1"><a href="7-2-clustering-methods.html#cb691-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(db)</span></code></pre></div>
<pre><code>## List of 3
##  $ cluster: int [1:75] 1 2 2 3 3 2 0 1 1 1 ...
##  $ eps    : num 0.32
##  $ minPts : num 4
##  - attr(*, &quot;class&quot;)= chr [1:2] &quot;dbscan_fast&quot; &quot;dbscan&quot;</code></pre>
<div class="sourceCode" id="cb693"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb693-1"><a href="7-2-clustering-methods.html#cb693-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(db<span class="sc">$</span>cluster)),</span>
<span id="cb693-2"><a href="7-2-clustering-methods.html#cb693-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(x, y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-252-1.png" width="672" /></p>
<p><strong>Note:</strong> Cluster 0 represents outliers).</p>
<div class="sourceCode" id="cb694"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb694-1"><a href="7-2-clustering-methods.html#cb694-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(db, ruspini_scaled, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-253-1.png" width="672" /></p>
<p>Play with eps (neighborhood size) and MinPts (minimum of points needed for core cluster)</p>
</div>
<div id="partitioning-around-medoids-pam" class="section level3" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Partitioning Around Medoids (PAM)</h3>
<p><a href="https://en.wikipedia.org/wiki/K-medoids">PAM</a> tries to solve the
<span class="math inline">\(k\)</span>-medoids problem.
The problem is similar to <span class="math inline">\(k\)</span>-means, but uses medoids instead of centroids to represent clusters. Like hierarchical clustering, it typically works with precomputed distance matrix.
An advantage is that you can use any distance metric not just Euclidean distances.
<strong>Note:</strong> The medoid is the most central data point in the middle of the cluster.</p>
<div class="sourceCode" id="cb695"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb695-1"><a href="7-2-clustering-methods.html#cb695-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;cluster&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     ruspini</code></pre>
<div class="sourceCode" id="cb698"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb698-1"><a href="7-2-clustering-methods.html#cb698-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">dist</span>(ruspini_scaled)</span>
<span id="cb698-2"><a href="7-2-clustering-methods.html#cb698-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(d)</span></code></pre></div>
<pre><code>##  &#39;dist&#39; num [1:2775] 1.89 1.51 3.04 2.98 1.81 ...
##  - attr(*, &quot;Size&quot;)= int 75
##  - attr(*, &quot;Diag&quot;)= logi FALSE
##  - attr(*, &quot;Upper&quot;)= logi FALSE
##  - attr(*, &quot;method&quot;)= chr &quot;Euclidean&quot;
##  - attr(*, &quot;call&quot;)= language dist(x = ruspini_scaled)</code></pre>
<div class="sourceCode" id="cb700"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb700-1"><a href="7-2-clustering-methods.html#cb700-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">pam</span>(d, <span class="at">k =</span> <span class="dv">4</span>)</span>
<span id="cb700-2"><a href="7-2-clustering-methods.html#cb700-2" aria-hidden="true" tabindex="-1"></a>p</span></code></pre></div>
<pre><code>## Medoids:
##      ID   
## [1,] 66 66
## [2,] 56 56
## [3,] 33 33
## [4,] 28 28
## Clustering vector:
##  [1] 1 2 2 3 3 2 4 1 1 1 3 1 3 2 4 3 2 2 4 2 1 3 4 1 4
## [26] 1 3 4 3 1 4 2 3 2 2 4 4 1 4 2 1 4 1 1 2 3 3 1 1 2
## [51] 4 1 2 1 3 2 1 3 1 2 4 2 2 4 4 1 2 3 4 1 1 3 2 4 1
## Objective function:
## build  swap 
## 0.442 0.319 
## 
## Available components:
## [1] &quot;medoids&quot;    &quot;id.med&quot;     &quot;clustering&quot; &quot;objective&quot; 
## [5] &quot;isolation&quot;  &quot;clusinfo&quot;   &quot;silinfo&quot;    &quot;diss&quot;      
## [9] &quot;call&quot;</code></pre>
<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb702-1"><a href="7-2-clustering-methods.html#cb702-1" aria-hidden="true" tabindex="-1"></a>ruspini_clustered <span class="ot">&lt;-</span> ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(p<span class="sc">$</span>cluster))</span>
<span id="cb702-2"><a href="7-2-clustering-methods.html#cb702-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb702-3"><a href="7-2-clustering-methods.html#cb702-3" aria-hidden="true" tabindex="-1"></a>medoids <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(ruspini_scaled[p<span class="sc">$</span>medoids, ], <span class="at">rownames =</span> <span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb702-4"><a href="7-2-clustering-methods.html#cb702-4" aria-hidden="true" tabindex="-1"></a>medoids</span></code></pre></div>
<pre><code>## # A tibble: 4 x 3
##   cluster      x      y
##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 1       -0.357  1.17 
## 2 2       -1.18  -0.555
## 3 3        0.463 -1.46 
## 4 4        1.45   0.554</code></pre>
<div class="sourceCode" id="cb704"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb704-1"><a href="7-2-clustering-methods.html#cb704-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_clustered, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb704-2"><a href="7-2-clustering-methods.html#cb704-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> medoids, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster), <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">size =</span> <span class="dv">10</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-254-1.png" width="672" /></p>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="7-2-clustering-methods.html#cb705-1" aria-hidden="true" tabindex="-1"></a><span class="do">## __Note:__ `fviz_cluster` needs the original data.</span></span>
<span id="cb705-2"><a href="7-2-clustering-methods.html#cb705-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">c</span>(p, <span class="fu">list</span>(<span class="at">data =</span> ruspini_scaled)), <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="at">ellipse.type =</span> <span class="st">&quot;norm&quot;</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-254-2.png" width="672" /></p>
</div>
<div id="gaussian-mixture-models" class="section level3" number="7.2.5">
<h3><span class="header-section-number">7.2.5</span> Gaussian Mixture Models</h3>
<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb706-1"><a href="7-2-clustering-methods.html#cb706-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mclust)</span></code></pre></div>
<pre><code>## Package &#39;mclust&#39; version 5.4.7
## Type &#39;citation(&quot;mclust&quot;)&#39; for citing this R package in publications.</code></pre>
<pre><code>## 
## Attaching package: &#39;mclust&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     map</code></pre>
<p><a href="https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model">Gaussian mixture models</a> assume that the data set is the
result of drawing data from a set of
Gaussian distributions where each distribution represents a cluster. Estimation algorithms try to identify the location parameters of the distributions and thus can be used to find clusters.
<code>Mclust()</code> uses Bayesian Information Criterion (BIC) to find the
number of clusters (model selection). BIC uses the likelihood and a
penalty term to guard against overfitting.</p>
<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb710-1"><a href="7-2-clustering-methods.html#cb710-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(ruspini_scaled)</span>
<span id="cb710-2"><a href="7-2-clustering-methods.html#cb710-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust EEI (diagonal, equal volume and shape) model
## with 5 components: 
## 
##  log-likelihood  n df  BIC  ICL
##           -91.3 75 16 -252 -252
## 
## Clustering table:
##  1  2  3  4  5 
## 23 20 15  3 14</code></pre>
<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb712-1"><a href="7-2-clustering-methods.html#cb712-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m, <span class="at">what =</span> <span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-256-1.png" width="672" /></p>
<p>Rerun with a fixed number of 4 clusters</p>
<div class="sourceCode" id="cb713"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb713-1"><a href="7-2-clustering-methods.html#cb713-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(ruspini_scaled, <span class="at">G=</span><span class="dv">4</span>)</span>
<span id="cb713-2"><a href="7-2-clustering-methods.html#cb713-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust EEI (diagonal, equal volume and shape) model
## with 4 components: 
## 
##  log-likelihood  n df  BIC  ICL
##            -102 75 13 -259 -259
## 
## Clustering table:
##  1  2  3  4 
## 23 20 15 17</code></pre>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb715-1"><a href="7-2-clustering-methods.html#cb715-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m, <span class="at">what =</span> <span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-257-1.png" width="672" /></p>
</div>
<div id="spectral-clustering" class="section level3" number="7.2.6">
<h3><span class="header-section-number">7.2.6</span> Spectral clustering</h3>
<p><a href="https://en.wikipedia.org/wiki/Spectral_clustering">Spectral clustering</a> works by embedding the data points of the partitioning problem into the subspace of the k largest eigenvectors of a normalized affinity/kernel matrix. Then uses a simple clustering method like k-means.</p>
<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb716-1"><a href="7-2-clustering-methods.html#cb716-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;kernlab&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;kernlab&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:scales&#39;:
## 
##     alpha</code></pre>
<pre><code>## The following object is masked from &#39;package:arules&#39;:
## 
##     size</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     alpha</code></pre>
<div class="sourceCode" id="cb722"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb722-1"><a href="7-2-clustering-methods.html#cb722-1" aria-hidden="true" tabindex="-1"></a>cluster_spec <span class="ot">&lt;-</span> <span class="fu">specc</span>(<span class="fu">as.matrix</span>(ruspini_scaled), <span class="at">centers =</span> <span class="dv">4</span>)</span>
<span id="cb722-2"><a href="7-2-clustering-methods.html#cb722-2" aria-hidden="true" tabindex="-1"></a>cluster_spec</span></code></pre></div>
<pre><code>## Spectral Clustering object of class &quot;specc&quot; 
## 
##  Cluster memberships: 
##  
## 1 3 3 2 2 3 4 1 1 1 2 1 2 3 4 2 3 3 4 3 1 2 4 1 4 1 2 4 2 1 4 3 2 3 3 4 4 1 4 3 1 4 1 1 3 2 2 1 1 3 4 1 3 1 2 3 1 2 1 3 4 3 3 4 4 1 3 2 4 1 1 2 3 4 1 
##  
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  41.7670067458421 
## 
## Centers:  
##        [,1]   [,2]
## [1,] -0.360  1.109
## [2,]  0.461 -1.491
## [3,] -1.139 -0.556
## [4,]  1.419  0.469
## 
## Cluster size:  
## [1] 23 15 20 17
## 
## Within-cluster sum of squares:  
## [1] 53.27 53.27  8.81 18.84</code></pre>
<div class="sourceCode" id="cb724"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb724-1"><a href="7-2-clustering-methods.html#cb724-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(cluster_spec)),</span>
<span id="cb724-2"><a href="7-2-clustering-methods.html#cb724-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(x, y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-258-1.png" width="672" /></p>
</div>
<div id="fuzzy-c-means-clustering" class="section level3" number="7.2.7">
<h3><span class="header-section-number">7.2.7</span> Fuzzy C-Means Clustering</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Fuzzy_clustering">fuzzy clustering</a> version of the k-means clustering problem. Each data point
has a degree of membership to for each cluster.</p>
<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb725-1"><a href="7-2-clustering-methods.html#cb725-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;e1071&quot;</span>)</span>
<span id="cb725-2"><a href="7-2-clustering-methods.html#cb725-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb725-3"><a href="7-2-clustering-methods.html#cb725-3" aria-hidden="true" tabindex="-1"></a>cluster_cmeans <span class="ot">&lt;-</span> <span class="fu">cmeans</span>(<span class="fu">as.matrix</span>(ruspini_scaled), <span class="at">centers =</span> <span class="dv">4</span>)</span>
<span id="cb725-4"><a href="7-2-clustering-methods.html#cb725-4" aria-hidden="true" tabindex="-1"></a>cluster_cmeans</span></code></pre></div>
<pre><code>## Fuzzy c-means clustering with 4 clusters
## 
## Cluster centers:
##        x      y
## 1 -1.137 -0.555
## 2  0.455 -1.476
## 3  1.505  0.516
## 4 -0.376  1.114
## 
## Memberships:
##              1        2        3        4
##  [1,] 0.012065 0.004750 7.76e-03 9.75e-01
##  [2,] 0.866509 0.074035 2.11e-02 3.84e-02
##  [3,] 0.971282 0.010239 4.91e-03 1.36e-02
##  [4,] 0.024935 0.947252 1.65e-02 1.14e-02
##  [5,] 0.020593 0.950361 1.82e-02 1.09e-02
##  [6,] 0.992095 0.003402 1.36e-03 3.14e-03
##  [7,] 0.039260 0.053619 8.11e-01 9.62e-02
##  [8,] 0.037605 0.013313 1.97e-02 9.29e-01
##  [9,] 0.024784 0.013940 3.40e-02 9.27e-01
## [10,] 0.025639 0.010355 1.73e-02 9.47e-01
## [11,] 0.008241 0.983990 4.42e-03 3.35e-03
## [12,] 0.001560 0.000705 1.32e-03 9.96e-01
## [13,] 0.003861 0.992177 2.30e-03 1.66e-03
## [14,] 0.768380 0.097124 4.14e-02 9.31e-02
## [15,] 0.005870 0.009963 9.73e-01 1.13e-02
## [16,] 0.024150 0.952363 1.34e-02 1.01e-02
## [17,] 0.828839 0.045276 2.77e-02 9.82e-02
## [18,] 0.904502 0.033979 1.64e-02 4.51e-02
## [19,] 0.003221 0.004747 9.85e-01 7.44e-03
## [20,] 0.934346 0.027260 1.15e-02 2.69e-02
## [21,] 0.003385 0.001497 2.77e-03 9.92e-01
## [22,] 0.020387 0.949234 1.93e-02 1.11e-02
## [23,] 0.107506 0.177387 5.41e-01 1.74e-01
## [24,] 0.011470 0.004817 8.41e-03 9.75e-01
## [25,] 0.018433 0.031839 9.16e-01 3.39e-02
## [26,] 0.004627 0.002182 4.27e-03 9.89e-01
## [27,] 0.003167 0.993633 1.85e-03 1.35e-03
## [28,] 0.000609 0.000943 9.97e-01 1.32e-03
## [29,] 0.028738 0.947019 1.34e-02 1.08e-02
## [30,] 0.071388 0.050971 1.76e-01 7.02e-01
## [31,] 0.000250 0.000411 9.99e-01 5.07e-04
## [32,] 0.939767 0.029086 1.01e-02 2.10e-02
## [33,] 0.000110 0.999766 7.43e-05 5.05e-05
## [34,] 0.860429 0.059383 2.50e-02 5.52e-02
## [35,] 0.895316 0.033633 1.80e-02 5.31e-02
## [36,] 0.065465 0.118857 7.06e-01 1.10e-01
## [37,] 0.128305 0.183755 4.70e-01 2.18e-01
## [38,] 0.011252 0.005928 1.35e-02 9.69e-01
## [39,] 0.007575 0.013540 9.65e-01 1.39e-02
## [40,] 0.890088 0.054964 1.83e-02 3.66e-02
## [41,] 0.067223 0.044821 1.33e-01 7.55e-01
## [42,] 0.022924 0.040523 8.96e-01 4.09e-02
## [43,] 0.009541 0.004635 9.54e-03 9.76e-01
## [44,] 0.048384 0.016805 2.45e-02 9.10e-01
## [45,] 0.914871 0.040505 1.46e-02 3.00e-02
## [46,] 0.049811 0.912543 2.04e-02 1.73e-02
## [47,] 0.038484 0.892180 4.59e-02 2.34e-02
## [48,] 0.004484 0.002237 4.75e-03 9.89e-01
## [49,] 0.015164 0.007890 1.73e-02 9.60e-01
## [50,] 0.872757 0.063345 2.13e-02 4.26e-02
## [51,] 0.006153 0.008725 9.70e-01 1.48e-02
## [52,] 0.075851 0.025668 3.63e-02 8.62e-01
## [53,] 0.942647 0.022073 9.90e-03 2.54e-02
## [54,] 0.041983 0.015519 2.38e-02 9.19e-01
## [55,] 0.017339 0.959100 1.45e-02 9.02e-03
## [56,] 0.998933 0.000436 1.84e-04 4.47e-04
## [57,] 0.020461 0.011470 2.85e-02 9.40e-01
## [58,] 0.018343 0.953743 1.78e-02 1.02e-02
## [59,] 0.037153 0.014629 2.37e-02 9.25e-01
## [60,] 0.962608 0.013809 6.49e-03 1.71e-02
## [61,] 0.013081 0.020545 9.40e-01 2.68e-02
## [62,] 0.930263 0.035820 1.14e-02 2.25e-02
## [63,] 0.954076 0.015519 7.84e-03 2.26e-02
## [64,] 0.010680 0.019237 9.51e-01 1.93e-02
## [65,] 0.039416 0.046127 7.88e-01 1.27e-01
## [66,] 0.000964 0.000451 8.88e-04 9.98e-01
## [67,] 0.973167 0.012776 4.51e-03 9.55e-03
## [68,] 0.025463 0.953144 1.19e-02 9.53e-03
## [69,] 0.003456 0.005041 9.83e-01 8.07e-03
## [70,] 0.010326 0.004135 6.88e-03 9.79e-01
## [71,] 0.033362 0.019994 5.51e-02 8.92e-01
## [72,] 0.003079 0.993497 2.03e-03 1.40e-03
## [73,] 0.887734 0.043108 2.00e-02 4.92e-02
## [74,] 0.001160 0.001840 9.95e-01 2.46e-03
## [75,] 0.092067 0.051905 1.05e-01 7.51e-01
## 
## Closest hard clustering:
##  [1] 4 1 1 2 2 1 3 4 4 4 2 4 2 1 3 2 1 1 3 1 4 2 3 4 3
## [26] 4 2 3 2 4 3 1 2 1 1 3 3 4 3 1 4 3 4 4 1 2 2 4 4 1
## [51] 3 4 1 4 2 1 4 2 4 1 3 1 1 3 3 4 1 2 3 4 4 2 1 3 4
## 
## Available components:
## [1] &quot;centers&quot;     &quot;size&quot;        &quot;cluster&quot;    
## [4] &quot;membership&quot;  &quot;iter&quot;        &quot;withinerror&quot;
## [7] &quot;call&quot;</code></pre>
<p>Plot membership (shown as small pie charts)</p>
<div class="sourceCode" id="cb727"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb727-1"><a href="7-2-clustering-methods.html#cb727-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;scatterpie&quot;</span>)</span>
<span id="cb727-2"><a href="7-2-clustering-methods.html#cb727-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()  <span class="sc">+</span></span>
<span id="cb727-3"><a href="7-2-clustering-methods.html#cb727-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_scatterpie</span>(<span class="at">data =</span> <span class="fu">cbind</span>(ruspini_scaled, cluster_cmeans<span class="sc">$</span>membership),</span>
<span id="cb727-4"><a href="7-2-clustering-methods.html#cb727-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">cols =</span> <span class="fu">colnames</span>(cluster_cmeans<span class="sc">$</span>membership), <span class="at">legend_name =</span> <span class="st">&quot;Membership&quot;</span>) <span class="sc">+</span> <span class="fu">coord_equal</span>()</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-260-1.png" width="672" /></p>
</div>
</div>
<p style="text-align: center;">
<a href="7-1-data-preparation.html"><button class="btn btn-default">Previous</button></a>
<a href="7-3-internal-cluster-validation.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
