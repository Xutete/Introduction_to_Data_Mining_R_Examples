[["index.html", "R Code Companion for the Textbook Introduction to Data Mining Preface", " R Code Companion for the Textbook Introduction to Data Mining Michael Hahsler 2021-07-09 Preface This book contains documented R examples to accompany several chapters of the popular data mining textbook Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach, Anuj Karpatne and Vipin Kumar (Tan, Steinbach, and Kumar 2005). The code can be used with either edition (2006 or 2017). The code examples collected in this book were developed for the course CS 7331 Data Mining held at SMU and will be regularly updated and improved. Please visit https://github.com/mhahsler/Introduction_to_Data_Mining_R_Examples to submit corrections or suggest improvements. The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Creative Commons License References "],["introduction.html", "Chapter 1 Introduction 1.1 Data Manipulation with Tidyverse 1.2 Visualization with ggplot2", " Chapter 1 Introduction Packages used for this chapter: ggplot2 (Wickham, Chang, et al. 2021), tidyverse (Wickham 2021c) This companion book assumes that you have R and RStudio Desktop installed and that you are familiar with how to run R code and install packages. The code in this book uses tidyverse to manipulate data and ggplot2 for visualization. library(tidyverse) ## ── Attaching packages ────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ───────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() 1.1 Data Manipulation with Tidyverse We use tidyverse tibbles to replace R’s built-in data.frames, the pipe operator %&gt;% to chain functions and data transformation functions like filter(), arrange(), select(), and mutate() provided by the tidyverse package dplyr. A good overview is given in the RStudio Data Transformation Cheat Sheet and an introduction can be found in the Section on Data Wrangling the book R for Data Science (Wickham and Grolemund 2017). 1.2 Visualization with ggplot2 The gg in ggplot2 stands for The Grammar of Graphics introduced by Leland Wilkinson (Wilkinson 2005). The main idea is that every graph is built from the same basic components: the data, a coordinate system, and visual marks representing the data (geoms). In ggplot2, the components are combined using the + operator. ggplot(data, mapping = aes(x = ..., y = ..., color = ...)) + geom_point() + coord_cartesian() Each geom_ function uses a stat_ function to calculate what is visualizes. For example, geom_bar uses stat_count to create a bar chart by counting how often each value appears in the data (see ? geom_bar). geom_point just uses the stat \"identity\" to display the points using the coordinates as they are. RStudio’s Data Visualization Cheat Sheet offers a comprehensive overview of available components. A good introduction can be found in the Chapter on Data Visualization of the book R for Data Science (Wickham and Grolemund 2017). References "],["data.html", "Chapter 2 Data 2.1 The Iris Dataset 2.2 Data Quality 2.3 Aggregation 2.4 Sampling 2.5 Features 2.6 Proximities: Similarities and Distances 2.7 Relationships Between Features 2.8 Density Estimation 2.9 Exploring Data 2.10 Visualization", " Chapter 2 Data Packages used for this chapter: arules (Hahsler et al. 2021), caret (Kuhn 2021), factoextra (Kassambara and Mundt 2020), GGally (Schloerke et al. 2021), plotly (Sievert et al. 2021), proxy (Meyer and Buchta 2021), sampling (Tillé and Matei 2021), seriation (Hahsler, Buchta, and Hornik 2021), tidyverse (Wickham 2021c) 2.1 The Iris Dataset We will use a toy dataset that comes with R. Fisher’s iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. For more details see: ? iris Load the iris data set and convert the data.frame into a tibble. Note: datasets that come with R or R packages can be loaded with data(). library(tidyverse) data(iris) iris &lt;- as_tibble(iris) iris ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # … with 140 more rows, and 1 more variable: ## # Species &lt;fct&gt; 2.2 Data Quality Inspect data (produce a scatterplot matrix using ggpairs from package GGally). Possibly you can see noise and ouliers. library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ggpairs(iris, aes(color = Species)) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. Get summary statistics for each column (outliers, missing values) summary(iris) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.00 Min. :1.00 ## 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 ## Median :5.80 Median :3.00 Median :4.35 ## Mean :5.84 Mean :3.06 Mean :3.76 ## 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 ## Max. :7.90 Max. :4.40 Max. :6.90 ## Petal.Width Species ## Min. :0.1 setosa :50 ## 1st Qu.:0.3 versicolor:50 ## Median :1.3 virginica :50 ## Mean :1.2 ## 3rd Qu.:1.8 ## Max. :2.5 just the mean iris %&gt;% summarize_if(is.numeric, mean) ## # A tibble: 1 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.84 3.06 3.76 1.20 Often you will do something like: clean.data &lt;- iris %&gt;% drop_na() %&gt;% unique() summary(clean.data) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.00 Min. :1.00 ## 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 ## Median :5.80 Median :3.00 Median :4.30 ## Mean :5.84 Mean :3.06 Mean :3.75 ## 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 ## Max. :7.90 Max. :4.40 Max. :6.90 ## Petal.Width Species ## Min. :0.10 setosa :50 ## 1st Qu.:0.30 versicolor:50 ## Median :1.30 virginica :49 ## Mean :1.19 ## 3rd Qu.:1.80 ## Max. :2.50 Note that one case (non-unique) is gone. All cases with missing values will also have been dropped. 2.3 Aggregation Aggregate by species. First group the data and then summarize each group. iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 ## 2 versicolor 5.94 2.77 4.26 ## 3 virginica 6.59 2.97 5.55 ## # … with 1 more variable: Petal.Width &lt;dbl&gt; iris %&gt;% group_by(Species) %&gt;% summarize_all(median) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5 3.4 1.5 ## 2 versicolor 5.9 2.8 4.35 ## 3 virginica 6.5 3 5.55 ## # … with 1 more variable: Petal.Width &lt;dbl&gt; 2.4 Sampling 2.4.1 Random Sampling Sample from a vector with replacement. sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), size = 10, replace = TRUE) ## [1] &quot;C&quot; &quot;B&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; Sampling rows from a tibble (I set the random number generator seed to make the results reproducible). set.seed(1000) s &lt;- iris %&gt;% sample_n(15) ggpairs(s, aes(color = Species)) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. 2.4.2 Stratified Sampling Stratified sampling is a method of sampling from a population which can be partitioned into subpopulations, while controlling the proportions of the subpopulation in the resulting sample. In the following, the subpopulations are the different types of species and we want to make sure to sample the same number (5) flowers from each. You need to install the package sampling with: install.packages(“sampling”) library(sampling) id2 &lt;- strata(iris, stratanames=&quot;Species&quot;, size=c(5,5,5), method=&quot;srswor&quot;) id2 ## Species ID_unit Prob Stratum ## 7 setosa 7 0.1 1 ## 9 setosa 9 0.1 1 ## 10 setosa 10 0.1 1 ## 24 setosa 24 0.1 1 ## 48 setosa 48 0.1 1 ## 58 versicolor 58 0.1 2 ## 62 versicolor 62 0.1 2 ## 74 versicolor 74 0.1 2 ## 78 versicolor 78 0.1 2 ## 99 versicolor 99 0.1 2 ## 106 virginica 106 0.1 3 ## 107 virginica 107 0.1 3 ## 127 virginica 127 0.1 3 ## 135 virginica 135 0.1 3 ## 145 virginica 145 0.1 3 s2 &lt;- iris %&gt;% slice(id2$ID_unit) ggpairs(s2, aes(color = Species)) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. 2.5 Features 2.5.1 Dimensionality Reduction 2.5.1.1 Principal Components Analysis (PCA) PCA calculates principal components (a new orthonormal basis vectors in the data space) from data points such that the first principal component explains the most variability in the data, the second the next most and so on. In data analysis, PCA is used to project high-dimensional data points onto the first few (typically two) principal components for visualization as a scatter plot and as preprocessing for modeling (e.g., before k-means clustering). Points that are closer together in the high-dimensional space, tend also be closer together in the lower-dimensional space, Look at the 3d data using an interactive 3d plot (needs package plotly). However, 3d plots are hard to print out and the iris data is actually in 4 dimensions. ##library(plotly) # I don&#39;t load the package because it&#39;s namespace clashes with select in dplyr. plotly::plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length, z = ~Sepal.Width, size = ~Petal.Width, color = ~Species, type=&quot;scatter3d&quot;) ## No scatter3d mode specifed: ## Setting the mode to markers ## Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode ## Warning: `line.width` does not currently support ## multiple values. ## Warning: `line.width` does not currently support ## multiple values. ## Warning: `line.width` does not currently support ## multiple values. Calculate the principal components using prcomp() pc &lt;- iris %&gt;% select(-Species) %&gt;% as.matrix() %&gt;% prcomp() summary(pc) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 2.056 0.4926 0.2797 0.15439 ## Proportion of Variance 0.925 0.0531 0.0171 0.00521 ## Cumulative Proportion 0.925 0.9777 0.9948 1.00000 How important is each principal component can also be seen using a scree plot. The plot shows how much variability in the data is explained by each additional principal component. plot(pc, type = &quot;line&quot;) Note: For the iris data, the first principal component (PC1) explains most of the variability in the data. Inspect the raw object (display structure) str(pc) ## List of 5 ## $ sdev : num [1:4] 2.056 0.493 0.28 0.154 ## $ rotation: num [1:4, 1:4] 0.3614 -0.0845 0.8567 0.3583 -0.6566 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## .. ..$ : chr [1:4] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## $ center : Named num [1:4] 5.84 3.06 3.76 1.2 ## ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## $ scale : logi FALSE ## $ x : num [1:150, 1:4] -2.68 -2.71 -2.89 -2.75 -2.73 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:4] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## - attr(*, &quot;class&quot;)= chr &quot;prcomp&quot; We can display the data points projected on the first two principal components. ggplot(as_tibble(pc$x), aes(x = PC1, y = PC2, color = iris$Species)) + geom_point() Since the first principal component represents most of the variability, we can also project the data only on PC1. ggplot(as_tibble(pc$x), aes(x = PC1, y = 0, color = iris$Species)) + geom_point() Plot the projected data and add the original axes as arrows. This is called a biplot. If old and new axes point roughly in the same direction, then they are correlated (linearly dependent). library(factoextra) ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa fviz_pca(pc) fviz_pca_var(pc) Another popular method to project data in lower dimensions for visualization is t-distributed stochastic neighbor embedding (t-SNE) available in package Rtsne. 2.5.1.2 Multi-Dimensional Scaling (MDS) MDS is similar to PCA. Instead of data points, it takes pairwise distances (i.e., a distance matrix) and produces a space where points are placed to represent these distances as well as possible. The axis in this space are called components and are similar to the principal components in PCA. Let’s calculate distances in the 4-d space of iris. d &lt;- iris %&gt;% select(-Species) %&gt;% dist() and do metric (classic) MDS to reconstruct a 2-d space. fit &lt;- cmdscale(d, k = 2) colnames(fit) &lt;- c(&quot;comp1&quot;, &quot;comp2&quot;) fit &lt;- as_tibble(fit) ggplot(fit, aes(x = comp1, y = comp2, color = iris$Species)) + geom_point() 2.5.1.3 Non-Parametric Multidimensional Scaling Non-parametric multidimensional scaling performs MDS while relaxing the need of linear relationships. Methods are available in package MASS as functions isoMDS and sammon. 2.5.2 Feature Selection We will talk about feature selection when we discuss classification models. 2.5.3 Discretize Features ggplot(iris, aes(x = Petal.Width, y = 1:150)) + geom_point() A histogram is a better visualization for the distribution of a single variable. ggplot(iris, aes(Petal.Width)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. Equal interval width iris %&gt;% pull(Sepal.Width) %&gt;% cut(breaks=3) ## [1] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [11] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (3.6,4.4] ## [16] (3.6,4.4] (3.6,4.4] (2.8,3.6] (3.6,4.4] (3.6,4.4] ## [21] (2.8,3.6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [26] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [31] (2.8,3.6] (2.8,3.6] (3.6,4.4] (3.6,4.4] (2.8,3.6] ## [36] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [41] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] (3.6,4.4] ## [46] (2.8,3.6] (3.6,4.4] (2.8,3.6] (3.6,4.4] (2.8,3.6] ## [51] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8] (2,2.8] ## [56] (2,2.8] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] ## [61] (2,2.8] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] ## [66] (2.8,3.6] (2.8,3.6] (2,2.8] (2,2.8] (2,2.8] ## [71] (2.8,3.6] (2,2.8] (2,2.8] (2,2.8] (2.8,3.6] ## [76] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] (2,2.8] ## [81] (2,2.8] (2,2.8] (2,2.8] (2,2.8] (2.8,3.6] ## [86] (2.8,3.6] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] ## [91] (2,2.8] (2.8,3.6] (2,2.8] (2,2.8] (2,2.8] ## [96] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8] (2,2.8] ## [101] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [106] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] (2.8,3.6] ## [111] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] (2,2.8] ## [116] (2.8,3.6] (2.8,3.6] (3.6,4.4] (2,2.8] (2,2.8] ## [121] (2.8,3.6] (2,2.8] (2,2.8] (2,2.8] (2.8,3.6] ## [126] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] (2.8,3.6] ## [131] (2,2.8] (3.6,4.4] (2,2.8] (2,2.8] (2,2.8] ## [136] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [141] (2.8,3.6] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] ## [146] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## Levels: (2,2.8] (2.8,3.6] (3.6,4.4] Other methods (equal frequency, k-means clustering, etc.) library(arules) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ## Attaching package: &#39;arules&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following objects are masked from &#39;package:base&#39;: ## ## abbreviate, write iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;interval&quot;, breaks = 3) ## [1] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [6] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [11] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [16] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [21] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [26] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [31] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [36] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [41] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [46] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [51] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [56] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [61] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [66] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [71] [1.7,2.5] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [76] [0.9,1.7) [0.9,1.7) [1.7,2.5] [0.9,1.7) [0.9,1.7) ## [81] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [86] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [91] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [96] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [101] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [106] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [111] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [116] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) ## [121] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [126] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) ## [131] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) [0.9,1.7) ## [136] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [141] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [146] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## attr(,&quot;discretized:breaks&quot;) ## [1] 0.1 0.9 1.7 2.5 ## attr(,&quot;discretized:method&quot;) ## [1] interval ## Levels: [0.1,0.9) [0.9,1.7) [1.7,2.5] iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;frequency&quot;, breaks = 3) ## [1] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [5] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [9] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [13] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [17] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [21] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [25] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [29] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [33] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [37] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [41] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [45] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [49] [0.1,0.867) [0.1,0.867) [0.867,1.6) [0.867,1.6) ## [53] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [57] [1.6,2.5] [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [61] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [65] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [69] [0.867,1.6) [0.867,1.6) [1.6,2.5] [0.867,1.6) ## [73] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [77] [0.867,1.6) [1.6,2.5] [0.867,1.6) [0.867,1.6) ## [81] [0.867,1.6) [0.867,1.6) [0.867,1.6) [1.6,2.5] ## [85] [0.867,1.6) [1.6,2.5] [0.867,1.6) [0.867,1.6) ## [89] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [93] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [97] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [101] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [105] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [109] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [113] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [117] [1.6,2.5] [1.6,2.5] [1.6,2.5] [0.867,1.6) ## [121] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [125] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [129] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [133] [1.6,2.5] [0.867,1.6) [0.867,1.6) [1.6,2.5] ## [137] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [141] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [145] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [149] [1.6,2.5] [1.6,2.5] ## attr(,&quot;discretized:breaks&quot;) ## [1] 0.1000 0.8667 1.6000 2.5000 ## attr(,&quot;discretized:method&quot;) ## [1] frequency ## Levels: [0.1,0.867) [0.867,1.6) [1.6,2.5] iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;cluster&quot;, breaks = 3) ## [1] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [4] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [7] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [10] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [13] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [16] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [19] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [22] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [25] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [28] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [31] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [34] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [37] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [40] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [43] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [46] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [49] [0.1,0.792) [0.1,0.792) [0.792,1.71) ## [52] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [55] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [58] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [61] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [64] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [67] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [70] [0.792,1.71) [1.71,2.5] [0.792,1.71) ## [73] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [76] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [79] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [82] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [85] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [88] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [91] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [94] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [97] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [100] [0.792,1.71) [1.71,2.5] [1.71,2.5] ## [103] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [106] [1.71,2.5] [0.792,1.71) [1.71,2.5] ## [109] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [112] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [115] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [118] [1.71,2.5] [1.71,2.5] [0.792,1.71) ## [121] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [124] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [127] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [130] [0.792,1.71) [1.71,2.5] [1.71,2.5] ## [133] [1.71,2.5] [0.792,1.71) [0.792,1.71) ## [136] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [139] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [142] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [145] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [148] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## attr(,&quot;discretized:breaks&quot;) ## [1] 0.1000 0.7915 1.7055 2.5000 ## attr(,&quot;discretized:method&quot;) ## [1] cluster ## Levels: [0.1,0.792) [0.792,1.71) [1.71,2.5] ggplot(iris, aes(Petal.Width)) + geom_histogram() + geom_vline(xintercept = iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;interval&quot;, breaks = 3, onlycuts = TRUE), color = &quot;blue&quot;) + labs(title = &quot;Discretization: interval&quot;, subtitle = &quot;Blue lines are boundaries&quot;) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ggplot(iris, aes(Petal.Width)) + geom_histogram() + geom_vline(xintercept = iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;frequency&quot;, breaks = 3, onlycuts = TRUE), color = &quot;blue&quot;) + labs(title = &quot;Discretization: frequency&quot;, subtitle = &quot;Blue lines are boundaries&quot;) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ggplot(iris, aes(Petal.Width)) + geom_histogram() + geom_vline(xintercept = iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;cluster&quot;, breaks = 3, onlycuts = TRUE), color = &quot;blue&quot;) + labs(title = &quot;Discretization: cluster&quot;, subtitle = &quot;Blue lines are boundaries&quot;) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. 2.5.4 Standardize Data (Z-Scores) Standardize the scale of features to make them comparable. For each column the mean is subtracted (centering) and it is divided by the standard deviation (scaling). Now most values should be in [-3,3]. Note: tidyverse currently does not have a simple scale function, so I make one that provides a wrapper for the standard scale function in R: scale_numeric &lt;- function(x) x %&gt;% mutate_if(is.numeric, function(y) as.vector(scale(y))) iris.scaled &lt;- iris %&gt;% scale_numeric() iris.scaled ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.898 1.02 -1.34 -1.31 ## 2 -1.14 -0.132 -1.34 -1.31 ## 3 -1.38 0.327 -1.39 -1.31 ## 4 -1.50 0.0979 -1.28 -1.31 ## 5 -1.02 1.25 -1.34 -1.31 ## 6 -0.535 1.93 -1.17 -1.05 ## 7 -1.50 0.786 -1.34 -1.18 ## 8 -1.02 0.786 -1.28 -1.31 ## 9 -1.74 -0.361 -1.34 -1.31 ## 10 -1.14 0.0979 -1.28 -1.44 ## # … with 140 more rows, and 1 more variable: ## # Species &lt;fct&gt; summary(iris.scaled) ## Sepal.Length Sepal.Width Petal.Length ## Min. :-1.8638 Min. :-2.426 Min. :-1.562 ## 1st Qu.:-0.8977 1st Qu.:-0.590 1st Qu.:-1.222 ## Median :-0.0523 Median :-0.132 Median : 0.335 ## Mean : 0.0000 Mean : 0.000 Mean : 0.000 ## 3rd Qu.: 0.6722 3rd Qu.: 0.557 3rd Qu.: 0.760 ## Max. : 2.4837 Max. : 3.080 Max. : 1.780 ## Petal.Width Species ## Min. :-1.442 setosa :50 ## 1st Qu.:-1.180 versicolor:50 ## Median : 0.132 virginica :50 ## Mean : 0.000 ## 3rd Qu.: 0.788 ## Max. : 1.706 2.6 Proximities: Similarities and Distances Note: R actually only uses dissimilarities/distances. 2.6.1 Minkowsky Distances The Minkowsky distance is a family of metric distances including Euclidean and Manhattan distance. iris_sample &lt;- iris.scaled %&gt;% select(-Species) %&gt;% slice(1:5) iris_sample ## # A tibble: 5 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.898 1.02 -1.34 -1.31 ## 2 -1.14 -0.132 -1.34 -1.31 ## 3 -1.38 0.327 -1.39 -1.31 ## 4 -1.50 0.0979 -1.28 -1.31 ## 5 -1.02 1.25 -1.34 -1.31 Calculate distances matrices between the first 5 flowers (use only the 4 numeric columns). iris_sample %&gt;% dist(method=&quot;euclidean&quot;) ## 1 2 3 4 ## 2 1.1723 ## 3 0.8428 0.5216 ## 4 1.1000 0.4326 0.2829 ## 5 0.2593 1.3819 0.9883 1.2460 iris_sample %&gt;% dist(method=&quot;manhattan&quot;) ## 1 2 3 4 ## 2 1.3887 ## 3 1.2280 0.7570 ## 4 1.5782 0.6484 0.4635 ## 5 0.3502 1.4973 1.3367 1.6868 iris_sample %&gt;% dist(method=&quot;maximum&quot;) ## 1 2 3 4 ## 2 1.1471 ## 3 0.6883 0.4589 ## 4 0.9177 0.3623 0.2294 ## 5 0.2294 1.3766 0.9177 1.1471 Note: Don’t forget to scale the data if the ranges are very different! 2.6.2 Distances for Binary Data b &lt;- rbind( c(0,0,0,1,1,1,1,0,0,1), c(0,0,1,1,1,0,0,1,0,0) ) b ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 0 0 0 1 1 1 1 0 0 ## [2,] 0 0 1 1 1 0 0 1 0 ## [,10] ## [1,] 1 ## [2,] 0 2.6.2.1 Jaccard Index Jaccard index is a similarity measure so R reports 1-Jaccard b %&gt;% dist(method = &quot;binary&quot;) ## 1 ## 2 0.7143 2.6.2.2 Hamming Distance Hamming distance is the number of mis-matches (equivalent to Manhattan distance on 0-1 data and also the squared Euclidean distance). b %&gt;% dist(method = &quot;manhattan&quot;) ## 1 ## 2 5 b %&gt;% dist(method = &quot;euclidean&quot;) %&gt;% &quot;^&quot;(2) ## 1 ## 2 5 Note: \"^\"(2) calculates the square. 2.6.3 Distances for Mixed Data 2.6.3.1 Gower’s Distance Works with mixed data data &lt;- tibble( height= c( 160, 185, 170), weight= c( 52, 90, 75), sex= c( &quot;female&quot;, &quot;male&quot;, &quot;male&quot;) ) data ## # A tibble: 3 x 3 ## height weight sex ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 160 52 female ## 2 185 90 male ## 3 170 75 male Note: Nominal variables need to be factors! data &lt;- data %&gt;% mutate_if(is.character, factor) data ## # A tibble: 3 x 3 ## height weight sex ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 160 52 female ## 2 185 90 male ## 3 170 75 male library(proxy) ## ## Attaching package: &#39;proxy&#39; ## The following object is masked from &#39;package:Matrix&#39;: ## ## as.matrix ## The following objects are masked from &#39;package:stats&#39;: ## ## as.dist, dist ## The following object is masked from &#39;package:base&#39;: ## ## as.matrix d_Gower &lt;- data %&gt;% dist(method=&quot;Gower&quot;) d_Gower ## 1 2 ## 2 1.0000 ## 3 0.6684 0.3316 Note: Gower’s distance automatically scales, so no need to scale the data first. 2.6.3.2 Using Euclidean Distance with Mixed Data Sometimes methods (e.g., k-means) only can use Euclidean distance. In this case, nominal features can be converted into 0-1 dummy variables. Euclidean distance on these will result in a usable distance measure. Create dummy variables library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:sampling&#39;: ## ## cluster ## The following object is masked from &#39;package:purrr&#39;: ## ## lift data_dummy &lt;- dummyVars(~., data) %&gt;% predict(data) data_dummy ## height weight sex.female sex.male ## 1 160 52 1 0 ## 2 185 90 0 1 ## 3 170 75 0 1 Since sex has now two columns, we need to weight them by 1/2 after scaling. weight &lt;- matrix(c(1,1,1/2,1/2), ncol = 4, nrow = nrow(data_dummy), byrow = TRUE) data_dummy_scaled &lt;- scale(data_dummy) * weight d_dummy &lt;- data_dummy_scaled %&gt;% dist() d_dummy ## 1 2 ## 2 3.064 ## 3 1.891 1.427 Distance is (mostly) consistent with Gower’s distance (other than that Gower’s distance is scaled between 0 and 1). ggplot(tibble(d_dummy, d_Gower), aes(x = d_dummy, y = d_Gower)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## Don&#39;t know how to automatically pick scale for object of type dist. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type dist. Defaulting to continuous. ## `geom_smooth()` using formula &#39;y ~ x&#39; 2.6.4 Additional proximity Measures Available in Package proxy library(proxy) pr_DB$get_entries() %&gt;% names() ## [1] &quot;Jaccard&quot; &quot;Kulczynski1&quot; ## [3] &quot;Kulczynski2&quot; &quot;Mountford&quot; ## [5] &quot;Fager&quot; &quot;Russel&quot; ## [7] &quot;simple matching&quot; &quot;Hamman&quot; ## [9] &quot;Faith&quot; &quot;Tanimoto&quot; ## [11] &quot;Dice&quot; &quot;Phi&quot; ## [13] &quot;Stiles&quot; &quot;Michael&quot; ## [15] &quot;Mozley&quot; &quot;Yule&quot; ## [17] &quot;Yule2&quot; &quot;Ochiai&quot; ## [19] &quot;Simpson&quot; &quot;Braun-Blanquet&quot; ## [21] &quot;cosine&quot; &quot;eJaccard&quot; ## [23] &quot;eDice&quot; &quot;correlation&quot; ## [25] &quot;Chi-squared&quot; &quot;Phi-squared&quot; ## [27] &quot;Tschuprow&quot; &quot;Cramer&quot; ## [29] &quot;Pearson&quot; &quot;Gower&quot; ## [31] &quot;Euclidean&quot; &quot;Mahalanobis&quot; ## [33] &quot;Bhjattacharyya&quot; &quot;Manhattan&quot; ## [35] &quot;supremum&quot; &quot;Minkowski&quot; ## [37] &quot;Canberra&quot; &quot;Wave&quot; ## [39] &quot;divergence&quot; &quot;Kullback&quot; ## [41] &quot;Bray&quot; &quot;Soergel&quot; ## [43] &quot;Levenshtein&quot; &quot;Podani&quot; ## [45] &quot;Chord&quot; &quot;Geodesic&quot; ## [47] &quot;Whittaker&quot; &quot;Hellinger&quot; ## [49] &quot;fJaccard&quot; 2.7 Relationships Between Features 2.7.1 Correlation Correlation can be used for ratio/interval scaled features. We typically think of the Pearson correlation coefficient between features (columns). cc &lt;- iris %&gt;% select(-Species) %&gt;% cor() ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; with(iris, cor(Petal.Length, Petal.Width)) ## [1] 0.9629 Note: with is the same as cor(iris$Petal.Length, iris$Petal.Width) with(iris, cor.test(Petal.Length, Petal.Width)) ## ## Pearson&#39;s product-moment correlation ## ## data: Petal.Length and Petal.Width ## t = 43, df = 148, p-value &lt;2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9491 0.9730 ## sample estimates: ## cor ## 0.9629 ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; with(iris, cor(Sepal.Length, Sepal.Width)) ## [1] -0.1176 with(iris, cor.test(Sepal.Length, Sepal.Width)) ## ## Pearson&#39;s product-moment correlation ## ## data: Sepal.Length and Sepal.Width ## t = -1.4, df = 148, p-value = 0.2 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.27269 0.04351 ## sample estimates: ## cor ## -0.1176 2.7.2 Rank Correlation Rank correlation is used for ordinal features. To show this, we first convert the continuous features in Iris into ordered factors with three levels using the function cut. iris_ord &lt;- iris %&gt;% mutate_if(is.numeric, function(x) cut(x, 3, labels = c(&quot;short&quot;, &quot;medium&quot;, &quot;long&quot;), ordered = TRUE)) iris_ord ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; ## 1 short medium short short ## 2 short medium short short ## 3 short medium short short ## 4 short medium short short ## 5 short medium short short ## 6 short long short short ## 7 short medium short short ## 8 short medium short short ## 9 short medium short short ## 10 short medium short short ## # … with 140 more rows, and 1 more variable: ## # Species &lt;fct&gt; summary(iris_ord) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## short :59 short :47 short :50 short :50 ## medium:71 medium:88 medium:54 medium:54 ## long :20 long :15 long :46 long :46 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 iris_ord %&gt;% pull(Sepal.Length) ## [1] short short short short short short short ## [8] short short short short short short short ## [15] medium medium short short medium short short ## [22] short short short short short short short ## [29] short short short short short short short ## [36] short short short short short short short ## [43] short short short short short short short ## [50] short long medium long short medium medium ## [57] medium short medium short short medium medium ## [64] medium medium medium medium medium medium medium ## [71] medium medium medium medium medium medium long ## [78] medium medium medium short short medium medium ## [85] short medium medium medium medium short short ## [92] medium medium short medium medium medium medium ## [99] short medium medium medium long medium medium ## [106] long short long medium long medium medium ## [113] long medium medium medium medium long long ## [120] medium long medium long medium medium long ## [127] medium medium medium long long long medium ## [134] medium medium long medium medium medium long ## [141] medium long medium long medium medium medium ## [148] medium medium medium ## Levels: short &lt; medium &lt; long Kendall’s tau rank correlation coefficient iris_ord %&gt;% select(-Species) %&gt;% sapply(xtfrm) %&gt;% cor(method=&quot;kendall&quot;) ## Sepal.Length Sepal.Width Petal.Length ## Sepal.Length 1.0000 -0.1438 0.7419 ## Sepal.Width -0.1438 1.0000 -0.3299 ## Petal.Length 0.7419 -0.3299 1.0000 ## Petal.Width 0.7295 -0.3154 0.9198 ## Petal.Width ## Sepal.Length 0.7295 ## Sepal.Width -0.3154 ## Petal.Length 0.9198 ## Petal.Width 1.0000 Spearman’s rho iris_ord %&gt;% select(-Species) %&gt;% sapply(xtfrm) %&gt;% cor(method=&quot;spearman&quot;) ## Sepal.Length Sepal.Width Petal.Length ## Sepal.Length 1.0000 -0.1570 0.7938 ## Sepal.Width -0.1570 1.0000 -0.3663 ## Petal.Length 0.7938 -0.3663 1.0000 ## Petal.Width 0.7843 -0.3517 0.9399 ## Petal.Width ## Sepal.Length 0.7843 ## Sepal.Width -0.3517 ## Petal.Length 0.9399 ## Petal.Width 1.0000 Note: unfortunately we have to transform the ordered factors into numbers representing the order with xtfrm first. Compare to the Pearson correlation on the original data iris %&gt;% select(-Species) %&gt;% cor() ## Sepal.Length Sepal.Width Petal.Length ## Sepal.Length 1.0000 -0.1176 0.8718 ## Sepal.Width -0.1176 1.0000 -0.4284 ## Petal.Length 0.8718 -0.4284 1.0000 ## Petal.Width 0.8179 -0.3661 0.9629 ## Petal.Width ## Sepal.Length 0.8179 ## Sepal.Width -0.3661 ## Petal.Length 0.9629 ## Petal.Width 1.0000 2.7.3 Relationship Between Nominal and Ordinal Features Is sepal length and species related? Use cross tabulation tbl &lt;- iris_ord %&gt;% select(Sepal.Length, Species) %&gt;% table() tbl ## Species ## Sepal.Length setosa versicolor virginica ## short 47 11 1 ## medium 3 36 32 ## long 0 3 17 Doing this with tidyverse is a little more involved and uses pivot operations and grouping. iris_ord %&gt;% select(Species, Sepal.Length) %&gt;% pivot_longer(cols = Sepal.Length) %&gt;% group_by(Species, value) %&gt;% count() %&gt;% ungroup() %&gt;% pivot_wider(names_from = Species, values_from = n) ## # A tibble: 3 x 4 ## value setosa versicolor virginica ## &lt;ord&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 short 47 11 1 ## 2 medium 3 36 32 ## 3 long NA 3 17 Test of Independence: Pearson’s chi-squared test is performed with the null hypothesis that the joint distribution of the cell counts in a 2-dimensional contingency table is the product of the row and column marginals. (h0 is independence) tbl %&gt;% chisq.test() ## ## Pearson&#39;s Chi-squared test ## ## data: . ## X-squared = 112, df = 4, p-value &lt;2e-16 Using xtabs instead x &lt;- xtabs(~Sepal.Length + Species, data = iris_ord) x ## Species ## Sepal.Length setosa versicolor virginica ## short 47 11 1 ## medium 3 36 32 ## long 0 3 17 summary(x) ## Call: xtabs(formula = ~Sepal.Length + Species, data = iris_ord) ## Number of cases in table: 150 ## Number of factors: 2 ## Test for independence of all factors: ## Chisq = 112, df = 4, p-value = 3e-23 Group-wise averages iris %&gt;% group_by(Species) %&gt;% summarize_at(vars(Sepal.Length), mean) ## # A tibble: 3 x 2 ## Species Sepal.Length ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 ## 2 versicolor 5.94 2.77 4.26 ## 3 virginica 6.59 2.97 5.55 ## # … with 1 more variable: Petal.Width &lt;dbl&gt; 2.8 Density Estimation Density estimation constructions an estimate of an unobservable probability density function (a distribution) based on observed data. Just plotting the data is not very helpful ggplot(iris, aes(Petal.Length, 1:150)) + geom_point() Histograms work better ggplot(iris, aes(Petal.Length)) + geom_histogram() + geom_rug(alpha = 1/10) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. Kernel density estimate KDE ggplot(iris, aes(Petal.Length)) + geom_rug(alpha = 1/10) + geom_density() Plot 2d kernel density estimate ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_jitter() + geom_density2d() ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_bin2d(bins = 10) + geom_jitter(color = &quot;red&quot;) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_hex(bins = 10) + geom_jitter(color = &quot;red&quot;) ## Warning: Computation failed in `stat_binhex()`: ## The `hexbin` package is required for `stat_binhex()` 2.9 Exploring Data 2.9.1 Basic statistics Load the iris data set. data(iris) Fisher’s iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica. We convert the data.frame into a tidyerse tibble. This is optional, tidyverse can work directly with data.frames. iris &lt;- as_tibble(iris) iris ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # … with 140 more rows, and 1 more variable: ## # Species &lt;fct&gt; Get summary statistics (using base R) summary(iris) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.00 Min. :1.00 ## 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 ## Median :5.80 Median :3.00 Median :4.35 ## Mean :5.84 Mean :3.06 Mean :3.76 ## 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 ## Max. :7.90 Max. :4.40 Max. :6.90 ## Petal.Width Species ## Min. :0.1 setosa :50 ## 1st Qu.:0.3 versicolor:50 ## Median :1.3 virginica :50 ## Mean :1.2 ## 3rd Qu.:1.8 ## Max. :2.5 Get mean and standard deviation for sepal length iris %&gt;% pull(Sepal.Length) %&gt;% mean() ## [1] 5.843 iris %&gt;% pull(Sepal.Length) %&gt;% sd() ## [1] 0.8281 Ignore missing values (Note: this data does not contain any, but this is what you would do) iris %&gt;% pull(Sepal.Length) %&gt;% mean(na.rm = TRUE) ## [1] 5.843 Robust mean (trim 10% of observations from each end of the distribution) iris %&gt;% pull(Sepal.Length) %&gt;% mean(trim = .1) ## [1] 5.808 Calculate a summary for all numeric columns iris %&gt;% summarize_if(is.numeric, mean) ## # A tibble: 1 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.84 3.06 3.76 1.20 iris %&gt;% summarize_if(is.numeric, sd) ## # A tibble: 1 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.828 0.436 1.77 0.762 iris %&gt;% summarize_if(is.numeric, list(min = min, median = median, max = max)) ## # A tibble: 1 x 12 ## Sepal.Length_min Sepal.Width_min Petal.Length_min ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.3 2 1 ## # … with 9 more variables: Petal.Width_min &lt;dbl&gt;, ## # Sepal.Length_median &lt;dbl&gt;, ## # Sepal.Width_median &lt;dbl&gt;, ## # Petal.Length_median &lt;dbl&gt;, ## # Petal.Width_median &lt;dbl&gt;, Sepal.Length_max &lt;dbl&gt;, ## # Sepal.Width_max &lt;dbl&gt;, Petal.Length_max &lt;dbl&gt;, ## # Petal.Width_max &lt;dbl&gt; MAD (median absolute deviation) iris %&gt;% summarize_if(is.numeric, mad) ## # A tibble: 1 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.04 0.445 1.85 1.04 2.9.2 Tabulate data Count the different species. iris %&gt;% count(Species) ## # A tibble: 3 x 2 ## Species n ## &lt;fct&gt; &lt;int&gt; ## 1 setosa 50 ## 2 versicolor 50 ## 3 virginica 50 Discretize the data first since there are too many values (cut divides the range by breaks, see package discretization for other methods) iris_discrete &lt;- iris %&gt;% mutate_if(is.numeric, function(x) cut(x, 3, labels = c(&quot;short&quot;, &quot;medium&quot;, &quot;long&quot;), ordered = TRUE)) iris_discrete ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; ## 1 short medium short short ## 2 short medium short short ## 3 short medium short short ## 4 short medium short short ## 5 short medium short short ## 6 short long short short ## 7 short medium short short ## 8 short medium short short ## 9 short medium short short ## 10 short medium short short ## # … with 140 more rows, and 1 more variable: ## # Species &lt;fct&gt; summary(iris_discrete) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## short :59 short :47 short :50 short :50 ## medium:71 medium:88 medium:54 medium:54 ## long :20 long :15 long :46 long :46 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 Create some tables (creating tables is a little harder using tidyverse) iris_discrete %&gt;% select(Sepal.Length, Sepal.Width) %&gt;% table() ## Sepal.Width ## Sepal.Length short medium long ## short 12 37 10 ## medium 31 37 3 ## long 4 14 2 iris_discrete %&gt;% select(Petal.Length, Petal.Width) %&gt;% table() ## Petal.Width ## Petal.Length short medium long ## short 50 0 0 ## medium 0 48 6 ## long 0 6 40 iris_discrete %&gt;% select(Petal.Length, Species) %&gt;% table() ## Species ## Petal.Length setosa versicolor virginica ## short 50 0 0 ## medium 0 48 6 ## long 0 2 44 Test if the two features are independent given the counts in the contingency table (H0: independence) p-value: the probability of seeing a more extreme value of the test statistic under the assumption that H0 is correct. Low p-values (typically less than .05 or .01) indicate that H0 should be rejected. tbl &lt;- iris_discrete %&gt;% select(Sepal.Length, Sepal.Width) %&gt;% table() tbl ## Sepal.Width ## Sepal.Length short medium long ## short 12 37 10 ## medium 31 37 3 ## long 4 14 2 chisq.test(tbl) ## Warning in chisq.test(tbl): Chi-squared approximation ## may be incorrect ## ## Pearson&#39;s Chi-squared test ## ## data: tbl ## X-squared = 13, df = 4, p-value = 0.01 Fisher’s exact test is better for small counts (cells with counts &lt;5) fisher.test(tbl) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: tbl ## p-value = 0.01 ## alternative hypothesis: two.sided Look at the distribution for a discrete variable iris_discrete %&gt;% count(Sepal.Length) ## # A tibble: 3 x 2 ## Sepal.Length n ## &lt;ord&gt; &lt;int&gt; ## 1 short 59 ## 2 medium 71 ## 3 long 20 2.9.3 Percentiles (Quantiles) By default calculates quantiles iris %&gt;% pull(Petal.Length) %&gt;% quantile() ## 0% 25% 50% 75% 100% ## 1.00 1.60 4.35 5.10 6.90 Interquartile range iris %&gt;% summarize(IQR = quantile(Petal.Length, probs = 0.75) - quantile(Petal.Length, probs = 0.25)) ## # A tibble: 1 x 1 ## IQR ## &lt;dbl&gt; ## 1 3.5 2.10 Visualization 2.10.1 Histogram Show the distribution of a single numeric variable ggplot(iris, aes(Petal.Width)) + geom_histogram(bins = 20) 2.10.2 Boxplot Compare the distribution of a variable between different groups. ggplot(iris, aes(Species, Sepal.Length)) + geom_boxplot() Group-wise averages iris %&gt;% group_by(Species) %&gt;% summarize_if(is.numeric, mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 ## 2 versicolor 5.94 2.77 4.26 ## 3 virginica 6.59 2.97 5.55 ## # … with 1 more variable: Petal.Width &lt;dbl&gt; To compare the distribution of the four features using a ggplot boxplot, we first have to transform the data into long format (i.e., all feature values are combined into a single column). library(tidyr) iris_long &lt;- iris %&gt;% mutate(id = row_number()) %&gt;% pivot_longer(1:4) ggplot(iris_long, aes(name, value)) + geom_boxplot() 2.10.3 Scatter plot Show the relationship between two numeric variables ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) + geom_point() 2.10.4 Scatter Plot Matrix Show the relationship between several numeric variables library(&quot;GGally&quot;) ggpairs(iris, aes(color = Species)) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. 2.10.5 Data Matrix Visualization We need the long format iris_long &lt;- iris %&gt;% mutate(id = row_number()) %&gt;% pivot_longer(1:4) head(iris_long) ## # A tibble: 6 x 4 ## Species id name value ## &lt;fct&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 setosa 1 Sepal.Length 5.1 ## 2 setosa 1 Sepal.Width 3.5 ## 3 setosa 1 Petal.Length 1.4 ## 4 setosa 1 Petal.Width 0.2 ## 5 setosa 2 Sepal.Length 4.9 ## 6 setosa 2 Sepal.Width 3 ggplot(iris_long, aes(x = name, y = id, fill = value)) + geom_tile() + scale_fill_viridis_c() values smaller than the average are blue and larger ones are red iris_scaled &lt;- iris %&gt;% select(-Species) %&gt;% scale() iris_scaled_long &lt;- iris_scaled %&gt;% as_tibble() %&gt;% mutate(id = row_number()) %&gt;% pivot_longer(cols = 1:4) ggplot(iris_scaled_long, aes(x = name, y = id, fill = value)) + geom_tile() + scale_fill_gradient2() Reorder library(seriation) ## Registered S3 methods overwritten by &#39;registry&#39;: ## method from ## print.registry_field proxy ## print.registry_entry proxy ## ## Attaching package: &#39;seriation&#39; ## The following object is masked from &#39;package:lattice&#39;: ## ## panel.lines iris_scaled_matrix &lt;- as.matrix(iris_scaled) o &lt;- seriate(iris_scaled_matrix) iris_ordered &lt;- permute(iris_scaled_matrix, o) iris_ordered_long &lt;- iris_ordered %&gt;% as_tibble %&gt;% mutate(id = row_number()) %&gt;% pivot_longer(cols = 1:4) ggplot(iris_ordered_long, aes(x = name, y = id, fill = value)) + geom_tile() + scale_fill_gradient2() 2.10.6 Correlation Matrix Calculate and visualize the correlation between features cm1 &lt;- iris %&gt;% select(-Species) %&gt;% as.matrix %&gt;% cor() cm1 ## Sepal.Length Sepal.Width Petal.Length ## Sepal.Length 1.0000 -0.1176 0.8718 ## Sepal.Width -0.1176 1.0000 -0.4284 ## Petal.Length 0.8718 -0.4284 1.0000 ## Petal.Width 0.8179 -0.3661 0.9629 ## Petal.Width ## Sepal.Length 0.8179 ## Sepal.Width -0.3661 ## Petal.Length 0.9629 ## Petal.Width 1.0000 library(ggcorrplot) ggcorrplot(cm1) use hmap from package seriation hmap(cm1, margin = c(7,7), cexRow = 1, cexCol = 1) Test if correlation is significantly different from 0 cor.test(iris$Sepal.Length, iris$Sepal.Width) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Sepal.Width ## t = -1.4, df = 148, p-value = 0.2 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.27269 0.04351 ## sample estimates: ## cor ## -0.1176 cor.test(iris$Petal.Length, iris$Petal.Width) #this one is significant ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Petal.Length and iris$Petal.Width ## t = 43, df = 148, p-value &lt;2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9491 0.9730 ## sample estimates: ## cor ## 0.9629 Correlation between objects cm2 &lt;- iris %&gt;% select(-Species) %&gt;% as.matrix() %&gt;% t() %&gt;% cor() ggcorrplot(cm2) 2.10.7 Parallel Coordinates Plot library(GGally) ggparcoord(as_tibble(iris), columns = 1:4, groupColumn = 5) Reorder with placing correlated features next to each other library(seriation) o &lt;- seriate(as.dist(1-cor(iris[,1:4])), method=&quot;BBURCG&quot;) get_order(o) ## Petal.Length Petal.Width Sepal.Length Sepal.Width ## 3 4 1 2 ggparcoord(as_tibble(iris), columns = get_order(o), groupColumn = 5) Look at https://www.r-graph-gallery.com/ for many example graphs. References "],["classification-basic-concepts-and-techniques.html", "Chapter 3 Classification: Basic Concepts and Techniques 3.1 The Zoo Dataset 3.2 Decision Trees 3.3 Model Evaluation with Caret 3.4 Testing: Confusion Matrix and Confidence Interval for Accuracy 3.5 Model Comparison 3.6 Feature Selection and Feature Preparation 3.7 Class Imbalance", " Chapter 3 Classification: Basic Concepts and Techniques Packages used for this chapter: caret (Kuhn 2021), FSelector (Romanski, Kotthoff, and Schratz 2021), lattice (Sarkar 2021), mlbench (Leisch and Dimitriadou. 2021), pROC (Robin et al. 2021), rpart (Therneau and Atkinson 2019), rpart.plot (Milborrow 2020), sampling (Tillé and Matei 2021), tidyverse (Wickham 2021c) You can read the free sample chapter from the textbook (Tan, Steinbach, and Kumar 2005): Chapter 3. Classification: Basic Concepts and Techniques 3.1 The Zoo Dataset We will use the Zoo dataset which is included in the R package mlbench (you may have to install it). The Zoo dataset containing 17 (mostly logical) variables on different 101 animals as a data frame with 17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize, type). We convert the data frame into a tidyverse tibble (optional). data(Zoo, package=&quot;mlbench&quot;) head(Zoo) ## hair feathers eggs milk airborne aquatic ## aardvark TRUE FALSE FALSE TRUE FALSE FALSE ## antelope TRUE FALSE FALSE TRUE FALSE FALSE ## bass FALSE FALSE TRUE FALSE FALSE TRUE ## bear TRUE FALSE FALSE TRUE FALSE FALSE ## boar TRUE FALSE FALSE TRUE FALSE FALSE ## buffalo TRUE FALSE FALSE TRUE FALSE FALSE ## predator toothed backbone breathes venomous ## aardvark TRUE TRUE TRUE TRUE FALSE ## antelope FALSE TRUE TRUE TRUE FALSE ## bass TRUE TRUE TRUE FALSE FALSE ## bear TRUE TRUE TRUE TRUE FALSE ## boar TRUE TRUE TRUE TRUE FALSE ## buffalo FALSE TRUE TRUE TRUE FALSE ## fins legs tail domestic catsize type ## aardvark FALSE 4 FALSE FALSE TRUE mammal ## antelope FALSE 4 TRUE FALSE TRUE mammal ## bass TRUE 0 TRUE FALSE FALSE fish ## bear FALSE 4 FALSE FALSE TRUE mammal ## boar FALSE 4 TRUE FALSE TRUE mammal ## buffalo FALSE 4 TRUE FALSE TRUE mammal Note: data.frames in R can have row names. The Zoo data set uses the animal name as the row names. tibbles from tidyverse do not support row names. To keep the animal name you can add a column with the animal name. library(tidyverse) as_tibble(Zoo, rownames = &quot;animal&quot;) ## # A tibble: 101 x 18 ## animal hair feathers eggs milk airborne aquatic ## &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 aardvark TRUE FALSE FALSE TRUE FALSE FALSE ## 2 antelope TRUE FALSE FALSE TRUE FALSE FALSE ## 3 bass FALSE FALSE TRUE FALSE FALSE TRUE ## 4 bear TRUE FALSE FALSE TRUE FALSE FALSE ## 5 boar TRUE FALSE FALSE TRUE FALSE FALSE ## 6 buffalo TRUE FALSE FALSE TRUE FALSE FALSE ## 7 calf TRUE FALSE FALSE TRUE FALSE FALSE ## 8 carp FALSE FALSE TRUE FALSE FALSE TRUE ## 9 catfish FALSE FALSE TRUE FALSE FALSE TRUE ## 10 cavy TRUE FALSE FALSE TRUE FALSE FALSE ## # … with 91 more rows, and 11 more variables: ## # predator &lt;lgl&gt;, toothed &lt;lgl&gt;, backbone &lt;lgl&gt;, ## # breathes &lt;lgl&gt;, venomous &lt;lgl&gt;, fins &lt;lgl&gt;, ## # legs &lt;int&gt;, tail &lt;lgl&gt;, domestic &lt;lgl&gt;, ## # catsize &lt;lgl&gt;, type &lt;fct&gt; You will have to remove the animal column before learning a model! In the following I use the data.frame. I translate all the TRUE/FALSE values into factors (nominal). This is often needed for building models. Always check summary() to make sure the data is ready for model learning. Zoo &lt;- Zoo %&gt;% modify_if(is.logical, factor, levels = c(TRUE, FALSE)) %&gt;% modify_if(is.character, factor) Zoo %&gt;% summary() ## hair feathers eggs milk ## TRUE :43 TRUE :20 TRUE :59 TRUE :41 ## FALSE:58 FALSE:81 FALSE:42 FALSE:60 ## ## ## ## ## ## airborne aquatic predator toothed ## TRUE :24 TRUE :36 TRUE :56 TRUE :61 ## FALSE:77 FALSE:65 FALSE:45 FALSE:40 ## ## ## ## ## ## backbone breathes venomous fins ## TRUE :83 TRUE :80 TRUE : 8 TRUE :17 ## FALSE:18 FALSE:21 FALSE:93 FALSE:84 ## ## ## ## ## ## legs tail domestic catsize ## Min. :0.00 TRUE :75 TRUE :13 TRUE :44 ## 1st Qu.:2.00 FALSE:26 FALSE:88 FALSE:57 ## Median :4.00 ## Mean :2.84 ## 3rd Qu.:4.00 ## Max. :8.00 ## ## type ## mammal :41 ## bird :20 ## reptile : 5 ## fish :13 ## amphibian : 4 ## insect : 8 ## mollusc.et.al:10 3.2 Decision Trees Recursive Partitioning (similar to CART) uses the Gini index to make splitting decisions and early stopping (pre-pruning). library(rpart) 3.2.1 Create Tree With Default Settings (uses pre-pruning) tree_default &lt;- Zoo %&gt;% rpart(type ~ ., data = .) tree_default ## n= 101 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099) ## 2) milk=TRUE 41 0 mammal (1 0 0 0 0 0 0) * ## 3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17) ## 6) feathers=TRUE 20 0 bird (0 1 0 0 0 0 0) * ## 7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25) ## 14) fins=TRUE 13 0 fish (0 0 0 1 0 0 0) * ## 15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37) ## 30) backbone=TRUE 9 4 reptile (0 0 0.56 0 0.44 0 0) * ## 31) backbone=FALSE 18 8 mollusc.et.al (0 0 0 0 0 0.44 0.56) * Notes: - %&gt;% supplies the data for rpart. Since data is not the first argument of rpart, the syntax data = . is used to specify where the data in Zoo goes. The call is equivalent to tree_default &lt;- rpart(type ~ ., data = Zoo). - The formula models the type variable by all other features represented by .. data = . means that the data provided by the pipe (%&gt;%) will be passed to rpart as the argument data. the class variable needs a factor (nominal) or rpart will create a regression tree instead of a decision tree. Use as.factor() if necessary. Plotting library(rpart.plot) rpart.plot(tree_default, extra = 2) ## Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables). ## To silence this warning: ## Call rpart.plot with roundint=FALSE, ## or rebuild the rpart model with model=TRUE. Note: extra=2 prints for each leaf node the number of correctly classified objects from data and the total number of objects from the training data falling into that node (correct/total). 3.2.2 Create a Full Tree To create a full tree, we set the complexity parameter cp to 0 (split even if it does not improve the tree) and we set the minimum number of observations in a node needed to split to the smallest value of 2 (see: ?rpart.control). Note: full trees overfit the training data! tree_full &lt;- Zoo %&gt;% rpart(type ~., data = ., control = rpart.control(minsplit = 2, cp = 0)) rpart.plot(tree_full, extra = 2, roundint=FALSE, box.palette = list(&quot;Gy&quot;, &quot;Gn&quot;, &quot;Bu&quot;, &quot;Bn&quot;, &quot;Or&quot;, &quot;Rd&quot;, &quot;Pu&quot;)) # specify 7 colors tree_full ## n= 101 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099) ## 2) milk=TRUE 41 0 mammal (1 0 0 0 0 0 0) * ## 3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17) ## 6) feathers=TRUE 20 0 bird (0 1 0 0 0 0 0) * ## 7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25) ## 14) fins=TRUE 13 0 fish (0 0 0 1 0 0 0) * ## 15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37) ## 30) backbone=TRUE 9 4 reptile (0 0 0.56 0 0.44 0 0) ## 60) aquatic=FALSE 4 0 reptile (0 0 1 0 0 0 0) * ## 61) aquatic=TRUE 5 1 amphibian (0 0 0.2 0 0.8 0 0) ## 122) eggs=FALSE 1 0 reptile (0 0 1 0 0 0 0) * ## 123) eggs=TRUE 4 0 amphibian (0 0 0 0 1 0 0) * ## 31) backbone=FALSE 18 8 mollusc.et.al (0 0 0 0 0 0.44 0.56) ## 62) airborne=TRUE 6 0 insect (0 0 0 0 0 1 0) * ## 63) airborne=FALSE 12 2 mollusc.et.al (0 0 0 0 0 0.17 0.83) ## 126) predator=FALSE 4 2 insect (0 0 0 0 0 0.5 0.5) ## 252) legs&gt;=3 2 0 insect (0 0 0 0 0 1 0) * ## 253) legs&lt; 3 2 0 mollusc.et.al (0 0 0 0 0 0 1) * ## 127) predator=TRUE 8 0 mollusc.et.al (0 0 0 0 0 0 1) * Training error on tree with pre-pruning predict(tree_default, Zoo) %&gt;% head () ## mammal bird reptile fish amphibian insect ## aardvark 1 0 0 0 0 0 ## antelope 1 0 0 0 0 0 ## bass 0 0 0 1 0 0 ## bear 1 0 0 0 0 0 ## boar 1 0 0 0 0 0 ## buffalo 1 0 0 0 0 0 ## mollusc.et.al ## aardvark 0 ## antelope 0 ## bass 0 ## bear 0 ## boar 0 ## buffalo 0 pred &lt;- predict(tree_default, Zoo, type=&quot;class&quot;) head(pred) ## aardvark antelope bass bear boar buffalo ## mammal mammal fish mammal mammal mammal ## 7 Levels: mammal bird reptile fish ... mollusc.et.al confusion_table &lt;- with(Zoo, table(type, pred)) confusion_table ## pred ## type mammal bird reptile fish amphibian ## mammal 41 0 0 0 0 ## bird 0 20 0 0 0 ## reptile 0 0 5 0 0 ## fish 0 0 0 13 0 ## amphibian 0 0 4 0 0 ## insect 0 0 0 0 0 ## mollusc.et.al 0 0 0 0 0 ## pred ## type insect mollusc.et.al ## mammal 0 0 ## bird 0 0 ## reptile 0 0 ## fish 0 0 ## amphibian 0 0 ## insect 0 8 ## mollusc.et.al 0 10 correct &lt;- confusion_table %&gt;% diag() %&gt;% sum() correct ## [1] 89 error &lt;- confusion_table %&gt;% sum() - correct error ## [1] 12 accuracy &lt;- correct / (correct + error) accuracy ## [1] 0.8812 Use a function for accuracy accuracy &lt;- function(truth, prediction) { tbl &lt;- table(truth, prediction) sum(diag(tbl))/sum(tbl) } accuracy(Zoo %&gt;% pull(type), pred) ## [1] 0.8812 Training error of the full tree accuracy(Zoo %&gt;% pull(type), predict(tree_full, Zoo, type=&quot;class&quot;)) ## [1] 1 Get a confusion table with more statistics (using caret) library(caret) confusionMatrix(data = pred, reference = Zoo %&gt;% pull(type)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction mammal bird reptile fish amphibian ## mammal 41 0 0 0 0 ## bird 0 20 0 0 0 ## reptile 0 0 5 0 4 ## fish 0 0 0 13 0 ## amphibian 0 0 0 0 0 ## insect 0 0 0 0 0 ## mollusc.et.al 0 0 0 0 0 ## Reference ## Prediction insect mollusc.et.al ## mammal 0 0 ## bird 0 0 ## reptile 0 0 ## fish 0 0 ## amphibian 0 0 ## insect 0 0 ## mollusc.et.al 8 10 ## ## Overall Statistics ## ## Accuracy : 0.881 ## 95% CI : (0.802, 0.937) ## No Information Rate : 0.406 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.843 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: mammal Class: bird ## Sensitivity 1.000 1.000 ## Specificity 1.000 1.000 ## Pos Pred Value 1.000 1.000 ## Neg Pred Value 1.000 1.000 ## Prevalence 0.406 0.198 ## Detection Rate 0.406 0.198 ## Detection Prevalence 0.406 0.198 ## Balanced Accuracy 1.000 1.000 ## Class: reptile Class: fish ## Sensitivity 1.0000 1.000 ## Specificity 0.9583 1.000 ## Pos Pred Value 0.5556 1.000 ## Neg Pred Value 1.0000 1.000 ## Prevalence 0.0495 0.129 ## Detection Rate 0.0495 0.129 ## Detection Prevalence 0.0891 0.129 ## Balanced Accuracy 0.9792 1.000 ## Class: amphibian Class: insect ## Sensitivity 0.0000 0.0000 ## Specificity 1.0000 1.0000 ## Pos Pred Value NaN NaN ## Neg Pred Value 0.9604 0.9208 ## Prevalence 0.0396 0.0792 ## Detection Rate 0.0000 0.0000 ## Detection Prevalence 0.0000 0.0000 ## Balanced Accuracy 0.5000 0.5000 ## Class: mollusc.et.al ## Sensitivity 1.000 ## Specificity 0.912 ## Pos Pred Value 0.556 ## Neg Pred Value 1.000 ## Prevalence 0.099 ## Detection Rate 0.099 ## Detection Prevalence 0.178 ## Balanced Accuracy 0.956 3.2.3 Make Predictions for New Data Make up my own animal: A lion with feathered wings my_animal &lt;- tibble(hair = TRUE, feathers = TRUE, eggs = FALSE, milk = TRUE, airborne = TRUE, aquatic = FALSE, predator = TRUE, toothed = TRUE, backbone = TRUE, breathes = TRUE, venomous = FALSE, fins = FALSE, legs = 4, tail = TRUE, domestic = FALSE, catsize = FALSE, type = NA) Fix columns to be factors like in the training set. my_animal &lt;- my_animal %&gt;% modify_if(is.logical, factor, levels = c(TRUE, FALSE)) my_animal ## # A tibble: 1 x 17 ## hair feathers eggs milk airborne aquatic predator ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 TRUE TRUE FALSE TRUE TRUE FALSE TRUE ## # … with 10 more variables: toothed &lt;fct&gt;, ## # backbone &lt;fct&gt;, breathes &lt;fct&gt;, venomous &lt;fct&gt;, ## # fins &lt;fct&gt;, legs &lt;dbl&gt;, tail &lt;fct&gt;, ## # domestic &lt;fct&gt;, catsize &lt;fct&gt;, type &lt;fct&gt; Make a prediction using the default tree predict(tree_default , my_animal, type = &quot;class&quot;) ## 1 ## mammal ## 7 Levels: mammal bird reptile fish ... mollusc.et.al 3.3 Model Evaluation with Caret The package caret makes preparing training sets, building classification (and regression) models and evaluation easier. A great cheat sheet can be found here. library(caret) Cross-validation runs are independent and can be done faster in parallel. To enable multi-core support, caret uses the package foreach and you need to load a do backend. For Linux, you can use doMC with 4 cores. Windows needs different backend like doParallel (see caret cheat sheet above). ## Linux backend # library(doMC) # registerDoMC(cores = 4) # getDoParWorkers() ## Windows backend # library(doParallel) # cl &lt;- makeCluster(4, type=&quot;SOCK&quot;) # registerDoParallel(cl) Set random number generator seed to make results reproducible set.seed(2000) 3.3.1 Hold out Test Data Test data is not used in the model building process and set aside purely for testing the model. Here, we partition data the 80% training and 20% testing. inTrain &lt;- createDataPartition(y = Zoo$type, p = .8, list = FALSE) Zoo_train &lt;- Zoo %&gt;% slice(inTrain) Zoo_test &lt;- Zoo %&gt;% slice(-inTrain) 3.3.2 Learn a Model and Tune Hyperparameters on the Training Data The package caret combines training and validation for hyperparameter tuning into a single function called train(). It internally splits the data into training and validation sets and thus will provide you with error estimates for different hyperparameter settings. trainControl is used to choose how testing is performed. For rpart, train tries to tune the cp parameter (tree complexity) using accuracy to chose the best model. I set minsplit to 2 since we have not much data. Note: Parameters used for tuning (in this case cp) need to be set using a data.frame in the argument tuneGrid! Setting it in control will be ignored. fit &lt;- Zoo_train %&gt;% train(type ~ ., data = . , method = &quot;rpart&quot;, control = rpart.control(minsplit = 2), trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneLength = 5) fit ## CART ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 77, 74, 75, 73, 74, 76, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.00 0.9385 0.9189 ## 0.08 0.8974 0.8682 ## 0.16 0.7448 0.6637 ## 0.22 0.6663 0.5540 ## 0.32 0.4735 0.1900 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was cp = 0. Note: Train has built 10 trees using the training folds for each value of cp and the reported values for accuracy and Kappa are the averages on the validation folds. A model using the best tuning parameters and using all the data supplied to train() is available as fit$finalModel. rpart.plot(fit$finalModel, extra = 2, box.palette = list(&quot;Gy&quot;, &quot;Gn&quot;, &quot;Bu&quot;, &quot;Bn&quot;, &quot;Or&quot;, &quot;Rd&quot;, &quot;Pu&quot;)) caret also computes variable importance. By default it uses competing splits (splits which would be runners up, but do not get chosen by the tree) for rpart models (see ? varImp). Toothed is the runner up for many splits, but it never gets chosen! varImp(fit) ## rpart variable importance ## ## Overall ## toothedFALSE 100.00 ## feathersFALSE 69.81 ## backboneFALSE 63.08 ## milkFALSE 55.56 ## eggsFALSE 53.61 ## hairFALSE 50.52 ## finsFALSE 46.98 ## tailFALSE 28.45 ## breathesFALSE 28.13 ## airborneFALSE 26.27 ## legs 25.86 ## aquaticFALSE 5.96 ## predatorFALSE 2.35 ## venomousFALSE 1.39 ## catsizeFALSE 0.00 ## domesticFALSE 0.00 Here is the variable importance without competing splits. imp &lt;- varImp(fit, compete = FALSE) imp ## rpart variable importance ## ## Overall ## milkFALSE 100.00 ## feathersFALSE 55.69 ## finsFALSE 39.45 ## toothedFALSE 22.96 ## airborneFALSE 22.48 ## aquaticFALSE 9.99 ## eggsFALSE 6.66 ## legs 5.55 ## predatorFALSE 1.85 ## domesticFALSE 0.00 ## breathesFALSE 0.00 ## catsizeFALSE 0.00 ## tailFALSE 0.00 ## hairFALSE 0.00 ## backboneFALSE 0.00 ## venomousFALSE 0.00 ggplot(imp) Note: Not all models provide a variable importance function. In this case caret might calculate varImp by itself and ignore the model (see ? varImp)! 3.4 Testing: Confusion Matrix and Confidence Interval for Accuracy Use the best model on the test data pred &lt;- predict(fit, newdata = Zoo_test) pred ## [1] mammal mammal mollusc.et.al ## [4] insect mammal mammal ## [7] mammal bird mammal ## [10] mammal bird fish ## [13] fish mammal mollusc.et.al ## [16] bird insect bird ## 7 Levels: mammal bird reptile fish ... mollusc.et.al Caret’s confusionMatrix() function calculates accuracy, confidence intervals, kappa and many more evaluation metrics. You need to use separate test data to create a confusion matrix based on the generalization error. confusionMatrix(data = pred, ref = Zoo_test$type) ## Confusion Matrix and Statistics ## ## Reference ## Prediction mammal bird reptile fish amphibian ## mammal 8 0 0 0 0 ## bird 0 4 0 0 0 ## reptile 0 0 0 0 0 ## fish 0 0 0 2 0 ## amphibian 0 0 0 0 0 ## insect 0 0 1 0 0 ## mollusc.et.al 0 0 0 0 0 ## Reference ## Prediction insect mollusc.et.al ## mammal 0 0 ## bird 0 0 ## reptile 0 0 ## fish 0 0 ## amphibian 0 0 ## insect 1 0 ## mollusc.et.al 0 2 ## ## Overall Statistics ## ## Accuracy : 0.944 ## 95% CI : (0.727, 0.999) ## No Information Rate : 0.444 ## P-Value [Acc &gt; NIR] : 1.08e-05 ## ## Kappa : 0.923 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: mammal Class: bird ## Sensitivity 1.000 1.000 ## Specificity 1.000 1.000 ## Pos Pred Value 1.000 1.000 ## Neg Pred Value 1.000 1.000 ## Prevalence 0.444 0.222 ## Detection Rate 0.444 0.222 ## Detection Prevalence 0.444 0.222 ## Balanced Accuracy 1.000 1.000 ## Class: reptile Class: fish ## Sensitivity 0.0000 1.000 ## Specificity 1.0000 1.000 ## Pos Pred Value NaN 1.000 ## Neg Pred Value 0.9444 1.000 ## Prevalence 0.0556 0.111 ## Detection Rate 0.0000 0.111 ## Detection Prevalence 0.0000 0.111 ## Balanced Accuracy 0.5000 1.000 ## Class: amphibian Class: insect ## Sensitivity NA 1.0000 ## Specificity 1 0.9412 ## Pos Pred Value NA 0.5000 ## Neg Pred Value NA 1.0000 ## Prevalence 0 0.0556 ## Detection Rate 0 0.0556 ## Detection Prevalence 0 0.1111 ## Balanced Accuracy NA 0.9706 ## Class: mollusc.et.al ## Sensitivity 1.000 ## Specificity 1.000 ## Pos Pred Value 1.000 ## Neg Pred Value 1.000 ## Prevalence 0.111 ## Detection Rate 0.111 ## Detection Prevalence 0.111 ## Balanced Accuracy 1.000 Some notes Many classification algorithms and train in caret do not deal well with missing values. If your classification model can deal with missing values (e.g., rpart) then use na.action = na.pass when you call train and predict. Otherwise, you need to remove observations with missing values with na.omit or use imputation to replace the missing values before you train the model. Make sure that you still have enough observations left. Make sure that nominal variables (this includes logical variables) are coded as factors. The class variable for train in caret cannot have level names that are keywords in R (e.g., TRUE and FALSE). Rename them to, for example, “yes” and “no.” Make sure that nominal variables (factors) have examples for all possible values. Some methods might have problems with variable values without examples. You can drop empty levels using droplevels or factor. Sampling in train might create a sample that does not contain examples for all values in a nominal (factor) variable. You will get an error message. This most likely happens for variables which have one very rare value. You may have to remove the variable. 3.5 Model Comparison We will compare decision trees with a k-nearest neighbors (kNN) classifier. We will create fixed sampling scheme (10-folds) so we compare the different models using exactly the same folds. It is specified as trControl during training. train_index &lt;- createFolds(Zoo_train$type, k = 10) Build models rpartFit &lt;- Zoo_train %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, tuneLength = 10, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index) ) Note: for kNN we ask train to scale the data using preProcess = \"scale\". Logicals will be used as 0-1 variables in Euclidean distance calculation. knnFit &lt;- Zoo_train %&gt;% train(type ~ ., data = ., method = &quot;knn&quot;, preProcess = &quot;scale&quot;, tuneLength = 10, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index) ) Compare accuracy over all folds. resamps &lt;- resamples(list( CART = rpartFit, kNearestNeighbors = knnFit )) summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: CART, kNearestNeighbors ## Number of resamples: 10 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. ## CART 0.6667 0.8750 0.8889 0.8722 0.8889 ## kNearestNeighbors 0.8750 0.9167 1.0000 0.9653 1.0000 ## Max. NA&#39;s ## CART 1 0 ## kNearestNeighbors 1 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. ## CART 0.5909 0.8333 0.8475 0.8342 0.857 ## kNearestNeighbors 0.8333 0.8977 1.0000 0.9547 1.000 ## Max. NA&#39;s ## CART 1 0 ## kNearestNeighbors 1 0 caret provides some visualizations using the package lattice. For example, a boxplot to compare the accuracy and kappa distribution (over the 10 folds). library(lattice) bwplot(resamps, layout = c(3, 1)) We see that kNN is performing consistently better on the folds than CART (except for some outlier folds). Find out if one models is statistically better than the other (is the difference in accuracy is not zero). difs &lt;- diff(resamps) difs ## ## Call: ## diff.resamples(x = resamps) ## ## Models: CART, kNearestNeighbors ## Metrics: Accuracy, Kappa ## Number of differences: 1 ## p-value adjustment: bonferroni summary(difs) ## ## Call: ## summary.diff.resamples(object = difs) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## Accuracy ## CART kNearestNeighbors ## CART -0.0931 ## kNearestNeighbors 0.0115 ## ## Kappa ## CART kNearestNeighbors ## CART -0.121 ## kNearestNeighbors 0.0104 p-values tells you the probability of seeing an even more extreme value (difference between accuracy) given that the null hypothesis (difference = 0) is true. For a better classifier, the p-value should be less than .05 or 0.01. diff automatically applies Bonferroni correction for multiple comparisons. In this case, kNN seems better but the classifiers do not perform statistically differently. 3.6 Feature Selection and Feature Preparation Decision trees implicitly select features for splitting, but we can also select features manually. library(FSelector) see: http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection#The_Feature_Ranking_Approach 3.6.1 Univariate Feature Importance Score These scores measure how related each feature is to the class variable. For discrete features (as in our case), the chi-square statistic can be used to derive a score. weights &lt;- Zoo_train %&gt;% chi.squared(type ~ ., data = .) %&gt;% as_tibble(rownames = &quot;feature&quot;) %&gt;% arrange(desc(attr_importance)) weights ## # A tibble: 16 x 2 ## feature attr_importance ## &lt;chr&gt; &lt;dbl&gt; ## 1 feathers 1 ## 2 milk 1 ## 3 backbone 1 ## 4 toothed 0.975 ## 5 eggs 0.933 ## 6 hair 0.907 ## 7 breathes 0.898 ## 8 airborne 0.848 ## 9 fins 0.845 ## 10 legs 0.828 ## 11 tail 0.779 ## 12 catsize 0.664 ## 13 aquatic 0.655 ## 14 venomous 0.475 ## 15 predator 0.385 ## 16 domestic 0.231 plot importance in descending order (using reorder to order factor levels used by ggplot). ggplot(weights, aes(x = attr_importance, y = reorder(feature, attr_importance))) + geom_bar(stat = &quot;identity&quot;) + xlab(&quot;Importance score&quot;) + ylab(&quot;Feature&quot;) Get the 5 best features subset &lt;- cutoff.k(weights %&gt;% column_to_rownames(&quot;feature&quot;), 5) subset ## [1] &quot;feathers&quot; &quot;milk&quot; &quot;backbone&quot; &quot;toothed&quot; ## [5] &quot;eggs&quot; Use only the best 5 features to build a model (Fselector provides as.simple.formula) f &lt;- as.simple.formula(subset, &quot;type&quot;) f ## type ~ feathers + milk + backbone + toothed + eggs ## &lt;environment: 0x55d79aecaf60&gt; m &lt;- Zoo_train %&gt;% rpart(f, data = .) rpart.plot(m, extra = 2, roundint = FALSE) There are many alternative ways to calculate univariate importance scores (see package FSelector). Some of them (also) work for continuous features. One example is the information gain ratio based on entropy as used in decision tree induction. Zoo_train %&gt;% gain.ratio(type ~ ., data = .) %&gt;% as_tibble(rownames = &quot;feature&quot;) %&gt;% arrange(desc(attr_importance)) ## # A tibble: 16 x 2 ## feature attr_importance ## &lt;chr&gt; &lt;dbl&gt; ## 1 milk 1 ## 2 backbone 1 ## 3 feathers 1 ## 4 toothed 0.919 ## 5 eggs 0.827 ## 6 breathes 0.821 ## 7 hair 0.782 ## 8 fins 0.689 ## 9 legs 0.682 ## 10 airborne 0.671 ## 11 tail 0.573 ## 12 aquatic 0.391 ## 13 catsize 0.383 ## 14 venomous 0.351 ## 15 predator 0.125 ## 16 domestic 0.0975 3.6.2 Feature Subset Selection Often features are related and calculating importance for each feature independently is not optimal. We can use greedy search heuristics. For example cfs uses correlation/entropy with best first search. Zoo_train %&gt;% cfs(type ~ ., data = .) ## [1] &quot;hair&quot; &quot;feathers&quot; &quot;eggs&quot; &quot;milk&quot; ## [5] &quot;toothed&quot; &quot;backbone&quot; &quot;breathes&quot; &quot;fins&quot; ## [9] &quot;legs&quot; &quot;tail&quot; Black-box feature selection uses an evaluator function (the black box) to calculate a score to be maximized. First, we define an evaluation function that builds a model given a subset of features and calculates a quality score. We use here the average for 5 bootstrap samples (method = \"cv\" can also be used instead), no tuning (to be faster), and the average accuracy as the score. evaluator &lt;- function(subset) { model &lt;- Zoo_train %&gt;% train(as.simple.formula(subset, &quot;type&quot;), data = ., method = &quot;rpart&quot;, trControl = trainControl(method = &quot;boot&quot;, number = 5), tuneLength = 0) results &lt;- model$resample$Accuracy cat(&quot;Trying features:&quot;, paste(subset, collapse = &quot; + &quot;), &quot;\\n&quot;) m &lt;- mean(results) cat(&quot;Accuracy:&quot;, round(m, 2), &quot;\\n\\n&quot;) m } Start with all features (but not the class variable type) features &lt;- Zoo_train %&gt;% colnames() %&gt;% setdiff(&quot;type&quot;) There are several (greedy) search strategies available. These run for a while! ##subset &lt;- backward.search(features, evaluator) ##subset &lt;- forward.search(features, evaluator) ##subset &lt;- best.first.search(features, evaluator) ##subset &lt;- hill.climbing.search(features, evaluator) ##subset 3.6.3 Using Dummy Variables for Factors Nominal features (factors) are often encoded as a series of 0-1 dummy variables. For example, let us try to predict if an animal is a predator given the type. First we use the original encoding of type as a factor with several values. tree_predator &lt;- Zoo_train %&gt;% rpart(predator ~ type, data = .) rpart.plot(tree_predator, extra = 2, roundint = FALSE) Note: Some splits use multiple values. Building the tree will become extremely slow if a factor has many levels (different values) since the tree has to check all possible splits into two subsets. This situation should be avoided. Recode type as a set of 0-1 dummy variables using class2ind. See also ? dummyVars in package caret. Zoo_train_dummy &lt;- as_tibble(class2ind(Zoo_train$type)) %&gt;% mutate_all(as.factor) %&gt;% add_column(predator = Zoo_train$predator) Zoo_train_dummy ## # A tibble: 83 x 8 ## mammal bird reptile fish amphibian insect ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 0 0 0 0 0 ## 2 1 0 0 0 0 0 ## 3 0 0 0 1 0 0 ## 4 1 0 0 0 0 0 ## 5 1 0 0 0 0 0 ## 6 1 0 0 0 0 0 ## 7 0 0 0 1 0 0 ## 8 0 0 0 1 0 0 ## 9 1 0 0 0 0 0 ## 10 0 1 0 0 0 0 ## # … with 73 more rows, and 2 more variables: ## # mollusc.et.al &lt;fct&gt;, predator &lt;fct&gt; tree_predator &lt;- Zoo_train_dummy %&gt;% rpart(predator ~ ., data = ., control = rpart.control(minsplit = 2, cp = 0.01)) rpart.plot(tree_predator, roundint = FALSE) Using caret on the original factor encoding automatically translates factors (here type) into 0-1 dummy variables (e.g., typeinsect = 0). The reason is that some models cannot directly use factors and caret tries to consistently work with all of them. fit &lt;- Zoo_train %&gt;% train(predator ~ type, data = ., method = &quot;rpart&quot;, control = rpart.control(minsplit = 2), tuneGrid = data.frame(cp = 0.01)) fit ## CART ## ## 83 samples ## 1 predictor ## 2 classes: &#39;TRUE&#39;, &#39;FALSE&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 83, 83, 83, 83, 83, 83, ... ## Resampling results: ## ## Accuracy Kappa ## 0.606 0.2034 ## ## Tuning parameter &#39;cp&#39; was held constant at a value ## of 0.01 rpart.plot(fit$finalModel, extra = 2) Note: To use a fixed value for the tuning parameter cp, we have to create a tuning grid that only icontains that value. 3.7 Class Imbalance Classifiers have a hard time to learn from data where we have much more observations for one class (called the majority class). This is called the class imbalance problem. Here is a very good article about the problem and solutions. library(rpart) library(rpart.plot) data(Zoo, package=&quot;mlbench&quot;) Class distribution ggplot(Zoo, aes(y = type)) + geom_bar() To create an imbalanced problem, we want to decide if an animal is an reptile. First, we change the class variable to make it into a binary reptile/no reptile classification problem. Note: We use here the training data for testing. You should use a separate testing data set! Zoo_reptile &lt;- Zoo %&gt;% mutate( type = factor(Zoo$type == &quot;reptile&quot;, levels = c(FALSE, TRUE), labels = c(&quot;nonreptile&quot;, &quot;reptile&quot;))) Do not forget to make the class variable a factor (a nominal variable) or you will get a regression tree instead of a classification tree. summary(Zoo_reptile) ## hair feathers eggs ## Mode :logical Mode :logical Mode :logical ## FALSE:58 FALSE:81 FALSE:42 ## TRUE :43 TRUE :20 TRUE :59 ## ## ## ## milk airborne aquatic ## Mode :logical Mode :logical Mode :logical ## FALSE:60 FALSE:77 FALSE:65 ## TRUE :41 TRUE :24 TRUE :36 ## ## ## ## predator toothed backbone ## Mode :logical Mode :logical Mode :logical ## FALSE:45 FALSE:40 FALSE:18 ## TRUE :56 TRUE :61 TRUE :83 ## ## ## ## breathes venomous fins ## Mode :logical Mode :logical Mode :logical ## FALSE:21 FALSE:93 FALSE:84 ## TRUE :80 TRUE :8 TRUE :17 ## ## ## ## legs tail domestic ## Min. :0.00 Mode :logical Mode :logical ## 1st Qu.:2.00 FALSE:26 FALSE:88 ## Median :4.00 TRUE :75 TRUE :13 ## Mean :2.84 ## 3rd Qu.:4.00 ## Max. :8.00 ## catsize type ## Mode :logical nonreptile:96 ## FALSE:57 reptile : 5 ## TRUE :44 ## ## ## See if we have a class imbalance problem. ggplot(Zoo_reptile, aes(y = type)) + geom_bar() Create test and training data. I use here a 50/50 split to make sure that the test set has some samples of the rare reptile class. set.seed(1234) inTrain &lt;- createDataPartition(y = Zoo_reptile$type, p = .5, list = FALSE) training_reptile &lt;- Zoo_reptile %&gt;% slice(inTrain) testing_reptile &lt;- Zoo_reptile %&gt;% slice(-inTrain) the new class variable is clearly not balanced. This is a problem for building a tree! 3.7.1 Option 1: Use the Data As Is and Hope For The Best fit &lt;- training_reptile %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, trControl = trainControl(method = &quot;cv&quot;)) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = ## weights, info = trainInfo, : There were missing values ## in resampled performance measures. Warnings: “There were missing values in resampled performance measures.” means that some test folds did not contain examples of both classes. This is very likely with class imbalance and small datasets. fit ## CART ## ## 51 samples ## 16 predictors ## 2 classes: &#39;nonreptile&#39;, &#39;reptile&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 46, 47, 46, 46, 45, 46, ... ## Resampling results: ## ## Accuracy Kappa ## 0.9467 0 ## ## Tuning parameter &#39;cp&#39; was held constant at a value of 0 rpart.plot(fit$finalModel, extra = 2) the tree predicts everything as non-reptile. Have a look at the error on the test set. confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 48 2 ## reptile 0 0 ## ## Accuracy : 0.96 ## 95% CI : (0.863, 0.995) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 0.677 ## ## Kappa : 0 ## ## Mcnemar&#39;s Test P-Value : 0.480 ## ## Sensitivity : 0.00 ## Specificity : 1.00 ## Pos Pred Value : NaN ## Neg Pred Value : 0.96 ## Prevalence : 0.04 ## Detection Rate : 0.00 ## Detection Prevalence : 0.00 ## Balanced Accuracy : 0.50 ## ## &#39;Positive&#39; Class : reptile ## Accuracy is high, but it is exactly the same as the no-information rate and kappa is zero. Sensitivity is also zero, meaning that we do not identify any positive (reptile). If the cost of missing a positive is much larger than the cost associated with misclassifying a negative, then accuracy is not a good measure! By dealing with imbalance, we are not concerned with accuracy, but we want to increase the sensitivity, i.e., the chance to identify positive examples. Note: The positive class value (the one that you want to detect) is set manually to reptile using positive = \"reptile\". Otherwise sensitivity/specificity will not be correctly calculated. 3.7.2 Option 2: Balance Data With Resampling We use stratified sampling with replacement (to oversample the minority/positive class). You could also use SMOTE (in package DMwR) or other sampling strategies (e.g., from package unbalanced). We use 50+50 observations here (Note: many samples will be chosen several times). library(sampling) set.seed(1000) # for repeatability id &lt;- strata(training_reptile, stratanames = &quot;type&quot;, size = c(50, 50), method = &quot;srswr&quot;) training_reptile_balanced &lt;- training_reptile %&gt;% slice(id$ID_unit) table(training_reptile_balanced$type) ## ## nonreptile reptile ## 50 50 fit &lt;- training_reptile_balanced %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, trControl = trainControl(method = &quot;cv&quot;), control = rpart.control(minsplit = 5)) fit ## CART ## ## 100 samples ## 16 predictor ## 2 classes: &#39;nonreptile&#39;, &#39;reptile&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 90, 90, 90, 90, 90, 90, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.18 0.81 0.62 ## 0.30 0.63 0.26 ## 0.34 0.53 0.06 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was cp = 0.18. rpart.plot(fit$finalModel, extra = 2) Check on the unbalanced testing data. confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 19 0 ## reptile 29 2 ## ## Accuracy : 0.42 ## 95% CI : (0.282, 0.568) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.05 ## ## Mcnemar&#39;s Test P-Value : 2e-07 ## ## Sensitivity : 1.0000 ## Specificity : 0.3958 ## Pos Pred Value : 0.0645 ## Neg Pred Value : 1.0000 ## Prevalence : 0.0400 ## Detection Rate : 0.0400 ## Detection Prevalence : 0.6200 ## Balanced Accuracy : 0.6979 ## ## &#39;Positive&#39; Class : reptile ## Note that the accuracy is below the no information rate! However, kappa (improvement of accuracy over randomness) and sensitivity (the ability to identify reptiles) have increased. There is a tradeoff between sensitivity and specificity (how many of the identified animals are really reptiles) The tradeoff can be controlled using the sample proportions. We can sample more reptiles to increase sensitivity at the cost of lower specificity (this effect cannot be seen in the data since the test set has only a few reptiles). id &lt;- strata(training_reptile, stratanames = &quot;type&quot;, size = c(50, 100), method = &quot;srswr&quot;) training_reptile_balanced &lt;- training_reptile %&gt;% slice(id$ID_unit) table(training_reptile_balanced$type) ## ## nonreptile reptile ## 50 100 fit &lt;- training_reptile_balanced %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, trControl = trainControl(method = &quot;cv&quot;), control = rpart.control(minsplit = 5)) confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 33 0 ## reptile 15 2 ## ## Accuracy : 0.7 ## 95% CI : (0.554, 0.821) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 1.000000 ## ## Kappa : 0.15 ## ## Mcnemar&#39;s Test P-Value : 0.000301 ## ## Sensitivity : 1.000 ## Specificity : 0.688 ## Pos Pred Value : 0.118 ## Neg Pred Value : 1.000 ## Prevalence : 0.040 ## Detection Rate : 0.040 ## Detection Prevalence : 0.340 ## Balanced Accuracy : 0.844 ## ## &#39;Positive&#39; Class : reptile ## 3.7.3 Option 3: Build A Larger Tree and use Predicted Probabilities Increase complexity and require less data for splitting a node. Here I also use AUC (area under the ROC) as the tuning metric. You need to specify the two class summary function. Note that the tree still trying to improve accuracy on the data and not AUC! I also enable class probabilities since I want to predict probabilities later. fit &lt;- training_reptile %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, tuneLength = 10, trControl = trainControl(method = &quot;cv&quot;, classProbs = TRUE, ## necessary for predict with type=&quot;prob&quot; summaryFunction=twoClassSummary), ## necessary for ROC metric = &quot;ROC&quot;, control = rpart.control(minsplit = 3)) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = ## weights, info = trainInfo, : There were missing values ## in resampled performance measures. fit ## CART ## ## 51 samples ## 16 predictors ## 2 classes: &#39;nonreptile&#39;, &#39;reptile&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 46, 47, 46, 46, 46, 45, ... ## Resampling results: ## ## ROC Sens Spec ## 0.3583 0.975 0 ## ## Tuning parameter &#39;cp&#39; was held constant at a value of 0 rpart.plot(fit$finalModel, extra = 2) confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 48 2 ## reptile 0 0 ## ## Accuracy : 0.96 ## 95% CI : (0.863, 0.995) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 0.677 ## ## Kappa : 0 ## ## Mcnemar&#39;s Test P-Value : 0.480 ## ## Sensitivity : 0.00 ## Specificity : 1.00 ## Pos Pred Value : NaN ## Neg Pred Value : 0.96 ## Prevalence : 0.04 ## Detection Rate : 0.00 ## Detection Prevalence : 0.00 ## Balanced Accuracy : 0.50 ## ## &#39;Positive&#39; Class : reptile ## Note: Accuracy is high, but it is close or below to the no-information rate! 3.7.3.1 Create A Biased Classifier We can create a classifier which will detect more reptiles at the expense of misclassifying non-reptiles. This is equivalent to increasing the cost of misclassifying a reptile as a non-reptile. The usual rule is to predict in each node the majority class from the test data in the node. For a binary classification problem that means a probability of &gt;50%. In the following, we reduce this threshold to 1% or more. This means that if the new observation ends up in a leaf node with 1% or more reptiles from training then the observation will be classified as a reptile. The data set is small and this works better with more data. prob &lt;- predict(fit, testing_reptile, type = &quot;prob&quot;) tail(prob) ## nonreptile reptile ## tuna 1.0000 0.00000 ## vole 0.9615 0.03846 ## wasp 0.5000 0.50000 ## wolf 0.9615 0.03846 ## worm 1.0000 0.00000 ## wren 0.9615 0.03846 pred &lt;- as.factor(ifelse(prob[,&quot;reptile&quot;]&gt;=0.01, &quot;reptile&quot;, &quot;nonreptile&quot;)) confusionMatrix(data = pred, ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 13 0 ## reptile 35 2 ## ## Accuracy : 0.3 ## 95% CI : (0.179, 0.446) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.029 ## ## Mcnemar&#39;s Test P-Value : 9.08e-09 ## ## Sensitivity : 1.0000 ## Specificity : 0.2708 ## Pos Pred Value : 0.0541 ## Neg Pred Value : 1.0000 ## Prevalence : 0.0400 ## Detection Rate : 0.0400 ## Detection Prevalence : 0.7400 ## Balanced Accuracy : 0.6354 ## ## &#39;Positive&#39; Class : reptile ## Note that accuracy goes down and is below the no information rate. However, both measures are based on the idea that all errors have the same cost. What is important is that we are now able to find more reptiles. 3.7.3.2 Plot the ROC Curve Since we have a binary classification problem and a classifier that predicts a probability for an observation to be a reptile, we can also use a receiver operating characteristic (ROC) curve. For the ROC curve all different cutoff thresholds for the probability are used and then connected with a line. The area under the curve represents a single number for how well the classifier works (the closer to one, the better). library(&quot;pROC&quot;) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var r &lt;- roc(testing_reptile$type == &quot;reptile&quot;, prob[,&quot;reptile&quot;]) ## Setting levels: control = FALSE, case = TRUE ## Setting direction: controls &lt; cases r ## ## Call: ## roc.default(response = testing_reptile$type == &quot;reptile&quot;, predictor = prob[, &quot;reptile&quot;]) ## ## Data: prob[, &quot;reptile&quot;] in 48 controls (testing_reptile$type == &quot;reptile&quot; FALSE) &lt; 2 cases (testing_reptile$type == &quot;reptile&quot; TRUE). ## Area under the curve: 0.766 ggroc(r) + geom_abline(intercept = 1, slope = 1, color = &quot;darkgrey&quot;) 3.7.4 Option 4: Use a Cost-Sensitive Classifier The implementation of CART in rpart can use a cost matrix for making splitting decisions (as parameter loss). The matrix has the form TP FP FN TN TP and TN have to be 0. We make FN very expensive (100). cost &lt;- matrix(c( 0, 1, 100, 0 ), byrow = TRUE, nrow = 2) cost ## [,1] [,2] ## [1,] 0 1 ## [2,] 100 0 fit &lt;- training_reptile %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, parms = list(loss = cost), trControl = trainControl(method = &quot;cv&quot;)) The warning “There were missing values in resampled performance measures” means that some folds did not contain any reptiles (because of the class imbalance) and thus the performance measures could not be calculates. fit ## CART ## ## 51 samples ## 16 predictors ## 2 classes: &#39;nonreptile&#39;, &#39;reptile&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 46, 46, 46, 45, 46, 45, ... ## Resampling results: ## ## Accuracy Kappa ## 0.4767 -0.03039 ## ## Tuning parameter &#39;cp&#39; was held constant at a value of 0 rpart.plot(fit$finalModel, extra = 2) confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 39 0 ## reptile 9 2 ## ## Accuracy : 0.82 ## 95% CI : (0.686, 0.914) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 0.99998 ## ## Kappa : 0.257 ## ## Mcnemar&#39;s Test P-Value : 0.00766 ## ## Sensitivity : 1.000 ## Specificity : 0.812 ## Pos Pred Value : 0.182 ## Neg Pred Value : 1.000 ## Prevalence : 0.040 ## Detection Rate : 0.040 ## Detection Prevalence : 0.220 ## Balanced Accuracy : 0.906 ## ## &#39;Positive&#39; Class : reptile ## The high cost for false negatives results in a classifier that does not miss any reptile. Note: Using a cost-sensitive classifier is often the best option. Unfortunately, the most classification algorithms (or their implementation) do not have the ability to consider misclassification cost. References "],["classification-alternative-techniques.html", "Chapter 4 Classification: Alternative Techniques 4.1 Training and Test Data 4.2 Fitting Different Classification Models to the Training Data 4.3 Comparing Models 4.4 Applying the Chosen Model to the Test Data 4.5 Decision Boundaries 4.6 More Information", " Chapter 4 Classification: Alternative Techniques Packages used for this chapter: C50 (Kuhn and Quinlan 2021), caret (Kuhn 2021), e1071 (Meyer et al. 2021), keras (Allaire and Chollet 2021), lattice (Sarkar 2021), MASS (Ripley 2021a), mlbench (Leisch and Dimitriadou. 2021), nnet (Ripley 2021b), randomForest (Breiman et al. 2018), rpart (Therneau and Atkinson 2019), RWeka (Hornik 2020), scales (Wickham and Seidel 2020), tidyverse (Wickham 2021c) We will use tidyverse to prepare the data. library(tidyverse) Show fewer digits options(digits=3) 4.1 Training and Test Data We will use the Zoo dataset which is included in the R package mlbench (you may have to install it). The Zoo dataset containing 17 (mostly logical) variables on different 101 animals as a data frame with 17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize, type). We convert the data frame into a tidyverse tibble (optional). data(Zoo, package=&quot;mlbench&quot;) Zoo &lt;- as_tibble(Zoo) Zoo ## # A tibble: 101 x 17 ## hair feathers eggs milk airborne aquatic predator ## &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 TRUE FALSE FALSE TRUE FALSE FALSE TRUE ## 2 TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## 3 FALSE FALSE TRUE FALSE FALSE TRUE TRUE ## 4 TRUE FALSE FALSE TRUE FALSE FALSE TRUE ## 5 TRUE FALSE FALSE TRUE FALSE FALSE TRUE ## 6 TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## 7 TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## 8 FALSE FALSE TRUE FALSE FALSE TRUE FALSE ## 9 FALSE FALSE TRUE FALSE FALSE TRUE TRUE ## 10 TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## # … with 91 more rows, and 10 more variables: ## # toothed &lt;lgl&gt;, backbone &lt;lgl&gt;, breathes &lt;lgl&gt;, ## # venomous &lt;lgl&gt;, fins &lt;lgl&gt;, legs &lt;int&gt;, ## # tail &lt;lgl&gt;, domestic &lt;lgl&gt;, catsize &lt;lgl&gt;, ## # type &lt;fct&gt; We will use the package caret to make preparing training sets and building classification (and regression) models easier. A great cheat sheet can be found here. library(caret) Use multi-core support for cross-validation. Note: It is commented out because it does not work with rJava used in RWeka below. ##library(doMC, quietly = TRUE) ##registerDoMC(cores = 4) ##getDoParWorkers() Test data is not used in the model building process and needs to be set aside purely for testing the model after it is completely built. Here I use 80% for training. inTrain &lt;- createDataPartition(y = Zoo$type, p = .8, list = FALSE) Zoo_train &lt;- Zoo %&gt;% slice(inTrain) Zoo_test &lt;- Zoo %&gt;% slice(-inTrain) 4.2 Fitting Different Classification Models to the Training Data Create a fixed sampling scheme (10-folds) so we can compare the fitted models later. train_index &lt;- createFolds(Zoo_train$type, k = 10) The fixed folds are used in train() with the argument trControl = trainControl(method = \"cv\", indexOut = train_index)). If you don’t need fixed folds, then remove indexOut = train_index in the code below. For help with building models in caret see: ? train Note: Be careful if you have many NA values in your data. train() and cross-validation many fail in some cases. If that is the case then you can remove features (columns) which have many NAs, omit NAs using na.omit() or use imputation to replace them with reasonable values (e.g., by the feature mean or via kNN). Highly imbalanced datasets are also problematic since there is a chance that a fold does not contain examples of each class leading to a hard to understand error message. 4.2.1 Conditional Inference Tree (Decision Tree) ctreeFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;ctree&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) ctreeFit ## Conditional Inference Tree ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 75, 76, 74, 74, 76, 74, ... ## Resampling results across tuning parameters: ## ## mincriterion Accuracy Kappa ## 0.010 0.808 0.747 ## 0.255 0.808 0.747 ## 0.500 0.808 0.747 ## 0.745 0.808 0.747 ## 0.990 0.808 0.747 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was mincriterion ## = 0.99. plot(ctreeFit$finalModel) The final model can be directly used for predict() predict(ctreeFit, head(Zoo_test)) ## [1] mammal mollusc.et.al bird ## [4] mammal mollusc.et.al bird ## 7 Levels: mammal bird reptile fish ... mollusc.et.al 4.2.2 C 4.5 Decision Tree library(RWeka) C45Fit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;J48&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) C45Fit ## C4.5-like Trees ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 76, 73, 74, 74, 76, 76, ... ## Resampling results across tuning parameters: ## ## C M Accuracy Kappa ## 0.010 1 0.978 0.971 ## 0.010 2 0.978 0.971 ## 0.010 3 0.978 0.971 ## 0.010 4 0.907 0.879 ## 0.010 5 0.918 0.893 ## 0.133 1 0.978 0.971 ## 0.133 2 0.978 0.971 ## 0.133 3 0.978 0.971 ## 0.133 4 0.907 0.879 ## 0.133 5 0.918 0.893 ## 0.255 1 0.989 0.985 ## 0.255 2 0.989 0.985 ## 0.255 3 0.978 0.971 ## 0.255 4 0.907 0.879 ## 0.255 5 0.918 0.893 ## 0.378 1 0.989 0.985 ## 0.378 2 0.989 0.985 ## 0.378 3 0.978 0.971 ## 0.378 4 0.907 0.879 ## 0.378 5 0.918 0.893 ## 0.500 1 0.989 0.985 ## 0.500 2 0.989 0.985 ## 0.500 3 0.978 0.971 ## 0.500 4 0.907 0.879 ## 0.500 5 0.918 0.893 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final values used for the model were C = 0.255 ## and M = 1. C45Fit$finalModel ## J48 pruned tree ## ------------------ ## ## feathersTRUE &lt;= 0 ## | milkTRUE &lt;= 0 ## | | toothedTRUE &lt;= 0 ## | | | airborneTRUE &lt;= 0 ## | | | | predatorTRUE &lt;= 0 ## | | | | | legs &lt;= 2: mollusc.et.al (2.0) ## | | | | | legs &gt; 2: insect (2.0) ## | | | | predatorTRUE &gt; 0: mollusc.et.al (6.0) ## | | | airborneTRUE &gt; 0: insect (5.0) ## | | toothedTRUE &gt; 0 ## | | | finsTRUE &lt;= 0 ## | | | | aquaticTRUE &lt;= 0: reptile (3.0) ## | | | | aquaticTRUE &gt; 0 ## | | | | | eggsTRUE &lt;= 0: reptile (1.0) ## | | | | | eggsTRUE &gt; 0: amphibian (4.0) ## | | | finsTRUE &gt; 0: fish (11.0) ## | milkTRUE &gt; 0: mammal (33.0) ## feathersTRUE &gt; 0: bird (16.0) ## ## Number of Leaves : 10 ## ## Size of the tree : 19 4.2.3 K-Nearest Neighbors Note: kNN uses Euclidean distance, so data should be standardized (scaled) first. Here legs are measured between 0 and 6 while all other variables are between 0 and 1. Scaling can be directly performed as preprocessing in train using the parameter preProcess = \"scale\". knnFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;knn&quot;, data = ., preProcess = &quot;scale&quot;, tuneLength = 5, tuneGrid=data.frame(k = 1:10), trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) knnFit ## k-Nearest Neighbors ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## Pre-processing: scaled (16) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 74, 73, 76, 74, 74, 75, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 1.000 1.000 ## 2 0.978 0.971 ## 3 0.967 0.957 ## 4 0.943 0.926 ## 5 0.965 0.954 ## 6 0.916 0.891 ## 7 0.883 0.850 ## 8 0.872 0.835 ## 9 0.883 0.848 ## 10 0.908 0.881 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was k = 1. knnFit$finalModel ## 1-nearest neighbor model ## Training set outcome distribution: ## ## mammal bird reptile ## 33 16 4 ## fish amphibian insect ## 11 4 7 ## mollusc.et.al ## 8 4.2.4 PART (Rule-based classifier) rulesFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;PART&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) rulesFit ## Rule-Based Classifier ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 77, 72, 77, 74, 74, 73, ... ## Resampling results across tuning parameters: ## ## threshold pruned Accuracy Kappa ## 0.010 yes 0.965 0.955 ## 0.010 no 0.988 0.984 ## 0.133 yes 0.965 0.955 ## 0.133 no 0.988 0.984 ## 0.255 yes 0.965 0.955 ## 0.255 no 0.988 0.984 ## 0.378 yes 0.965 0.955 ## 0.378 no 0.988 0.984 ## 0.500 yes 0.965 0.955 ## 0.500 no 0.988 0.984 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final values used for the model were threshold ## = 0.5 and pruned = no. rulesFit$finalModel ## PART decision list ## ------------------ ## ## feathersTRUE &lt;= 0 AND ## milkTRUE &gt; 0: mammal (33.0) ## ## feathersTRUE &gt; 0: bird (16.0) ## ## toothedTRUE &lt;= 0 AND ## airborneTRUE &lt;= 0 AND ## predatorTRUE &gt; 0: mollusc.et.al (6.0) ## ## toothedTRUE &lt;= 0 AND ## legs &gt; 2: insect (7.0) ## ## finsTRUE &gt; 0: fish (11.0) ## ## toothedTRUE &gt; 0 AND ## aquaticTRUE &lt;= 0: reptile (3.0) ## ## aquaticTRUE &gt; 0 AND ## venomousTRUE &lt;= 0: amphibian (3.0) ## ## aquaticTRUE &lt;= 0: mollusc.et.al (2.0) ## ## : reptile (2.0/1.0) ## ## Number of Rules : 9 4.2.5 Linear Support Vector Machines svmFit &lt;- Zoo_train %&gt;% train(type ~., method = &quot;svmLinear&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) svmFit ## Support Vector Machines with Linear Kernel ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 73, 75, 75, 74, 74, 76, ... ## Resampling results: ## ## Accuracy Kappa ## 1 1 ## ## Tuning parameter &#39;C&#39; was held constant at a value of 1 svmFit$finalModel ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Linear (vanilla) kernel function. ## ## Number of Support Vectors : 44 ## ## Objective Function Value : -0.143 -0.198 -0.148 -0.175 -0.0945 -0.104 -0.19 -0.0814 -0.154 -0.0917 -0.115 -0.177 -0.568 -0.104 -0.15 -0.119 -0.0478 -0.083 -0.123 -0.148 -0.58 ## Training error : 0 4.2.6 Random Forest randomForestFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;rf&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) randomForestFit ## Random Forest ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 74, 76, 75, 74, 73, 76, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.976 0.968 ## 5 0.976 0.968 ## 9 0.976 0.968 ## 12 0.965 0.954 ## 16 0.976 0.969 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was mtry = 2. randomForestFit$finalModel ## ## Call: ## randomForest(x = x, y = y, mtry = min(param$mtry, ncol(x))) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 3.61% ## Confusion matrix: ## mammal bird reptile fish amphibian ## mammal 33 0 0 0 0 ## bird 0 16 0 0 0 ## reptile 0 0 2 1 1 ## fish 0 0 0 11 0 ## amphibian 0 0 0 0 4 ## insect 0 0 0 0 0 ## mollusc.et.al 0 0 0 0 0 ## insect mollusc.et.al class.error ## mammal 0 0 0.000 ## bird 0 0 0.000 ## reptile 0 0 0.500 ## fish 0 0 0.000 ## amphibian 0 0 0.000 ## insect 7 0 0.000 ## mollusc.et.al 1 7 0.125 4.2.7 Gradient Boosted Decision Trees (xgboost) xgboostFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;xgbTree&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index), tuneGrid = expand.grid( nrounds = 20, max_depth = 3, colsample_bytree = .6, eta = 0.1, gamma=0, min_child_weight = 1, subsample = .5 )) xgboostFit ## eXtreme Gradient Boosting ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 76, 75, 75, 74, 76, 74, ... ## Resampling results: ## ## Accuracy Kappa ## 0.976 0.969 ## ## Tuning parameter &#39;nrounds&#39; was held constant at ## a value of 1 ## Tuning parameter &#39;subsample&#39; was ## held constant at a value of 0.5 xgboostFit$finalModel ## ##### xgb.Booster ## raw: 83.5 Kb ## call: ## xgboost::xgb.train(params = list(eta = param$eta, max_depth = param$max_depth, ## gamma = param$gamma, colsample_bytree = param$colsample_bytree, ## min_child_weight = param$min_child_weight, subsample = param$subsample), ## data = x, nrounds = param$nrounds, num_class = length(lev), ## objective = &quot;multi:softprob&quot;) ## params (as set within xgb.train): ## eta = &quot;0.1&quot;, max_depth = &quot;3&quot;, gamma = &quot;0&quot;, colsample_bytree = &quot;0.6&quot;, min_child_weight = &quot;1&quot;, subsample = &quot;0.5&quot;, num_class = &quot;7&quot;, objective = &quot;multi:softprob&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.print.evaluation(period = print_every_n) ## # of features: 16 ## niter: 20 ## nfeatures : 16 ## xNames : hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE ## problemType : Classification ## tuneValue : ## nrounds max_depth eta gamma colsample_bytree ## 1 20 3 0.1 0 0.6 ## min_child_weight subsample ## 1 1 0.5 ## obsLevels : mammal bird reptile fish amphibian insect mollusc.et.al ## param : ## list() 4.2.8 Artificial Neural Network nnetFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;nnet&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index), trace = FALSE) nnetFit ## Neural Network ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 75, 77, 72, 75, 75, 76, ... ## Resampling results across tuning parameters: ## ## size decay Accuracy Kappa ## 1 0e+00 0.694 0.558 ## 1 1e-04 0.807 0.728 ## 1 1e-03 0.892 0.852 ## 1 1e-02 0.825 0.766 ## 1 1e-01 0.727 0.633 ## 3 0e+00 0.954 0.939 ## 3 1e-04 0.989 0.986 ## 3 1e-03 0.989 0.986 ## 3 1e-02 0.989 0.986 ## 3 1e-01 0.989 0.986 ## 5 0e+00 0.939 0.917 ## 5 1e-04 0.965 0.954 ## 5 1e-03 0.989 0.986 ## 5 1e-02 0.989 0.986 ## 5 1e-01 0.989 0.986 ## 7 0e+00 0.989 0.986 ## 7 1e-04 0.989 0.986 ## 7 1e-03 0.989 0.986 ## 7 1e-02 1.000 1.000 ## 7 1e-01 0.989 0.986 ## 9 0e+00 0.989 0.986 ## 9 1e-04 0.989 0.986 ## 9 1e-03 0.989 0.986 ## 9 1e-02 0.989 0.986 ## 9 1e-01 1.000 1.000 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final values used for the model were size = 7 ## and decay = 0.01. nnetFit$finalModel ## a 16-7-7 network with 175 weights ## inputs: hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE ## output(s): .outcome ## options were - softmax modelling decay=0.01 4.3 Comparing Models Collect the performance metrics from the models trained on the same data. resamps &lt;- resamples(list( ctree = ctreeFit, C45 = C45Fit, SVM = svmFit, KNN = knnFit, rules = rulesFit, randomForest = randomForestFit, xgboost = xgboostFit, NeuralNet = nnetFit )) resamps ## ## Call: ## resamples.default(x = list(ctree = ctreeFit, C45 ## = rulesFit, randomForest = randomForestFit, xgboost ## = xgboostFit, NeuralNet = nnetFit)) ## ## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet ## Number of resamples: 10 ## Performance metrics: Accuracy, Kappa ## Time estimates for: everything, final model fit Calculate summary statistics summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet ## Number of resamples: 10 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## ctree 0.750 0.778 0.778 0.808 0.851 0.889 ## C45 0.889 1.000 1.000 0.989 1.000 1.000 ## SVM 1.000 1.000 1.000 1.000 1.000 1.000 ## KNN 1.000 1.000 1.000 1.000 1.000 1.000 ## rules 0.875 1.000 1.000 0.988 1.000 1.000 ## randomForest 0.875 1.000 1.000 0.976 1.000 1.000 ## xgboost 0.875 1.000 1.000 0.976 1.000 1.000 ## NeuralNet 1.000 1.000 1.000 1.000 1.000 1.000 ## NA&#39;s ## ctree 0 ## C45 0 ## SVM 0 ## KNN 0 ## rules 0 ## randomForest 0 ## xgboost 0 ## NeuralNet 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## ctree 0.673 0.701 0.723 0.747 0.798 0.852 ## C45 0.850 1.000 1.000 0.985 1.000 1.000 ## SVM 1.000 1.000 1.000 1.000 1.000 1.000 ## KNN 1.000 1.000 1.000 1.000 1.000 1.000 ## rules 0.837 1.000 1.000 0.984 1.000 1.000 ## randomForest 0.833 1.000 1.000 0.968 1.000 1.000 ## xgboost 0.833 1.000 1.000 0.969 1.000 1.000 ## NeuralNet 1.000 1.000 1.000 1.000 1.000 1.000 ## NA&#39;s ## ctree 0 ## C45 0 ## SVM 0 ## KNN 0 ## rules 0 ## randomForest 0 ## xgboost 0 ## NeuralNet 0 library(lattice) bwplot(resamps, layout = c(3, 1)) Perform inference about differences between models. For each metric, all pair-wise differences are computed and tested to assess if the difference is equal to zero. By default Bonferroni correction for multiple comparison is used. Differences are shown in the upper triangle and p-values are in the lower triangle. difs &lt;- diff(resamps) difs ## ## Call: ## diff.resamples(x = resamps) ## ## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet ## Metrics: Accuracy, Kappa ## Number of differences: 28 ## p-value adjustment: bonferroni summary(difs) ## ## Call: ## summary.diff.resamples(object = difs) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## Accuracy ## ctree C45 SVM KNN ## ctree -0.18095 -0.19206 -0.19206 ## C45 0.000109 -0.01111 -0.01111 ## SVM 3.49e-05 1.000000 0.00000 ## KNN 3.49e-05 1.000000 NA ## rules 5.75e-05 1.000000 1.000000 1.000000 ## randomForest 0.000126 1.000000 1.000000 1.000000 ## xgboost 0.001617 1.000000 1.000000 1.000000 ## NeuralNet 3.49e-05 1.000000 NA NA ## rules randomForest xgboost NeuralNet ## ctree -0.17956 -0.16845 -0.16845 -0.19206 ## C45 0.00139 0.01250 0.01250 -0.01111 ## SVM 0.01250 0.02361 0.02361 0.00000 ## KNN 0.01250 0.02361 0.02361 0.00000 ## rules 0.01111 0.01111 -0.01250 ## randomForest 1.000000 0.00000 -0.02361 ## xgboost 1.000000 1.000000 -0.02361 ## NeuralNet 1.000000 1.000000 1.000000 ## ## Kappa ## ctree C45 SVM KNN ## ctree -0.238389 -0.253389 -0.253389 ## C45 6.36e-05 -0.015000 -0.015000 ## SVM 2.08e-05 1.00000 0.000000 ## KNN 2.08e-05 1.00000 NA ## rules 3.70e-05 1.00000 1.00000 1.00000 ## randomForest 7.76e-05 1.00000 1.00000 1.00000 ## xgboost 0.00124 1.00000 1.00000 1.00000 ## NeuralNet 2.08e-05 1.00000 NA NA ## rules randomForest xgboost ## ctree -0.237063 -0.221723 -0.222437 ## C45 0.001327 0.016667 0.015952 ## SVM 0.016327 0.031667 0.030952 ## KNN 0.016327 0.031667 0.030952 ## rules 0.015340 0.014626 ## randomForest 1.00000 -0.000714 ## xgboost 1.00000 1.00000 ## NeuralNet 1.00000 1.00000 1.00000 ## NeuralNet ## ctree -0.253389 ## C45 -0.015000 ## SVM 0.000000 ## KNN 0.000000 ## rules -0.016327 ## randomForest -0.031667 ## xgboost -0.030952 ## NeuralNet All perform similarly well except ctree (differences in the first row are negative and the p-values in the first column are &lt;.05 indicating that the null-hypothesis of a difference of 0 can be rejected). 4.4 Applying the Chosen Model to the Test Data Most models do similarly well on the data. We choose here the random forest model. pr &lt;- predict(randomForestFit, Zoo_test) pr ## [1] mammal mollusc.et.al bird ## [4] mammal insect bird ## [7] mammal mollusc.et.al mammal ## [10] mammal bird bird ## [13] fish mammal fish ## [16] mammal bird mammal ## 7 Levels: mammal bird reptile fish ... mollusc.et.al Calculate the confusion matrix for the held-out test data. confusionMatrix(pr, reference = Zoo_test$type) ## Confusion Matrix and Statistics ## ## Reference ## Prediction mammal bird reptile fish amphibian ## mammal 8 0 0 0 0 ## bird 0 4 1 0 0 ## reptile 0 0 0 0 0 ## fish 0 0 0 2 0 ## amphibian 0 0 0 0 0 ## insect 0 0 0 0 0 ## mollusc.et.al 0 0 0 0 0 ## Reference ## Prediction insect mollusc.et.al ## mammal 0 0 ## bird 0 0 ## reptile 0 0 ## fish 0 0 ## amphibian 0 0 ## insect 1 0 ## mollusc.et.al 0 2 ## ## Overall Statistics ## ## Accuracy : 0.944 ## 95% CI : (0.727, 0.999) ## No Information Rate : 0.444 ## P-Value [Acc &gt; NIR] : 1.08e-05 ## ## Kappa : 0.922 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: mammal Class: bird ## Sensitivity 1.000 1.000 ## Specificity 1.000 0.929 ## Pos Pred Value 1.000 0.800 ## Neg Pred Value 1.000 1.000 ## Prevalence 0.444 0.222 ## Detection Rate 0.444 0.222 ## Detection Prevalence 0.444 0.278 ## Balanced Accuracy 1.000 0.964 ## Class: reptile Class: fish ## Sensitivity 0.0000 1.000 ## Specificity 1.0000 1.000 ## Pos Pred Value NaN 1.000 ## Neg Pred Value 0.9444 1.000 ## Prevalence 0.0556 0.111 ## Detection Rate 0.0000 0.111 ## Detection Prevalence 0.0000 0.111 ## Balanced Accuracy 0.5000 1.000 ## Class: amphibian Class: insect ## Sensitivity NA 1.0000 ## Specificity 1 1.0000 ## Pos Pred Value NA 1.0000 ## Neg Pred Value NA 1.0000 ## Prevalence 0 0.0556 ## Detection Rate 0 0.0556 ## Detection Prevalence 0 0.0556 ## Balanced Accuracy NA 1.0000 ## Class: mollusc.et.al ## Sensitivity 1.000 ## Specificity 1.000 ## Pos Pred Value 1.000 ## Neg Pred Value 1.000 ## Prevalence 0.111 ## Detection Rate 0.111 ## Detection Prevalence 0.111 ## Balanced Accuracy 1.000 4.5 Decision Boundaries Classifiers create decision boundaries to discriminate between classes. Different classifiers are able to create different shapes of decision boundaries (e.g., some are strictly linear) and thus some classifiers may perform better for certain datasets. This page visualizes the decision boundaries found by several popular classification methods. The following plot adds the decision boundary by evaluating the classifier at evenly spaced grid points. Note that low resolution (to make evaluation faster) will make the decision boundary look like it has small steps even if it is a (straight) line. library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor library(tidyverse) library(ggplot2) library(caret) decisionplot &lt;- function(model, x, cl = NULL, predict_type = &quot;class&quot;, resolution = 100) { if(!is.null(cl)) { x_data &lt;- x %&gt;% dplyr::select(-all_of(cl)) cl &lt;- x %&gt;% pull(cl) } else cl &lt;- 1 k &lt;- length(unique(cl)) # resubstitution accuracy prediction &lt;- predict(model, x_data, type = predict_type) if(is.list(prediction)) prediction &lt;- prediction$class if(is.numeric(prediction)) prediction &lt;- factor(prediction, labels = levels(cl)) else prediction &lt;- factor(prediction, levels = levels(cl)) cm &lt;- confusionMatrix(data = prediction, reference = cl) acc &lt;- cm$overall[&quot;Accuracy&quot;] # evaluate model on a grid r &lt;- sapply(x[, 1:2], range, na.rm = TRUE) xs &lt;- seq(r[1,1], r[2,1], length.out = resolution) ys &lt;- seq(r[1,2], r[2,2], length.out = resolution) g &lt;- cbind(rep(xs, each = resolution), rep(ys, time = resolution)) colnames(g) &lt;- colnames(r) g &lt;- as_tibble(g) ### guess how to get class labels from predict ### (unfortunately not very consistent between models) prediction &lt;- predict(model, g, type = predict_type) if(is.list(prediction)) prediction &lt;- prediction$class if(is.numeric(prediction)) prediction &lt;- factor(prediction, labels = levels(cl)) else prediction &lt;- factor(prediction, levels = levels(cl)) g &lt;- g %&gt;% add_column(prediction) ggplot(g, mapping = aes_string( x = colnames(g)[1], y = colnames(g)[2])) + geom_tile(mapping = aes(fill = prediction)) + geom_point(data = x, mapping = aes_string( x = colnames(x)[1], y = colnames(x)[2], shape = colnames(x)[3]), alpha = .5) + labs(subtitle = paste(&quot;Training accuracy:&quot;, round(acc, 2))) } 4.5.1 Iris Dataset For easier visualization, we use on two dimensions of the Iris dataset. set.seed(1000) data(iris) iris &lt;- as_tibble(iris) ### Three classes (MASS also has a select function) x &lt;- iris %&gt;% dplyr::select(Sepal.Length, Sepal.Width, Species) x ## # A tibble: 150 x 3 ## Sepal.Length Sepal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 setosa ## 2 4.9 3 setosa ## 3 4.7 3.2 setosa ## 4 4.6 3.1 setosa ## 5 5 3.6 setosa ## 6 5.4 3.9 setosa ## 7 4.6 3.4 setosa ## 8 5 3.4 setosa ## 9 4.4 2.9 setosa ## 10 4.9 3.1 setosa ## # … with 140 more rows ggplot(x, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point() Note: There is some overplotting and you could use geom_jitter() instead of geom_point(). 4.5.1.1 K-Nearest Neighbors Classifier library(caret) model &lt;- x %&gt;% knn3(Species ~ ., data = ., k = 1) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;kNN (1 neighbor)&quot;) model &lt;- x %&gt;% knn3(Species ~ ., data = ., k = 10) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;kNN (10 neighbor)&quot;) 4.5.1.2 Naive Bayes Classifier library(e1071) model &lt;- x %&gt;% naiveBayes(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;Naive Bayes&quot;) 4.5.1.3 Linear Discriminant Analysis library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select model &lt;- x %&gt;% lda(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;LDA&quot;) 4.5.1.4 Multinomial Logistic Regression (implemented in nnet) Multinomial logistic regression is an extension of logistic regression to problems with more than two classes. library(nnet) model &lt;- x %&gt;% multinom(Species ~., data = .) ## # weights: 12 (6 variable) ## initial value 164.791843 ## iter 10 value 62.715967 ## iter 20 value 59.808291 ## iter 30 value 55.445984 ## iter 40 value 55.375704 ## iter 50 value 55.346472 ## iter 60 value 55.301707 ## iter 70 value 55.253532 ## iter 80 value 55.243230 ## iter 90 value 55.230241 ## iter 100 value 55.212479 ## final value 55.212479 ## stopped after 100 iterations decisionplot(model, x, cl = &quot;Species&quot;) + labs(titel = &quot;Multinomial Logistic Regression&quot;) 4.5.1.5 Decision Trees library(&quot;rpart&quot;) model &lt;- x %&gt;% rpart(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;CART&quot;) model &lt;- x %&gt;% rpart(Species ~ ., data = ., control = rpart.control(cp = 0.001, minsplit = 1)) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;CART (overfitting)&quot;) library(C50) model &lt;- x %&gt;% C5.0(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;C5.0&quot;) library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin model &lt;- x %&gt;% randomForest(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;Random Forest&quot;) 4.5.1.6 SVM library(e1071) model &lt;- x %&gt;% svm(Species ~ ., data = ., kernel = &quot;linear&quot;) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;SVM (linear kernel)&quot;) model &lt;- x %&gt;% svm(Species ~ ., data = ., kernel = &quot;radial&quot;) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;SVM (radial kernel)&quot;) model &lt;- x %&gt;% svm(Species ~ ., data = ., kernel = &quot;polynomial&quot;) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;SVM (polynomial kernel)&quot;) model &lt;- x %&gt;% svm(Species ~ ., data = ., kernel = &quot;sigmoid&quot;) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;SVM (sigmoid kernel)&quot;) 4.5.1.7 Single Layer Feed-forward Neural Networks library(nnet) model &lt;-x %&gt;% nnet(Species ~ ., data = ., size = 1, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;NN (1 neuron)&quot;) model &lt;-x %&gt;% nnet(Species ~ ., data = ., size = 2, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;NN (2 neurons)&quot;) model &lt;-x %&gt;% nnet(Species ~ ., data = ., size = 4, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;NN (4 neurons)&quot;) model &lt;-x %&gt;% nnet(Species ~ ., data = ., size = 10, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;NN (10 neurons)&quot;) 4.5.1.8 Deep Learning with keras library(keras) define predict so it works with decision plot predict.keras.engine.training.Model &lt;- function(object, newdata, ...) predict_classes(object, as.matrix(newdata)) Choices are the activation function, number of layers, number of units per layer and the optimizer. A L2 regularizer is used for the dense layer weights to reduce overfitting. The output is a categorical class value, therefore the output layer uses the softmax activation function, the loss is categorical crossentropy, and the metric is accuracy. model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 10, activation = &#39;relu&#39;, input_shape = c(2), kernel_regularizer=regularizer_l2(l=0.01)) %&gt;% layer_dense(units = 4, activation = &#39;softmax&#39;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39;) history &lt;- model %&gt;% fit( as.matrix(x[,1:2]), x %&gt;% pull(3) %&gt;% as.integer %&gt;% to_categorical(), epochs = 100, batch_size = 10 ) history ## ## Final epoch (plot to see history): ## loss: 0.6385 ## accuracy: 0.7 decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;keras (relu activation)&quot;) model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 10, activation = &#39;tanh&#39;, input_shape = c(2), kernel_regularizer = regularizer_l2(l = 0.01)) %&gt;% layer_dense(units = 4, activation = &#39;softmax&#39;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39;) history &lt;- model %&gt;% fit( as.matrix(x[,1:2]), x %&gt;% pull(3) %&gt;% as.integer %&gt;% to_categorical(), epochs = 100, batch_size = 10 ) history ## ## Final epoch (plot to see history): ## loss: 0.5671 ## accuracy: 0.7733 decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;keras (tanh activation)&quot;) 4.5.2 Circle Dataset This set is not linearly separable! set.seed(1000) library(mlbench) x &lt;- mlbench.circle(500) ###x &lt;- mlbench.cassini(500) ###x &lt;- mlbench.spirals(500, sd = .1) ###x &lt;- mlbench.smiley(500) x &lt;- cbind(as.data.frame(x$x), factor(x$classes)) colnames(x) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;class&quot;) x &lt;- as_tibble(x) x ## # A tibble: 500 x 3 ## x y class ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.344 0.448 1 ## 2 0.518 0.915 2 ## 3 -0.772 -0.0913 1 ## 4 0.382 0.412 1 ## 5 0.0328 0.438 1 ## 6 -0.865 -0.354 2 ## 7 0.477 0.640 2 ## 8 0.167 -0.809 2 ## 9 -0.568 -0.281 1 ## 10 -0.488 0.638 2 ## # … with 490 more rows ggplot(x, aes(x = x, y = y, color = class)) + geom_point() 4.5.2.1 K-Nearest Neighbors Classifier library(caret) model &lt;- x %&gt;% knn3(class ~ ., data = ., k = 1) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;kNN (1 neighbor)&quot;) model &lt;- x %&gt;% knn3(class ~ ., data = ., k = 10) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;kNN (10 neighbor)&quot;) 4.5.2.2 Naive Bayes Classifier library(e1071) model &lt;- x %&gt;% naiveBayes(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;naive Bayes&quot;) 4.5.2.3 Linear Discriminant Analysis library(MASS) model &lt;- x %&gt;% lda(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;LDA&quot;) 4.5.2.4 Multinomial Logistic Regression (implemented in nnet) Multinomial logistic regression is an extension of logistic regression to problems with more than two classes. library(nnet) model &lt;- x %&gt;% multinom(class ~., data = .) ## # weights: 4 (3 variable) ## initial value 346.573590 ## final value 346.308371 ## converged decisionplot(model, x, cl = &quot;class&quot;) + labs(titel = &quot;Multinomial Logistic Regression&quot;) 4.5.2.5 Decision Trees library(&quot;rpart&quot;) model &lt;- x %&gt;% rpart(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;CART&quot;) model &lt;- x %&gt;% rpart(class ~ ., data = ., control = rpart.control(cp = 0.001, minsplit = 1)) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;CART (overfitting)&quot;) library(C50) model &lt;- x %&gt;% C5.0(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;C5.0&quot;) library(randomForest) model &lt;- x %&gt;% randomForest(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;Random Forest&quot;) 4.5.2.6 SVM library(e1071) model &lt;- x %&gt;% svm(class ~ ., data = ., kernel = &quot;linear&quot;) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;SVM (linear kernel)&quot;) model &lt;- x %&gt;% svm(class ~ ., data = ., kernel = &quot;radial&quot;) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;SVM (radial kernel)&quot;) model &lt;- x %&gt;% svm(class ~ ., data = ., kernel = &quot;polynomial&quot;) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;SVM (polynomial kernel)&quot;) model &lt;- x %&gt;% svm(class ~ ., data = ., kernel = &quot;sigmoid&quot;) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;SVM (sigmoid kernel)&quot;) 4.5.2.7 Single Layer Feed-forward Neural Networks library(nnet) model &lt;-x %&gt;% nnet(class ~ ., data = ., size = 1, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;NN (1 neuron)&quot;) model &lt;-x %&gt;% nnet(class ~ ., data = ., size = 2, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;NN (2 neurons)&quot;) model &lt;-x %&gt;% nnet(class ~ ., data = ., size = 4, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;NN (4 neurons)&quot;) model &lt;-x %&gt;% nnet(class ~ ., data = ., size = 10, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;NN (10 neurons)&quot;) 4.5.2.8 Deep Learning with keras library(keras) redefine predict so it works with decision plot predict.keras.engine.training.Model &lt;- function(object, newdata, ...) predict_classes(object, as.matrix(newdata)) Choices are the activation function, number of layers, number of units per layer and the optimizer. A L2 regularizer is used for the dense layer weights to reduce overfitting. The output is a categorical class value, therefore the output layer uses the softmax activation function, the loss is categorical crossentropy, and the metric is accuracy. model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 10, activation = &#39;relu&#39;, input_shape = c(2), kernel_regularizer=regularizer_l2(l = 0.0001)) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39;) history &lt;- model %&gt;% fit( as.matrix(x[,1:2]), x %&gt;% pull(3) %&gt;% as.integer %&gt;% to_categorical(), epochs = 100, batch_size = 10 ) history ## ## Final epoch (plot to see history): ## loss: 0.1885 ## accuracy: 0.964 decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;keras (relu activation)&quot;) model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 10, activation = &#39;tanh&#39;, input_shape = c(2), kernel_regularizer = regularizer_l2(l = 0.0001)) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39;) history &lt;- model %&gt;% fit( as.matrix(x[,1:2]), x %&gt;% pull(3) %&gt;% as.integer %&gt;% to_categorical(), epochs = 100, batch_size = 10 ) history ## ## Final epoch (plot to see history): ## loss: 0.4085 ## accuracy: 0.902 decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;keras (tanh activation)&quot;) 4.6 More Information Example using deep learning with keras. Package caret: http://topepo.github.io/caret/index.html Tidymodels (machine learning with tidyverse): https://www.tidymodels.org/ R taskview on machine learning: http://cran.r-project.org/web/views/MachineLearning.html References "],["association-analysis-basic-concepts-and-algorithms.html", "Chapter 5 Association Analysis: Basic Concepts and Algorithms 5.1 The arules Package 5.2 Transactions 5.3 Frequent Itemsets 5.4 Association Rules 5.5 Association Rule Visualization 5.6 Interactive Visualizations", " Chapter 5 Association Analysis: Basic Concepts and Algorithms Packages used for this chapter: arules (Hahsler et al. 2021), arulesViz (Hahsler 2021), mlbench (Leisch and Dimitriadou. 2021), tidyverse (Wickham 2021c) You can read the free sample chapter from the textbook (Tan, Steinbach, and Kumar 2005): Chapter 5. Association Analysis: Basic Concepts and Algorithms 5.1 The arules Package Association rule mining in R is implemented in the package arules. library(tidyverse) library(arules) library(arulesViz) For information about the arules package try: help(package=\"arules\") and vignette(\"arules\") (also available at CRAN) arules uses the S4 object system to implement classes and methods. Standard R objects use the S3 object system which do not use formal class definitions and are usually implemented as a list with a class attribute. arules and many other R packages use the S4 object system which is based on formal class definitions with member variables and methods (similar to object-oriented programming languages like Java and C++). Some important differences of using S4 objects compared to the usual S3 objects are: coercion (casting): as(from, \"class_name\") help for classes: class? class_name 5.2 Transactions 5.2.1 Create Transactions We will use the Zoo dataset from mlbench. data(Zoo, package = &quot;mlbench&quot;) head(Zoo) ## hair feathers eggs milk airborne aquatic ## aardvark TRUE FALSE FALSE TRUE FALSE FALSE ## antelope TRUE FALSE FALSE TRUE FALSE FALSE ## bass FALSE FALSE TRUE FALSE FALSE TRUE ## bear TRUE FALSE FALSE TRUE FALSE FALSE ## boar TRUE FALSE FALSE TRUE FALSE FALSE ## buffalo TRUE FALSE FALSE TRUE FALSE FALSE ## predator toothed backbone breathes venomous ## aardvark TRUE TRUE TRUE TRUE FALSE ## antelope FALSE TRUE TRUE TRUE FALSE ## bass TRUE TRUE TRUE FALSE FALSE ## bear TRUE TRUE TRUE TRUE FALSE ## boar TRUE TRUE TRUE TRUE FALSE ## buffalo FALSE TRUE TRUE TRUE FALSE ## fins legs tail domestic catsize type ## aardvark FALSE 4 FALSE FALSE TRUE mammal ## antelope FALSE 4 TRUE FALSE TRUE mammal ## bass TRUE 0 TRUE FALSE FALSE fish ## bear FALSE 4 FALSE FALSE TRUE mammal ## boar FALSE 4 TRUE FALSE TRUE mammal ## buffalo FALSE 4 TRUE FALSE TRUE mammal The data in the data.frame need to be converted into a set of transactions where each row represents a transaction and each column is translated into items. This is done using the contructor transactions(). For the Zoo data set this means that we consider animals as transactions and the different traits (features) will become items that each animal has. For example the animal antelope has the item hair in its transaction. trans &lt;- transactions(Zoo) ## Warning: Column(s) 13 not logical or factor. Applying ## default discretization (see &#39;? discretizeDF&#39;). The conversion gives a warning because only discrete features (factor and logical) can be directly translated into items. Continuous features need to be discretized first. What is column 13? summary(Zoo[13]) ## legs ## Min. :0.00 ## 1st Qu.:2.00 ## Median :4.00 ## Mean :2.84 ## 3rd Qu.:4.00 ## Max. :8.00 ggplot(Zoo, aes(legs)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. table(Zoo$legs) ## ## 0 2 4 5 6 8 ## 23 27 38 1 10 2 Possible solution: Make legs into has/does not have legs Zoo_has_legs &lt;- Zoo %&gt;% mutate(legs = legs &gt; 0) ggplot(Zoo_has_legs, aes(legs)) + geom_bar() table(Zoo_has_legs$legs) ## ## FALSE TRUE ## 23 78 Alternatives: use each unique value as an item: Zoo_unique_leg_values &lt;- Zoo %&gt;% mutate(legs = factor(legs)) head(Zoo_unique_leg_values$legs) ## [1] 4 4 0 4 4 4 ## Levels: 0 2 4 5 6 8 discretize (see ? discretize and discretization in the code for Chapter 2): Zoo_discretized_legs &lt;- Zoo %&gt;% mutate( legs = discretize(legs, breaks = 2, method=&quot;interval&quot;) ) table(Zoo_discretized_legs$legs) ## ## [0,4) [4,8] ## 50 51 Convert data into a set of transactions trans &lt;- transactions(Zoo_has_legs) trans ## transactions in sparse format with ## 101 transactions (rows) and ## 23 items (columns) 5.2.2 Inspect Transactions summary(trans) ## transactions as itemMatrix in sparse format with ## 101 rows (elements/itemsets/transactions) and ## 23 columns (items) and a density of 0.361 ## ## most frequent items: ## backbone breathes legs tail toothed (Other) ## 83 80 78 75 61 462 ## ## element (itemset/transaction) length distribution: ## sizes ## 3 4 5 6 7 8 9 10 11 12 ## 3 2 6 5 8 21 27 25 3 1 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.00 8.00 9.00 8.31 10.00 12.00 ## ## includes extended item information - examples: ## labels variables levels ## 1 hair hair TRUE ## 2 feathers feathers TRUE ## 3 eggs eggs TRUE ## ## includes extended transaction information - examples: ## transactionID ## 1 aardvark ## 2 antelope ## 3 bass Look at created items. They are still called column names since the transactions are actually stored as a large sparse logical matrix (see below). colnames(trans) ## [1] &quot;hair&quot; &quot;feathers&quot; ## [3] &quot;eggs&quot; &quot;milk&quot; ## [5] &quot;airborne&quot; &quot;aquatic&quot; ## [7] &quot;predator&quot; &quot;toothed&quot; ## [9] &quot;backbone&quot; &quot;breathes&quot; ## [11] &quot;venomous&quot; &quot;fins&quot; ## [13] &quot;legs&quot; &quot;tail&quot; ## [15] &quot;domestic&quot; &quot;catsize&quot; ## [17] &quot;type=mammal&quot; &quot;type=bird&quot; ## [19] &quot;type=reptile&quot; &quot;type=fish&quot; ## [21] &quot;type=amphibian&quot; &quot;type=insect&quot; ## [23] &quot;type=mollusc.et.al&quot; Compare with the original features (column names) from Zoo colnames(Zoo) ## [1] &quot;hair&quot; &quot;feathers&quot; &quot;eggs&quot; &quot;milk&quot; ## [5] &quot;airborne&quot; &quot;aquatic&quot; &quot;predator&quot; &quot;toothed&quot; ## [9] &quot;backbone&quot; &quot;breathes&quot; &quot;venomous&quot; &quot;fins&quot; ## [13] &quot;legs&quot; &quot;tail&quot; &quot;domestic&quot; &quot;catsize&quot; ## [17] &quot;type&quot; Look at a (first) few transactions as a matrix. 1 indicates the presence of an item. as(trans, &quot;matrix&quot;)[1:3,] ## hair feathers eggs milk airborne aquatic ## aardvark TRUE FALSE FALSE TRUE FALSE FALSE ## antelope TRUE FALSE FALSE TRUE FALSE FALSE ## bass FALSE FALSE TRUE FALSE FALSE TRUE ## predator toothed backbone breathes venomous ## aardvark TRUE TRUE TRUE TRUE FALSE ## antelope FALSE TRUE TRUE TRUE FALSE ## bass TRUE TRUE TRUE FALSE FALSE ## fins legs tail domestic catsize ## aardvark FALSE TRUE FALSE FALSE TRUE ## antelope FALSE TRUE TRUE FALSE TRUE ## bass TRUE FALSE TRUE FALSE FALSE ## type=mammal type=bird type=reptile type=fish ## aardvark TRUE FALSE FALSE FALSE ## antelope TRUE FALSE FALSE FALSE ## bass FALSE FALSE FALSE TRUE ## type=amphibian type=insect type=mollusc.et.al ## aardvark FALSE FALSE FALSE ## antelope FALSE FALSE FALSE ## bass FALSE FALSE FALSE Look at the transactions as sets of items inspect(trans[1:3]) ## items transactionID ## [1] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## catsize, ## type=mammal} aardvark ## [2] {hair, ## milk, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} antelope ## [3] {eggs, ## aquatic, ## predator, ## toothed, ## backbone, ## fins, ## tail, ## type=fish} bass Plot the binary matrix. Dark dots represent 1s. image(trans) Look at the relative frequency (=support) of items in the data set. Here we look at the 10 most frequent items. itemFrequencyPlot(trans,topN = 20) ggplot( tibble( Support = sort(itemFrequency(trans, type = &quot;absolute&quot;), decreasing = TRUE), Item = seq_len(ncol(trans)) ), aes(x = Item, y = Support)) + geom_line() Alternative encoding: Also create items for FALSE (use factor) sapply(Zoo_has_legs, class) ## hair feathers eggs milk airborne ## &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; ## aquatic predator toothed backbone breathes ## &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; ## venomous fins legs tail domestic ## &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; ## catsize type ## &quot;logical&quot; &quot;factor&quot; Zoo_factors &lt;- Zoo_has_legs %&gt;% mutate_if(is.logical, factor) sapply(Zoo_factors, class) ## hair feathers eggs milk airborne aquatic ## &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; ## predator toothed backbone breathes venomous fins ## &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; ## legs tail domestic catsize type ## &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; summary(Zoo_factors) ## hair feathers eggs milk ## FALSE:58 FALSE:81 FALSE:42 FALSE:60 ## TRUE :43 TRUE :20 TRUE :59 TRUE :41 ## ## ## ## ## ## airborne aquatic predator toothed ## FALSE:77 FALSE:65 FALSE:45 FALSE:40 ## TRUE :24 TRUE :36 TRUE :56 TRUE :61 ## ## ## ## ## ## backbone breathes venomous fins ## FALSE:18 FALSE:21 FALSE:93 FALSE:84 ## TRUE :83 TRUE :80 TRUE : 8 TRUE :17 ## ## ## ## ## ## legs tail domestic catsize ## FALSE:23 FALSE:26 FALSE:88 FALSE:57 ## TRUE :78 TRUE :75 TRUE :13 TRUE :44 ## ## ## ## ## ## type ## mammal :41 ## bird :20 ## reptile : 5 ## fish :13 ## amphibian : 4 ## insect : 8 ## mollusc.et.al:10 trans_factors &lt;- transactions(Zoo_factors) trans_factors ## transactions in sparse format with ## 101 transactions (rows) and ## 39 items (columns) itemFrequencyPlot(trans_factors, topN = 20) ## Select transactions that contain a certain item trans_insects &lt;- trans_factors[trans %in% &quot;type=insect&quot;] trans_insects ## transactions in sparse format with ## 8 transactions (rows) and ## 39 items (columns) inspect(trans_insects) ## items transactionID ## [1] {hair=FALSE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=FALSE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} flea ## [2] {hair=FALSE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} gnat ## [3] {hair=TRUE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=TRUE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=TRUE, ## catsize=FALSE, ## type=insect} honeybee ## [4] {hair=TRUE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} housefly ## [5] {hair=FALSE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=TRUE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} ladybird ## [6] {hair=TRUE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} moth ## [7] {hair=FALSE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=FALSE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} termite ## [8] {hair=TRUE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=TRUE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} wasp 5.2.3 Vertical Layout (Transaction ID Lists) The default layout for transactions is horizontal layout (i.e. each transaction is a row). The vertical layout represents transaction data as a list of transaction IDs for each item (= transaction ID lists). vertical &lt;- as(trans, &quot;tidLists&quot;) as(vertical, &quot;matrix&quot;)[1:10, 1:5] ## aardvark antelope bass bear boar ## hair TRUE TRUE FALSE TRUE TRUE ## feathers FALSE FALSE FALSE FALSE FALSE ## eggs FALSE FALSE TRUE FALSE FALSE ## milk TRUE TRUE FALSE TRUE TRUE ## airborne FALSE FALSE FALSE FALSE FALSE ## aquatic FALSE FALSE TRUE FALSE FALSE ## predator TRUE FALSE TRUE TRUE TRUE ## toothed TRUE TRUE TRUE TRUE TRUE ## backbone TRUE TRUE TRUE TRUE TRUE ## breathes TRUE TRUE FALSE TRUE TRUE 5.3 Frequent Itemsets 5.3.1 Mine Frequent Itemsets For this dataset we have already a huge number of possible itemsets 2^ncol(trans) ## [1] 8388608 Find frequent itemsets (target=“frequent”) with the default settings. its &lt;- apriori(trans, parameter=list(target = &quot;frequent&quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## NA 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.1 1 10 frequent itemsets TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 10 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [18 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans, parameter = list(target = ## &quot;frequent&quot;)): Mining stopped (maxlen reached). Only ## patterns up to a length of 10 returned! ## done [0.00s]. ## sorting transactions ... done [0.00s]. ## writing ... [1465 set(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. its ## set of 1465 itemsets Default minimum support is .1 (10%). Note: We use here a very small data set. For larger datasets the default minimum support might be to low and you may run out of memory. You probably want to start out with a higher minimum support like .5 (50%) and then work your way down. 5/nrow(trans) ## [1] 0.0495 In order to find itemsets that effect 5 animals I need to go down to a support of about 5%. its &lt;- apriori(trans, parameter=list(target = &quot;frequent&quot;, support = 0.05)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## NA 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.05 1 10 frequent itemsets TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 5 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [21 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans, parameter = list(target = ## &quot;frequent&quot;, support = 0.05)): Mining stopped (maxlen ## reached). Only patterns up to a length of 10 returned! ## done [0.00s]. ## sorting transactions ... done [0.00s]. ## writing ... [2537 set(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. its ## set of 2537 itemsets Sort by support its &lt;- sort(its, by = &quot;support&quot;) inspect(head(its, n = 10)) ## items support count ## [1] {backbone} 0.822 83 ## [2] {breathes} 0.792 80 ## [3] {legs} 0.772 78 ## [4] {tail} 0.743 75 ## [5] {backbone, tail} 0.733 74 ## [6] {breathes, legs} 0.723 73 ## [7] {backbone, breathes} 0.683 69 ## [8] {backbone, legs} 0.634 64 ## [9] {backbone, breathes, legs} 0.634 64 ## [10] {toothed} 0.604 61 Look at frequent itemsets with many items (set breaks manually since Automatically chosen breaks look bad) ggplot(tibble(`Itemset Size` = factor(size(its))), aes(`Itemset Size`)) + geom_bar() inspect(its[size(its) &gt; 8]) ## items support count ## [1] {hair, ## milk, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.2376 24 ## [2] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## catsize, ## type=mammal} 0.1584 16 ## [3] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## type=mammal} 0.1485 15 ## [4] {hair, ## milk, ## predator, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1386 14 ## [5] {hair, ## milk, ## predator, ## toothed, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [6] {hair, ## milk, ## predator, ## toothed, ## backbone, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [7] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [8] {milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [9] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize} 0.1287 13 ## [10] {hair, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [11] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [12] {hair, ## milk, ## toothed, ## backbone, ## breathes, ## legs, ## domestic, ## catsize, ## type=mammal} 0.0594 6 ## [13] {hair, ## milk, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## domestic, ## type=mammal} 0.0594 6 ## [14] {feathers, ## eggs, ## airborne, ## predator, ## backbone, ## breathes, ## legs, ## tail, ## type=bird} 0.0594 6 5.3.2 Concise Representation of Itemsets Find maximal frequent itemsets (no superset if frequent) its_max &lt;- its[is.maximal(its)] its_max ## set of 22 itemsets inspect(head(its_max, by = &quot;support&quot;)) ## items support count ## [1] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [2] {eggs, ## aquatic, ## predator, ## toothed, ## backbone, ## fins, ## tail, ## type=fish} 0.0891 9 ## [3] {aquatic, ## predator, ## toothed, ## backbone, ## breathes} 0.0792 8 ## [4] {aquatic, ## predator, ## toothed, ## backbone, ## fins, ## tail, ## catsize} 0.0693 7 ## [5] {eggs, ## venomous} 0.0594 6 ## [6] {predator, ## venomous} 0.0594 6 Find closed frequent itemsets (no superset if frequent) its_closed &lt;- its[is.closed(its)] its_closed ## set of 230 itemsets inspect(head(its_closed, by = &quot;support&quot;)) ## items support count ## [1] {backbone} 0.822 83 ## [2] {breathes} 0.792 80 ## [3] {legs} 0.772 78 ## [4] {tail} 0.743 75 ## [5] {backbone, tail} 0.733 74 ## [6] {breathes, legs} 0.723 73 counts &lt;- c( frequent=length(its), closed=length(its_closed), maximal=length(its_max) ) ggplot(as_tibble(counts, rownames = &quot;Itemsets&quot;), aes(Itemsets, counts)) + geom_bar(stat = &quot;identity&quot;) 5.4 Association Rules 5.4.1 Mine Association Rules We use the APRIORI algorithm (see ? apriori) rules &lt;- apriori(trans, parameter = list(support = 0.05, confidence = 0.9)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.9 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.05 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 5 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [21 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans, parameter = list(support ## = 0.05, confidence = 0.9)): Mining stopped (maxlen ## reached). Only patterns up to a length of 10 returned! ## done [0.00s]. ## writing ... [7174 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. length(rules) ## [1] 7174 inspect(head(rules)) ## lhs rhs support ## [1] {type=insect} =&gt; {eggs} 0.0792 ## [2] {type=insect} =&gt; {legs} 0.0792 ## [3] {type=insect} =&gt; {breathes} 0.0792 ## [4] {type=mollusc.et.al} =&gt; {eggs} 0.0891 ## [5] {type=fish} =&gt; {fins} 0.1287 ## [6] {type=fish} =&gt; {aquatic} 0.1287 ## confidence coverage lift count ## [1] 1.0 0.0792 1.71 8 ## [2] 1.0 0.0792 1.29 8 ## [3] 1.0 0.0792 1.26 8 ## [4] 0.9 0.0990 1.54 9 ## [5] 1.0 0.1287 5.94 13 ## [6] 1.0 0.1287 2.81 13 quality(head(rules)) ## support confidence coverage lift count ## 1 0.0792 1.0 0.0792 1.71 8 ## 2 0.0792 1.0 0.0792 1.29 8 ## 3 0.0792 1.0 0.0792 1.26 8 ## 4 0.0891 0.9 0.0990 1.54 9 ## 5 0.1287 1.0 0.1287 5.94 13 ## 6 0.1287 1.0 0.1287 2.81 13 Look at rules with highest lift rules &lt;- sort(rules, by = &quot;lift&quot;) inspect(head(rules, n = 10)) ## lhs rhs support confidence coverage lift count ## [1] {eggs, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [2] {eggs, ## aquatic, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [3] {eggs, ## predator, ## fins} =&gt; {type=fish} 0.0891 1 0.0891 7.77 9 ## [4] {eggs, ## toothed, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [5] {eggs, ## fins, ## tail} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [6] {eggs, ## backbone, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [7] {eggs, ## aquatic, ## predator, ## fins} =&gt; {type=fish} 0.0891 1 0.0891 7.77 9 ## [8] {eggs, ## aquatic, ## toothed, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [9] {eggs, ## aquatic, ## fins, ## tail} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [10] {eggs, ## aquatic, ## backbone, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 Create rules using the alternative encoding (with “FALSE” item) r &lt;- apriori(trans_factors) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.8 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.1 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 10 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[39 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [34 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans_factors): Mining stopped ## (maxlen reached). Only patterns up to a length of 10 ## returned! ## done [0.09s]. ## writing ... [1517191 rule(s)] done [0.25s]. ## creating S4 object ... done [1.07s]. r ## set of 1517191 rules print(object.size(r), unit = &quot;Mb&quot;) ## 110.2 Mb inspect(r[1:10]) ## lhs rhs support ## [1] {} =&gt; {feathers=FALSE} 0.802 ## [2] {} =&gt; {backbone=TRUE} 0.822 ## [3] {} =&gt; {fins=FALSE} 0.832 ## [4] {} =&gt; {domestic=FALSE} 0.871 ## [5] {} =&gt; {venomous=FALSE} 0.921 ## [6] {domestic=TRUE} =&gt; {predator=FALSE} 0.109 ## [7] {domestic=TRUE} =&gt; {aquatic=FALSE} 0.119 ## [8] {domestic=TRUE} =&gt; {legs=TRUE} 0.119 ## [9] {domestic=TRUE} =&gt; {breathes=TRUE} 0.119 ## [10] {domestic=TRUE} =&gt; {backbone=TRUE} 0.119 ## confidence coverage lift count ## [1] 0.802 1.000 1.00 81 ## [2] 0.822 1.000 1.00 83 ## [3] 0.832 1.000 1.00 84 ## [4] 0.871 1.000 1.00 88 ## [5] 0.921 1.000 1.00 93 ## [6] 0.846 0.129 1.90 11 ## [7] 0.923 0.129 1.43 12 ## [8] 0.923 0.129 1.20 12 ## [9] 0.923 0.129 1.17 12 ## [10] 0.923 0.129 1.12 12 inspect(head(r, n = 10, by = &quot;lift&quot;)) ## lhs rhs support confidence coverage lift count ## [1] {breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [2] {eggs=TRUE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [3] {milk=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [4] {breathes=FALSE, ## fins=TRUE, ## legs=FALSE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [5] {aquatic=TRUE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [6] {hair=FALSE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [7] {eggs=TRUE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [8] {milk=FALSE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [9] {toothed=TRUE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [10] {breathes=FALSE, ## fins=TRUE, ## tail=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 5.4.2 Calculate Additional Interest Measures interestMeasure(rules[1:10], measure = c(&quot;phi&quot;, &quot;gini&quot;), trans = trans) ## phi gini ## 1 1.000 0.224 ## 2 1.000 0.224 ## 3 0.814 0.149 ## 4 1.000 0.224 ## 5 1.000 0.224 ## 6 1.000 0.224 ## 7 0.814 0.149 ## 8 1.000 0.224 ## 9 1.000 0.224 ## 10 1.000 0.224 Add measures to the rules quality(rules) &lt;- cbind(quality(rules), interestMeasure(rules, measure = c(&quot;phi&quot;, &quot;gini&quot;), trans = trans)) Find rules which score high for Phi correlation inspect(head(rules, by = &quot;phi&quot;)) ## lhs rhs support confidence coverage lift count phi gini ## [1] {eggs, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [2] {eggs, ## aquatic, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [3] {eggs, ## toothed, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [4] {eggs, ## fins, ## tail} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [5] {eggs, ## backbone, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [6] {eggs, ## aquatic, ## toothed, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 5.4.3 Mine Using Templates Sometimes it is beneficial to specify what items should be where in the rule. For apriori we can use the parameter appearance to specify this (see ? APappearance). In the following we restrict rules to an animal type in the RHS and any item in the LHS. type &lt;- grep(&quot;type=&quot;, itemLabels(trans), value = TRUE) type ## [1] &quot;type=mammal&quot; &quot;type=bird&quot; ## [3] &quot;type=reptile&quot; &quot;type=fish&quot; ## [5] &quot;type=amphibian&quot; &quot;type=insect&quot; ## [7] &quot;type=mollusc.et.al&quot; rules_type &lt;- apriori(trans, appearance= list(rhs = type)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.8 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.1 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 10 ## ## set item appearances ...[7 item(s)] done [0.00s]. ## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [18 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans, appearance = list(rhs = ## type)): Mining stopped (maxlen reached). Only patterns ## up to a length of 10 returned! ## done [0.00s]. ## writing ... [571 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. inspect(head(sort(rules_type, by = &quot;lift&quot;))) ## lhs rhs support confidence coverage lift count ## [1] {eggs, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [2] {eggs, ## aquatic, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [3] {eggs, ## toothed, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [4] {eggs, ## fins, ## tail} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [5] {eggs, ## backbone, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [6] {eggs, ## aquatic, ## toothed, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 Saving rules as a CSV-file to be opened with Excel or other tools. write(rules, file = \"rules.csv\", quote = TRUE) 5.5 Association Rule Visualization library(arulesViz) Default scatterplot plot(rules) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. Note that some jitter (randomly move points) was added to show how many rules have the same confidence and support value. Without jitter: plot(rules, control = list(jitter = 0)) plot(rules, shading = &quot;order&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. ##plot(rules, interactive = TRUE) Grouped plot plot(rules, method = &quot;grouped&quot;) ##plot(rules, method = &quot;grouped&quot;, engine = &quot;interactive&quot;) As a graph plot(rules, method = &quot;graph&quot;) ## Warning: Too many rules supplied. Only plotting the ## best 100 rules using lift (change control parameter max ## if needed) plot(head(rules, by = &quot;phi&quot;, n = 100), method = &quot;graph&quot;) 5.6 Interactive Visualizations We will use the association rules mined from the Iris dataset for the following examples. data(iris) summary(iris) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.00 Min. :1.00 ## 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 ## Median :5.80 Median :3.00 Median :4.35 ## Mean :5.84 Mean :3.06 Mean :3.76 ## 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 ## Max. :7.90 Max. :4.40 Max. :6.90 ## Petal.Width Species ## Min. :0.1 setosa :50 ## 1st Qu.:0.3 versicolor:50 ## Median :1.3 virginica :50 ## Mean :1.2 ## 3rd Qu.:1.8 ## Max. :2.5 Convert the data to transactions. Note that the features are numeric and need to be discretized. The conversion automatically applies frequency-based discretization with 3 classes to each numeric feature (with a warning). iris_trans &lt;- transactions(iris) ## Warning: Column(s) 1, 2, 3, 4 not logical or factor. ## Applying default discretization (see &#39;? discretizeDF&#39;). inspect(head(iris_trans)) ## items transactionID ## [1] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[3.2,4.4], ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 1 ## [2] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[2.9,3.2), ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 2 ## [3] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[3.2,4.4], ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 3 ## [4] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[2.9,3.2), ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 4 ## [5] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[3.2,4.4], ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 5 ## [6] {Sepal.Length=[5.4,6.3), ## Sepal.Width=[3.2,4.4], ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 6 Next, we mine association rules. rules &lt;- apriori(iris_trans, parameter = list(support = 0.1, confidence = 0.8)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.8 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.1 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 15 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[15 item(s), 150 transaction(s)] done [0.00s]. ## sorting and recoding items ... [15 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 done [0.00s]. ## writing ... [144 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. rules ## set of 144 rules 5.6.1 Interactive Inspect With Sorting, Filtering and Paging inspectDT(rules) 5.6.2 Scatter Plot Plot rules as a scatter plot using an interactive html widget. To avoid overplotting, jitter is added automatically. Set jitter = 0 to disable jitter. Hovering over rules shows rule information. Note: plotly/javascript does not do well with too many points, so plot selects the top 1000 rules with a warning if more rules are supplied. plot(rules, engine = &quot;html&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. 5.6.3 Matrix Visualization Plot rules as a matrix using an interactive html widget. plot(rules, method = &quot;matrix&quot;, engine = &quot;html&quot;) 5.6.4 Visualization as Graph Plot rules as a graph using an interactive html widget. Note: the used javascript library does not do well with too many graph nodes, so plot selects the top 100 rules only (with a warning). plot(rules, method = &quot;graph&quot;, engine = &quot;html&quot;) ## Warning: Too many rules supplied. Only plotting the ## best 100 rules using lift (change control parameter max ## if needed) 5.6.5 Interactive Rule Explorer You can specify a rule set or a dataset. To explore rules that can be mined from iris, use: ruleExplorer(iris) The rule explorer creates an interactive Shiny application that can be used locally or deployed on a server for sharing. A deployed version of the ruleExplorer is available here (using shinyapps.io). References "],["association-analysis-advanced-concepts.html", "Chapter 6 Association Analysis: Advanced Concepts", " Chapter 6 Association Analysis: Advanced Concepts No code is available for this Chapter. Some topics were already covered in the code for the previous chapter. "],["clustering-analysis.html", "Chapter 7 Clustering Analysis 7.1 Data Preparation 7.2 Clustering methods 7.3 Internal Cluster Validation 7.4 External Cluster Validation 7.5 Advanced Data Preparation for Clustering", " Chapter 7 Clustering Analysis Packages used for this chapter: cluster (Maechler et al. 2021), dbscan (Hahsler and Piekenbrock 2021), e1071 (Meyer et al. 2021), factoextra (Kassambara and Mundt 2020), fpc (Hennig 2020), GGally (Schloerke et al. 2021), kernlab (Karatzoglou, Smola, and Hornik 2019), mclust (Fraley, Raftery, and Scrucca 2020), mlbench (Leisch and Dimitriadou. 2021), scatterpie (Yu 2021), seriation (Hahsler, Buchta, and Hornik 2021), tidyverse (Wickham 2021c) You can read the free sample chapter from the textbook (Tan, Steinbach, and Kumar 2005): Chapter 7. Cluster Analysis: Basic Concepts and Algorithms 7.1 Data Preparation library(tidyverse) We will use here a small and very clean dataset called Ruspini which is included in the R package cluster. data(ruspini, package = &quot;cluster&quot;) The Ruspini data set, consisting of 75 points in four groups that is popular for illustrating clustering techniques. It is a very simple data set with well separated clusters. The original dataset has the points ordered by group. We can shuffle the data (rows) using sample_frac which samples by default 100%. ruspini &lt;- as_tibble(ruspini) %&gt;% sample_frac() ruspini ## # A tibble: 75 x 2 ## x y ## &lt;int&gt; &lt;int&gt; ## 1 38 143 ## 2 30 52 ## 3 22 74 ## 4 70 4 ## 5 77 12 ## 6 18 61 ## 7 85 115 ## 8 34 141 ## 9 53 144 ## 10 35 153 ## # … with 65 more rows 7.1.1 Data cleaning ggplot(ruspini, aes(x = x, y = y)) + geom_point() summary(ruspini) ## x y ## Min. : 4.0 Min. : 4.0 ## 1st Qu.: 31.5 1st Qu.: 56.5 ## Median : 52.0 Median : 96.0 ## Mean : 54.9 Mean : 92.0 ## 3rd Qu.: 76.5 3rd Qu.:141.5 ## Max. :117.0 Max. :156.0 For most clustering algorithms it is necessary to handle missing values and outliers (e.g., remove the observations). For details see Section “Outlier removal” below. This data set has not missing values or strong outlier and looks like it has some very clear groups. 7.1.2 Scale data Clustering algorithms use distances and the variables with the largest number range will dominate distance calculation. The summary above shows that this is not an issue for the Ruspini dataset with both, x and y, being roughly between 0 and 150. Most data analysts will still scale each column in the data to zero mean and unit standard deviation (z-scores). Note: The standard scale() function scales a whole data matrix so we implement a function for a single vector and apply it to all numeric columns. ## I use this till tidyverse implements a scale function scale_numeric &lt;- function(x) x %&gt;% mutate_if(is.numeric, function(y) as.vector(scale(y))) ruspini_scaled &lt;- ruspini %&gt;% scale_numeric() summary(ruspini_scaled) ## x y ## Min. :-1.668 Min. :-1.807 ## 1st Qu.:-0.766 1st Qu.:-0.729 ## Median :-0.094 Median : 0.082 ## Mean : 0.000 Mean : 0.000 ## 3rd Qu.: 0.709 3rd Qu.: 1.016 ## Max. : 2.037 Max. : 1.314 After scaling, most z-scores will fall in the range \\([-3,3]\\) (z-scores are measured in standard deviations from the mean), where \\(0\\) means average. 7.2 Clustering methods 7.2.1 k-means Clustering k-means implicitly assumes Euclidean distances. We use \\(k = 4\\) clusters and run the algorithm 10 times with random initialized centroids. The best result is returned. km &lt;- kmeans(ruspini_scaled, centers = 4, nstart = 10) km ## K-means clustering with 4 clusters of sizes 17, 23, 15, 20 ## ## Cluster means: ## x y ## 1 1.419 0.469 ## 2 -0.360 1.109 ## 3 0.461 -1.491 ## 4 -1.139 -0.556 ## ## Clustering vector: ## [1] 2 4 4 3 3 4 1 2 2 2 3 2 3 4 1 3 4 4 1 4 2 3 1 2 1 ## [26] 2 3 1 3 2 1 4 3 4 4 1 1 2 1 4 2 1 2 2 4 3 3 2 2 4 ## [51] 1 2 4 2 3 4 2 3 2 4 1 4 4 1 1 2 4 3 1 2 2 3 4 1 2 ## ## Within cluster sum of squares by cluster: ## [1] 3.64 2.66 1.08 2.71 ## (between_SS / total_SS = 93.2 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; ## [4] &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; ## [7] &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; km is an R object implemented as a list. The clustering vector contains the cluster assignment for each data row and can be accessed using km$cluster. I add the cluster assignment as a column to the scaled dataset (I make it a factor since it represents a nominal label). ruspini_clustered &lt;- ruspini_scaled %&gt;% add_column(cluster = factor(km$cluster)) ruspini_clustered ## # A tibble: 75 x 3 ## x y cluster ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.553 1.05 2 ## 2 -0.816 -0.822 4 ## 3 -1.08 -0.370 4 ## 4 0.496 -1.81 3 ## 5 0.725 -1.64 3 ## 6 -1.21 -0.637 4 ## 7 0.987 0.472 1 ## 8 -0.685 1.01 2 ## 9 -0.0616 1.07 2 ## 10 -0.652 1.25 2 ## # … with 65 more rows ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() Add the centroids to the plot. centroids &lt;- as_tibble(km$centers, rownames = &quot;cluster&quot;) centroids ## # A tibble: 4 x 3 ## cluster x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.42 0.469 ## 2 2 -0.360 1.11 ## 3 3 0.461 -1.49 ## 4 4 -1.14 -0.556 ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10) Use the factoextra package for visualization library(factoextra) fviz_cluster(km, data = ruspini_scaled, centroids = TRUE, repel = TRUE, ellipse.type = &quot;norm&quot;) ## Warning: ggrepel: 10 unlabeled data points (too many ## overlaps). Consider increasing max.overlaps 7.2.1.1 Inspect clusters We inspect the clusters created by the 4-cluster k-means solution. The following code can be adapted to be used for other clustering methods. 7.2.1.1.1 Cluster Profiles Inspect the centroids with horizontal bar charts organized by cluster. To group the plots by cluster, we have to change the data format to the “long”-format using a pivot operation. I use colors to match the clusters in the scatter plots. ggplot(pivot_longer(centroids, cols = c(x, y), names_to = &quot;feature&quot;), aes(x = value, y = feature, fill = cluster)) + geom_bar(stat = &quot;identity&quot;) + facet_grid(rows = vars(cluster)) 7.2.1.1.2 Extract a single cluster You need is to filter the rows corresponding to the cluster index. The next example calculates summary statistics and then plots all data points of cluster 1. cluster1 &lt;- ruspini_clustered %&gt;% filter(cluster == 1) cluster1 ## # A tibble: 17 x 3 ## x y cluster ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.987 0.472 1 ## 2 1.74 0.492 1 ## 3 1.38 0.615 1 ## 4 0.758 0.0405 1 ## 5 1.97 0.513 1 ## 6 1.45 0.554 1 ## 7 1.51 0.472 1 ## 8 0.987 0.0816 1 ## 9 0.627 0.0816 1 ## 10 1.74 0.390 1 ## 11 2.04 0.472 1 ## 12 1.45 0.739 1 ## 13 1.84 0.698 1 ## 14 1.81 0.390 1 ## 15 1.02 0.821 1 ## 16 1.41 0.657 1 ## 17 1.41 0.492 1 summary(cluster1) ## x y cluster ## Min. :0.627 Min. :0.041 1:17 ## 1st Qu.:1.020 1st Qu.:0.390 2: 0 ## Median :1.446 Median :0.492 3: 0 ## Mean :1.419 Mean :0.469 4: 0 ## 3rd Qu.:1.741 3rd Qu.:0.615 ## Max. :2.037 Max. :0.821 ggplot(cluster1, aes(x = x, y = y)) + geom_point() + coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) What happens if we try to cluster with 8 centers? fviz_cluster(kmeans(ruspini_scaled, centers = 8), data = ruspini_scaled, centroids = TRUE, geom = &quot;point&quot;, ellipse.type = &quot;norm&quot;) ## Too few points to calculate an ellipse 7.2.2 Hierarchical Clustering Hierarchical clustering starts with a distance matrix. dist() defaults to method=“Euclidean.” Note: Distance matrices become very large quickly (size and time complexity is \\(O(n^2)\\) where \\(n\\) is the number if data points). It is only possible to calculate and store the matrix for small data sets (maybe a few hundred thousand data points) in main memory. If your data is too large then you can use sampling. d &lt;- dist(ruspini_scaled) hclust() implements agglomerative hierarchical clustering. We cluster using complete link. hc &lt;- hclust(d, method = &quot;complete&quot;) Hierarchical clustering does not return cluster assignments but a dendrogram. The standard plot function plots the dendrogram. plot(hc) Use factoextra (ggplot version). We can specify the number of clusters to visualize how the dendrogram will be cut into clusters. fviz_dend(hc, k = 4) ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. ## Please use `guides(&lt;scale&gt; = &quot;none&quot;)` instead. More plotting options for dendrograms, including plotting parts of large dendrograms can be found here. Extract cluster assignments by cutting the dendrogram into four parts and add the cluster id to the data. clusters &lt;- cutree(hc, k = 4) cluster_complete &lt;- ruspini_scaled %&gt;% add_column(cluster = factor(clusters)) cluster_complete ## # A tibble: 75 x 3 ## x y cluster ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.553 1.05 1 ## 2 -0.816 -0.822 2 ## 3 -1.08 -0.370 2 ## 4 0.496 -1.81 3 ## 5 0.725 -1.64 3 ## 6 -1.21 -0.637 2 ## 7 0.987 0.472 4 ## 8 -0.685 1.01 1 ## 9 -0.0616 1.07 1 ## 10 -0.652 1.25 1 ## # … with 65 more rows ggplot(cluster_complete, aes(x, y, color = cluster)) + geom_point() Try 8 clusters (Note: fviz_cluster needs a list with data and the cluster labels for hclust) fviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc, k = 8)), geom = &quot;point&quot;) Clustering with single link hc_single &lt;- hclust(d, method = &quot;single&quot;) fviz_dend(hc_single, k = 4) ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. ## Please use `guides(&lt;scale&gt; = &quot;none&quot;)` instead. fviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc_single, k = 4)), geom = &quot;point&quot;) 7.2.3 Density-based clustering with DBSCAN library(dbscan) DBSCAN stands for “Density-Based Spatial Clustering of Applications with Noise.” It groups together points that are closely packed together and treats points in low-density regions as outliers. Parameters: minPts defines how many points in the epsilon neighborhood are needed to make a point a core point. It is often chosen as a smoothing parameter. I use here minPts = 4. To decide on epsilon, the knee in the kNN distance plot is often used. Note that minPts contains the point itself, while the k-nearest neighbor does not. We therefore have to use k = minPts - 1! The knee is around eps = .32. kNNdistplot(ruspini_scaled, k = 3) abline(h = .32, col = &quot;red&quot;) run dbscan db &lt;- dbscan(ruspini_scaled, eps = .32, minPts = 4) db ## DBSCAN clustering for 75 objects. ## Parameters: eps = 0.32, minPts = 4 ## The clustering contains 4 cluster(s) and 5 noise points. ## ## 0 1 2 3 4 ## 5 23 20 15 12 ## ## Available fields: cluster, eps, minPts str(db) ## List of 3 ## $ cluster: int [1:75] 1 2 2 3 3 2 0 1 1 1 ... ## $ eps : num 0.32 ## $ minPts : num 4 ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;dbscan_fast&quot; &quot;dbscan&quot; ggplot(ruspini_scaled %&gt;% add_column(cluster = factor(db$cluster)), aes(x, y, color = cluster)) + geom_point() Note: Cluster 0 represents outliers). fviz_cluster(db, ruspini_scaled, geom = &quot;point&quot;) Play with eps (neighborhood size) and MinPts (minimum of points needed for core cluster) 7.2.4 Partitioning Around Medoids (PAM) PAM tries to solve the \\(k\\)-medoids problem. The problem is similar to \\(k\\)-means, but uses medoids instead of centroids to represent clusters. Like hierarchical clustering, it typically works with precomputed distance matrix. An advantage is that you can use any distance metric not just Euclidean distances. Note: The medoid is the most central data point in the middle of the cluster. library(cluster) ## ## Attaching package: &#39;cluster&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## ruspini d &lt;- dist(ruspini_scaled) str(d) ## &#39;dist&#39; num [1:2775] 1.89 1.51 3.04 2.98 1.81 ... ## - attr(*, &quot;Size&quot;)= int 75 ## - attr(*, &quot;Diag&quot;)= logi FALSE ## - attr(*, &quot;Upper&quot;)= logi FALSE ## - attr(*, &quot;method&quot;)= chr &quot;Euclidean&quot; ## - attr(*, &quot;call&quot;)= language dist(x = ruspini_scaled) p &lt;- pam(d, k = 4) p ## Medoids: ## ID ## [1,] 66 66 ## [2,] 56 56 ## [3,] 33 33 ## [4,] 28 28 ## Clustering vector: ## [1] 1 2 2 3 3 2 4 1 1 1 3 1 3 2 4 3 2 2 4 2 1 3 4 1 4 ## [26] 1 3 4 3 1 4 2 3 2 2 4 4 1 4 2 1 4 1 1 2 3 3 1 1 2 ## [51] 4 1 2 1 3 2 1 3 1 2 4 2 2 4 4 1 2 3 4 1 1 3 2 4 1 ## Objective function: ## build swap ## 0.442 0.319 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; ## [5] &quot;isolation&quot; &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; ## [9] &quot;call&quot; ruspini_clustered &lt;- ruspini_scaled %&gt;% add_column(cluster = factor(p$cluster)) medoids &lt;- as_tibble(ruspini_scaled[p$medoids, ], rownames = &quot;cluster&quot;) medoids ## # A tibble: 4 x 3 ## cluster x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.357 1.17 ## 2 2 -1.18 -0.555 ## 3 3 0.463 -1.46 ## 4 4 1.45 0.554 ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = medoids, aes(x = x, y = y, color = cluster), shape = 3, size = 10) ## __Note:__ `fviz_cluster` needs the original data. fviz_cluster(c(p, list(data = ruspini_scaled)), geom = &quot;point&quot;, ellipse.type = &quot;norm&quot;) 7.2.5 Gaussian Mixture Models library(mclust) ## Package &#39;mclust&#39; version 5.4.7 ## Type &#39;citation(&quot;mclust&quot;)&#39; for citing this R package in publications. ## ## Attaching package: &#39;mclust&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map Gaussian mixture models assume that the data set is the result of drawing data from a set of Gaussian distributions where each distribution represents a cluster. Estimation algorithms try to identify the location parameters of the distributions and thus can be used to find clusters. Mclust() uses Bayesian Information Criterion (BIC) to find the number of clusters (model selection). BIC uses the likelihood and a penalty term to guard against overfitting. m &lt;- Mclust(ruspini_scaled) summary(m) ## ---------------------------------------------------- ## Gaussian finite mixture model fitted by EM algorithm ## ---------------------------------------------------- ## ## Mclust EEI (diagonal, equal volume and shape) model ## with 5 components: ## ## log-likelihood n df BIC ICL ## -91.3 75 16 -252 -252 ## ## Clustering table: ## 1 2 3 4 5 ## 23 20 15 3 14 plot(m, what = &quot;classification&quot;) Rerun with a fixed number of 4 clusters m &lt;- Mclust(ruspini_scaled, G=4) summary(m) ## ---------------------------------------------------- ## Gaussian finite mixture model fitted by EM algorithm ## ---------------------------------------------------- ## ## Mclust EEI (diagonal, equal volume and shape) model ## with 4 components: ## ## log-likelihood n df BIC ICL ## -102 75 13 -259 -259 ## ## Clustering table: ## 1 2 3 4 ## 23 20 15 17 plot(m, what = &quot;classification&quot;) 7.2.6 Spectral clustering Spectral clustering works by embedding the data points of the partitioning problem into the subspace of the k largest eigenvectors of a normalized affinity/kernel matrix. Then uses a simple clustering method like k-means. library(&quot;kernlab&quot;) ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## alpha ## The following object is masked from &#39;package:arules&#39;: ## ## size ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha cluster_spec &lt;- specc(as.matrix(ruspini_scaled), centers = 4) cluster_spec ## Spectral Clustering object of class &quot;specc&quot; ## ## Cluster memberships: ## ## 1 3 3 2 2 3 4 1 1 1 2 1 2 3 4 2 3 3 4 3 1 2 4 1 4 1 2 4 2 1 4 3 2 3 3 4 4 1 4 3 1 4 1 1 3 2 2 1 1 3 4 1 3 1 2 3 1 2 1 3 4 3 3 4 4 1 3 2 4 1 1 2 3 4 1 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 41.7670067458421 ## ## Centers: ## [,1] [,2] ## [1,] -0.360 1.109 ## [2,] 0.461 -1.491 ## [3,] -1.139 -0.556 ## [4,] 1.419 0.469 ## ## Cluster size: ## [1] 23 15 20 17 ## ## Within-cluster sum of squares: ## [1] 53.27 53.27 8.81 18.84 ggplot(ruspini_scaled %&gt;% add_column(cluster = factor(cluster_spec)), aes(x, y, color = cluster)) + geom_point() 7.2.7 Fuzzy C-Means Clustering The fuzzy clustering version of the k-means clustering problem. Each data point has a degree of membership to for each cluster. library(&quot;e1071&quot;) cluster_cmeans &lt;- cmeans(as.matrix(ruspini_scaled), centers = 4) cluster_cmeans ## Fuzzy c-means clustering with 4 clusters ## ## Cluster centers: ## x y ## 1 -1.137 -0.555 ## 2 0.455 -1.476 ## 3 1.505 0.516 ## 4 -0.376 1.114 ## ## Memberships: ## 1 2 3 4 ## [1,] 0.012065 0.004750 7.76e-03 9.75e-01 ## [2,] 0.866509 0.074035 2.11e-02 3.84e-02 ## [3,] 0.971282 0.010239 4.91e-03 1.36e-02 ## [4,] 0.024935 0.947252 1.65e-02 1.14e-02 ## [5,] 0.020593 0.950361 1.82e-02 1.09e-02 ## [6,] 0.992095 0.003402 1.36e-03 3.14e-03 ## [7,] 0.039260 0.053619 8.11e-01 9.62e-02 ## [8,] 0.037605 0.013313 1.97e-02 9.29e-01 ## [9,] 0.024784 0.013940 3.40e-02 9.27e-01 ## [10,] 0.025639 0.010355 1.73e-02 9.47e-01 ## [11,] 0.008241 0.983990 4.42e-03 3.35e-03 ## [12,] 0.001560 0.000705 1.32e-03 9.96e-01 ## [13,] 0.003861 0.992177 2.30e-03 1.66e-03 ## [14,] 0.768380 0.097124 4.14e-02 9.31e-02 ## [15,] 0.005870 0.009963 9.73e-01 1.13e-02 ## [16,] 0.024150 0.952363 1.34e-02 1.01e-02 ## [17,] 0.828839 0.045276 2.77e-02 9.82e-02 ## [18,] 0.904502 0.033979 1.64e-02 4.51e-02 ## [19,] 0.003221 0.004747 9.85e-01 7.44e-03 ## [20,] 0.934346 0.027260 1.15e-02 2.69e-02 ## [21,] 0.003385 0.001497 2.77e-03 9.92e-01 ## [22,] 0.020387 0.949234 1.93e-02 1.11e-02 ## [23,] 0.107506 0.177387 5.41e-01 1.74e-01 ## [24,] 0.011470 0.004817 8.41e-03 9.75e-01 ## [25,] 0.018433 0.031839 9.16e-01 3.39e-02 ## [26,] 0.004627 0.002182 4.27e-03 9.89e-01 ## [27,] 0.003167 0.993633 1.85e-03 1.35e-03 ## [28,] 0.000609 0.000943 9.97e-01 1.32e-03 ## [29,] 0.028738 0.947019 1.34e-02 1.08e-02 ## [30,] 0.071388 0.050971 1.76e-01 7.02e-01 ## [31,] 0.000250 0.000411 9.99e-01 5.07e-04 ## [32,] 0.939767 0.029086 1.01e-02 2.10e-02 ## [33,] 0.000110 0.999766 7.43e-05 5.05e-05 ## [34,] 0.860429 0.059383 2.50e-02 5.52e-02 ## [35,] 0.895316 0.033633 1.80e-02 5.31e-02 ## [36,] 0.065465 0.118857 7.06e-01 1.10e-01 ## [37,] 0.128305 0.183755 4.70e-01 2.18e-01 ## [38,] 0.011252 0.005928 1.35e-02 9.69e-01 ## [39,] 0.007575 0.013540 9.65e-01 1.39e-02 ## [40,] 0.890088 0.054964 1.83e-02 3.66e-02 ## [41,] 0.067223 0.044821 1.33e-01 7.55e-01 ## [42,] 0.022924 0.040523 8.96e-01 4.09e-02 ## [43,] 0.009541 0.004635 9.54e-03 9.76e-01 ## [44,] 0.048384 0.016805 2.45e-02 9.10e-01 ## [45,] 0.914871 0.040505 1.46e-02 3.00e-02 ## [46,] 0.049811 0.912543 2.04e-02 1.73e-02 ## [47,] 0.038484 0.892180 4.59e-02 2.34e-02 ## [48,] 0.004484 0.002237 4.75e-03 9.89e-01 ## [49,] 0.015164 0.007890 1.73e-02 9.60e-01 ## [50,] 0.872757 0.063345 2.13e-02 4.26e-02 ## [51,] 0.006153 0.008725 9.70e-01 1.48e-02 ## [52,] 0.075851 0.025668 3.63e-02 8.62e-01 ## [53,] 0.942647 0.022073 9.90e-03 2.54e-02 ## [54,] 0.041983 0.015519 2.38e-02 9.19e-01 ## [55,] 0.017339 0.959100 1.45e-02 9.02e-03 ## [56,] 0.998933 0.000436 1.84e-04 4.47e-04 ## [57,] 0.020461 0.011470 2.85e-02 9.40e-01 ## [58,] 0.018343 0.953743 1.78e-02 1.02e-02 ## [59,] 0.037153 0.014629 2.37e-02 9.25e-01 ## [60,] 0.962608 0.013809 6.49e-03 1.71e-02 ## [61,] 0.013081 0.020545 9.40e-01 2.68e-02 ## [62,] 0.930263 0.035820 1.14e-02 2.25e-02 ## [63,] 0.954076 0.015519 7.84e-03 2.26e-02 ## [64,] 0.010680 0.019237 9.51e-01 1.93e-02 ## [65,] 0.039416 0.046127 7.88e-01 1.27e-01 ## [66,] 0.000964 0.000451 8.88e-04 9.98e-01 ## [67,] 0.973167 0.012776 4.51e-03 9.55e-03 ## [68,] 0.025463 0.953144 1.19e-02 9.53e-03 ## [69,] 0.003456 0.005041 9.83e-01 8.07e-03 ## [70,] 0.010326 0.004135 6.88e-03 9.79e-01 ## [71,] 0.033362 0.019994 5.51e-02 8.92e-01 ## [72,] 0.003079 0.993497 2.03e-03 1.40e-03 ## [73,] 0.887734 0.043108 2.00e-02 4.92e-02 ## [74,] 0.001160 0.001840 9.95e-01 2.46e-03 ## [75,] 0.092067 0.051905 1.05e-01 7.51e-01 ## ## Closest hard clustering: ## [1] 4 1 1 2 2 1 3 4 4 4 2 4 2 1 3 2 1 1 3 1 4 2 3 4 3 ## [26] 4 2 3 2 4 3 1 2 1 1 3 3 4 3 1 4 3 4 4 1 2 2 4 4 1 ## [51] 3 4 1 4 2 1 4 2 4 1 3 1 1 3 3 4 1 2 3 4 4 2 1 3 4 ## ## Available components: ## [1] &quot;centers&quot; &quot;size&quot; &quot;cluster&quot; ## [4] &quot;membership&quot; &quot;iter&quot; &quot;withinerror&quot; ## [7] &quot;call&quot; Plot membership (shown as small pie charts) library(&quot;scatterpie&quot;) ggplot() + geom_scatterpie(data = cbind(ruspini_scaled, cluster_cmeans$membership), aes(x = x, y = y), cols = colnames(cluster_cmeans$membership), legend_name = &quot;Membership&quot;) + coord_equal() 7.3 Internal Cluster Validation 7.3.1 Compare the Clustering Quality The two most popular quality metrics are the within-cluster sum of squares (WCSS) used by \\(k\\)-means and the average silhouette width. Look at within.cluster.ss and avg.silwidth below. ##library(fpc) Notes: * I do not load fpc since the NAMESPACE overwrites dbscan. * The clustering (second argument below) has to be supplied as a vector with numbers (cluster IDs) and cannot be a factor (use as.integer() to convert the factor to an ID). fpc::cluster.stats(d, km$cluster) ## $n ## [1] 75 ## ## $cluster.number ## [1] 4 ## ## $cluster.size ## [1] 17 23 15 20 ## ## $min.cluster.size ## [1] 15 ## ## $noisen ## [1] 0 ## ## $diameter ## [1] 1.463 1.159 0.836 1.119 ## ## $average.distance ## [1] 0.581 0.429 0.356 0.482 ## ## $median.distance ## [1] 0.502 0.393 0.338 0.449 ## ## $separation ## [1] 0.768 0.768 1.158 1.158 ## ## $average.toother ## [1] 2.29 2.15 2.31 2.16 ## ## $separation.matrix ## [,1] [,2] [,3] [,4] ## [1,] 0.000 0.768 1.31 1.34 ## [2,] 0.768 0.000 1.96 1.22 ## [3,] 1.308 1.958 0.00 1.16 ## [4,] 1.340 1.220 1.16 0.00 ## ## $ave.between.matrix ## [,1] [,2] [,3] [,4] ## [1,] 0.00 1.92 2.22 2.77 ## [2,] 1.92 0.00 2.75 1.89 ## [3,] 2.22 2.75 0.00 1.87 ## [4,] 2.77 1.89 1.87 0.00 ## ## $average.between ## [1] 2.22 ## ## $average.within ## [1] 0.463 ## ## $n.between ## [1] 2091 ## ## $n.within ## [1] 684 ## ## $max.diameter ## [1] 1.46 ## ## $min.separation ## [1] 0.768 ## ## $within.cluster.ss ## [1] 10.1 ## ## $clus.avg.silwidths ## 1 2 3 4 ## 0.681 0.745 0.807 0.721 ## ## $avg.silwidth ## [1] 0.737 ## ## $g2 ## NULL ## ## $g3 ## NULL ## ## $pearsongamma ## [1] 0.842 ## ## $dunn ## [1] 0.525 ## ## $dunn2 ## [1] 3.23 ## ## $entropy ## [1] 1.37 ## ## $wb.ratio ## [1] 0.209 ## ## $ch ## [1] 324 ## ## $cwidegap ## [1] 0.415 0.315 0.235 0.261 ## ## $widestgap ## [1] 0.415 ## ## $sindex ## [1] 0.858 ## ## $corrected.rand ## NULL ## ## $vi ## NULL Read ? cluster.stats for an explanation of all the available indices. sapply( list( km = km$cluster, hc_compl = cutree(hc, k = 4), hc_single = cutree(hc_single, k = 4) ), FUN = function(x) fpc::cluster.stats(d, x))[c(&quot;within.cluster.ss&quot;, &quot;avg.silwidth&quot;), ] ## km hc_compl hc_single ## within.cluster.ss 10.1 10.1 10.1 ## avg.silwidth 0.737 0.737 0.737 7.3.2 Silhouette plot library(cluster) plot(silhouette(km$cluster, d)) Note: The silhouette plot does not show correctly in R Studio if you have too many objects (bars are missing). I will work when you open a new plotting device with windows(), x11() or quartz(). ggplot visualization using factoextra fviz_silhouette(silhouette(km$cluster, d)) ## cluster size ave.sil.width ## 1 1 17 0.68 ## 2 2 23 0.75 ## 3 3 15 0.81 ## 4 4 20 0.72 7.3.3 Find Optimal Number of Clusters for k-means ggplot(ruspini_scaled, aes(x, y)) + geom_point() ## We will use different methods and try 1-10 clusters. set.seed(1234) ks &lt;- 2:10 7.3.3.1 Elbow Method: Within-Cluster Sum of Squares Calculate the within-cluster sum of squares for different numbers of clusters and look for the knee or elbow in the plot. (nstart = 5 just repeats k-means 5 times and returns the best solution) WCSS &lt;- sapply(ks, FUN = function(k) { kmeans(ruspini_scaled, centers = k, nstart = 5)$tot.withinss }) ggplot(as_tibble(ks, WCSS), aes(ks, WCSS)) + geom_line() + geom_vline(xintercept = 4, color = &quot;red&quot;, linetype = 2) 7.3.3.2 Average Silhouette Width Plot the average silhouette width for different number of clusters and look for the maximum in the plot. ASW &lt;- sapply(ks, FUN=function(k) { fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart = 5)$cluster)$avg.silwidth }) best_k &lt;- ks[which.max(ASW)] best_k ## [1] 4 ggplot(as_tibble(ks, ASW), aes(ks, ASW)) + geom_line() + geom_vline(xintercept = best_k, color = &quot;red&quot;, linetype = 2) 7.3.3.3 Dunn Index Use Dunn index (another internal measure given by min. separation/ max. diameter) DI &lt;- sapply(ks, FUN=function(k) { fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart=5)$cluster)$dunn }) best_k &lt;- ks[which.max(DI)] ggplot(as_tibble(ks, DI), aes(ks, DI)) + geom_line() + geom_vline(xintercept = best_k, color = &quot;red&quot;, linetype = 2) 7.3.3.4 Gap Statistic Compares the change in within-cluster dispersion with that expected from a null model (see ? clusGap). The default method is to choose the smallest k such that its value Gap(k) is not more than 1 standard error away from the first local maximum. library(cluster) k &lt;- clusGap(ruspini_scaled, FUN = kmeans, nstart = 10, K.max = 10) k ## Clustering Gap statistic [&quot;clusGap&quot;] from call: ## clusGap(x = ruspini_scaled, FUNcluster = kmeans, K.max = 10, nstart = 10) ## B=100 simulated reference sets, k = 1..10; spaceH0=&quot;scaledPCA&quot; ## --&gt; Number of clusters (method &#39;firstSEmax&#39;, SE.factor=1): 4 ## logW E.logW gap SE.sim ## [1,] 3.50 3.47 -0.0308 0.0357 ## [2,] 3.07 3.15 0.0762 0.0374 ## [3,] 2.68 2.90 0.2247 0.0380 ## [4,] 2.11 2.70 0.5971 0.0363 ## [5,] 1.99 2.57 0.5827 0.0347 ## [6,] 1.86 2.45 0.5871 0.0365 ## [7,] 1.73 2.35 0.6156 0.0395 ## [8,] 1.66 2.26 0.5987 0.0413 ## [9,] 1.61 2.17 0.5630 0.0409 ## [10,] 1.50 2.09 0.5910 0.0393 plot(k) Note: these methods can also be used for hierarchical clustering. There have been many other methods and indices proposed to determine the number of clusters. See, e.g., package NbClust. 7.3.4 Visualizing the Distance Matrix ggplot(ruspini_scaled, aes(x, y, color = factor(km$cluster))) + geom_point() d &lt;- dist(ruspini_scaled) Inspect the distance matrix between the first 5 objects. as.matrix(d)[1:5, 1:5] ## 1 2 3 4 5 ## 1 0.00 1.887 1.511 3.041 2.978 ## 2 1.89 0.000 0.522 1.640 1.746 ## 3 1.51 0.522 0.000 2.131 2.207 ## 4 3.04 1.640 2.131 0.000 0.282 ## 5 2.98 1.746 2.207 0.282 0.000 A false-color image visualizes each value in the matrix as a pixel with the color representing the value. library(seriation) pimage(d, col = bluered(100)) Rows and columns are the objects as they are ordered in the data set. The diagonal represents the distance between an object and itself and has by definition a distance of 0 (dark line). Visualizing the unordered distance matrix does not show much structure, but we can reorder the matrix (rows and columns) using the k-means cluster labels from cluster 1 to 4. A clear block structure representing the clusters becomes visible. pimage(d, order=order(km$cluster), col = bluered(100)) Plot function dissplot in package seriation rearranges the matrix and adds lines and cluster labels. In the lower half of the plot, it shows average dissimilarities between clusters. The function organizes the objects by cluster and then reorders clusters and objects within clusters so that more similar objects are closer together. dissplot(d, labels = km$cluster, options=list(main=&quot;k-means with k=4&quot;)) The reordering by dissplot makes the misspecification of k visible as blocks. dissplot(d, labels = kmeans(ruspini_scaled, centers = 3)$cluster, col = bluered(100)) dissplot(d, labels = kmeans(ruspini_scaled, centers = 9)$cluster, col = bluered(100)) Using factoextra fviz_dist(d) 7.4 External Cluster Validation External cluster validation uses ground truth information. That is, the user has an idea how the data should be grouped. This could be a known class label not provided to the clustering algorithm. We use an artificial data set with known groups. library(mlbench) set.seed(1234) shapes &lt;- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05) plot(shapes) Prepare data truth &lt;- as.integer(shapes$class) shapes &lt;- scale(shapes$x) colnames(shapes) &lt;- c(&quot;x&quot;, &quot;y&quot;) shapes &lt;- as_tibble(shapes) ggplot(shapes, aes(x, y)) + geom_point() Find optimal number of Clusters for k-means ks &lt;- 2:20 Use within sum of squares (look for the knee) WCSS &lt;- sapply(ks, FUN = function(k) { kmeans(shapes, centers = k, nstart = 10)$tot.withinss }) ggplot(as_tibble(ks, WCSS), aes(ks, WCSS)) + geom_line() Looks like it could be 7 clusters km &lt;- kmeans(shapes, centers = 7, nstart = 10) ggplot(shapes %&gt;% add_column(cluster = factor(km$cluster)), aes(x, y, color = cluster)) + geom_point() Hierarchical clustering: We use single-link because of the mouth is non-convex and chaining may help. d &lt;- dist(shapes) hc &lt;- hclust(d, method = &quot;single&quot;) Find optimal number of clusters ASW &lt;- sapply(ks, FUN = function(k) { fpc::cluster.stats(d, cutree(hc, k))$avg.silwidth }) ggplot(as_tibble(ks, ASW), aes(ks, ASW)) + geom_line() The maximum is clearly at 4 clusters. hc_4 &lt;- cutree(hc, 4) ggplot(shapes %&gt;% add_column(cluster = factor(hc_4)), aes(x, y, color = cluster)) + geom_point() Compare with ground truth with the corrected (=adjusted) Rand index (ARI), the variation of information (VI) index, entropy and purity. cluster_stats computes ARI and VI as comparative measures. I define functions for entropy and purity here: entropy &lt;- function(cluster, truth) { k &lt;- max(cluster, truth) cluster &lt;- factor(cluster, levels = 1:k) truth &lt;- factor(truth, levels = 1:k) w &lt;- table(cluster)/length(cluster) cnts &lt;- sapply(split(truth, cluster), table) p &lt;- sweep(cnts, 1, rowSums(cnts), &quot;/&quot;) p[is.nan(p)] &lt;- 0 e &lt;- -p * log(p, 2) sum(w * rowSums(e, na.rm = TRUE)) } purity &lt;- function(cluster, truth) { k &lt;- max(cluster, truth) cluster &lt;- factor(cluster, levels = 1:k) truth &lt;- factor(truth, levels = 1:k) w &lt;- table(cluster)/length(cluster) cnts &lt;- sapply(split(truth, cluster), table) p &lt;- sweep(cnts, 1, rowSums(cnts), &quot;/&quot;) p[is.nan(p)] &lt;- 0 sum(w * apply(p, 1, max)) } calculate measures (for comparison we also use random “clusterings” with 4 and 6 clusters) random_4 &lt;- sample(1:4, nrow(shapes), replace = TRUE) random_6 &lt;- sample(1:6, nrow(shapes), replace = TRUE) r &lt;- rbind( kmeans_7 = c( unlist(fpc::cluster.stats(d, km$cluster, truth, compareonly = TRUE)), entropy = entropy(km$cluster, truth), purity = purity(km$cluster, truth) ), hc_4 = c( unlist(fpc::cluster.stats(d, hc_4, truth, compareonly = TRUE)), entropy = entropy(hc_4, truth), purity = purity(hc_4, truth) ), random_4 = c( unlist(fpc::cluster.stats(d, random_4, truth, compareonly = TRUE)), entropy = entropy(random_4, truth), purity = purity(random_4, truth) ), random_6 = c( unlist(fpc::cluster.stats(d, random_6, truth, compareonly = TRUE)), entropy = entropy(random_6, truth), purity = purity(random_6, truth) ) ) r ## corrected.rand vi entropy purity ## kmeans_7 0.63823 0.571 0.229 0.464 ## hc_4 1.00000 0.000 0.000 1.000 ## random_4 -0.00324 2.683 1.988 0.288 ## random_6 -0.00213 3.076 1.728 0.144 Notes: Hierarchical clustering found the perfect clustering. Entropy and purity are heavily impacted by the number of clusters (more clusters improve the metric). The corrected rand index shows clearly that the random clusterings have no relationship with the ground truth (very close to 0). This is a very helpful property. Read ? cluster.stats for an explanation of all the available indices. 7.5 Advanced Data Preparation for Clustering 7.5.1 Outlier Removal Most clustering algorithms perform complete assignment (i.e., all data points need to be assigned to a cluster). Outliers will affect the clustering. It is useful to identify outliers and remove strong outliers prior to clustering. A density based method to identify outlier is LOF (Local Outlier Factor). It is related to dbscan and compares the density around a point with the densities around its neighbors (you have to specify the neighborhood size \\(k\\)). The LOF value for a regular data point is 1. The larger the LOF value gets, the more likely the point is an outlier. library(dbscan) Add a clear outlier to the scaled Ruspini dataset that is 10 standard deviations above the average for the x axis. ruspini_scaled_outlier &lt;- ruspini_scaled %&gt;% add_case(x=10,y=0) 7.5.1.1 Visual inspection of the data Outliers can be identified using summary statistics, histograms, scatterplots (pairs plots), and boxplots, etc. We use here a pairs plot (the diagonal contains smoothed histograms). The outlier is visible as the single separate point in the scatter plot and as the long tail of the smoothed histogram for x (we would expect most observations to fall in the range [-3,3] in normalized data). library(&quot;GGally&quot;) ggpairs(ruspini_scaled_outlier) The outlier is a problem for k-means km &lt;- kmeans(ruspini_scaled_outlier, centers = 4, nstart = 10) ruspini_scaled_outlier_km &lt;- ruspini_scaled_outlier%&gt;% add_column(cluster = factor(km$cluster)) centroids &lt;- as_tibble(km$centers, rownames = &quot;cluster&quot;) ggplot(ruspini_scaled_outlier_km, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10) This problem can be fixed by increasing the number of clusters and removing small clusters in a post-processing step or by identifying and removing outliers before clustering. 7.5.1.2 Local Outlier Factor (LOF) The Local Outlier Factor is related to concepts of DBSCAN can help to identify potential outliers. Calculate the LOF (I choose a neighborhood size of 10 for density estimation), lof &lt;- lof(ruspini_scaled_outlier, k = 10) ## Warning in lof(ruspini_scaled_outlier, k = 10): lof: k ## is now deprecated. use minPts = 11 instead . lof ## [1] 0.978 1.044 0.926 1.024 1.022 0.966 1.154 ## [8] 1.071 1.124 1.058 1.008 0.933 1.000 1.074 ## [15] 1.008 0.987 1.184 0.989 0.984 1.080 0.911 ## [22] 1.019 1.524 0.979 1.045 0.958 1.022 0.934 ## [29] 0.979 1.470 0.964 0.988 0.973 1.236 1.082 ## [36] 1.326 1.566 1.018 0.998 1.029 1.378 1.107 ## [43] 0.952 1.083 1.091 1.029 1.181 1.009 1.031 ## [50] 1.030 1.002 1.201 1.001 1.071 0.968 0.954 ## [57] 1.046 0.970 1.066 1.045 0.989 0.966 1.028 ## [64] 0.991 1.152 0.942 0.977 1.000 0.984 0.998 ## [71] 1.174 0.996 1.116 0.934 1.588 17.027 ggplot(ruspini_scaled_outlier %&gt;% add_column(lof = lof), aes(x, y, color = lof)) + geom_point() + scale_color_gradient(low = &quot;gray&quot;, high = &quot;red&quot;) Plot the points sorted by increasing LOF and look for a knee. ggplot(tibble(index = seq_len(length(lof)), lof = sort(lof)), aes(index, lof)) + geom_line() + geom_hline(yintercept = 1, color = &quot;red&quot;, linetype = 2) Choose a threshold above 1. ggplot(ruspini_scaled_outlier %&gt;% add_column(outlier = lof &gt;= 2), aes(x, y, color = outlier)) + geom_point() ## Analyze the found outliers (they might be interesting data points) and then cluster the data without them. ruspini_scaled_clean &lt;- ruspini_scaled_outlier %&gt;% filter(lof &lt; 2) km &lt;- kmeans(ruspini_scaled_clean, centers = 4, nstart = 10) ruspini_scaled_clean_km &lt;- ruspini_scaled_clean%&gt;% add_column(cluster = factor(km$cluster)) centroids &lt;- as_tibble(km$centers, rownames = &quot;cluster&quot;) ggplot(ruspini_scaled_clean_km, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10) There are many other outlier removal strategies available. See, e.g., package outliers. 7.5.2 Clustering Tendency Most clustering algorithms will always produce a clustering, even if the data does not contain a cluster structure. It is typically good to check cluster tendency before attempting to cluster the data. We use again the smiley data. library(mlbench) shapes &lt;- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)$x colnames(shapes) &lt;- c(&quot;x&quot;, &quot;y&quot;) shapes &lt;- as_tibble(shapes) 7.5.2.1 Scatter plots The first step is visual inspection using scatter plots. ggplot(shapes, aes(x = x, y = y)) + geom_point() Cluster tendency is typically indicated by several separated point clouds. Often an appropriate number of clusters can also be visually obtained by counting the number of point clouds. We see four clusters, but the mouth is not convex/spherical and thus will pose a problems to algorithms like k-means. If the data has more than two features then you can use a pairs plot (scatterplot matrix) or look at a scatterplot of the first two principal components using PCA. #### Visual Analysis for Cluster Tendency Assessment (VAT) VAT reorders the objects to show potential clustering tendency as a block structure (dark blocks along the main diagonal). We scale the data before using Euclidean distance. library(seriation) d_shapes &lt;- dist(scale(shapes)) VAT(d_shapes, col = bluered(100)) iVAT uses the largest distances for all possible paths between two objects instead of the direct distances to make the block structure better visible. iVAT(d_shapes, col = bluered(100)) 7.5.2.2 Hopkins statistic factoextra can also create a VAT plot and calculate the Hopkins statistic to assess clustering tendency. For the Hopkins statistic, a sample of size \\(n\\) is drawn from the data and then compares the nearest neighbor distribution with a simulated dataset drawn from a random uniform distribution (see detailed explanation). A values &gt;.5 indicates usually a clustering tendency. get_clust_tendency(shapes, n = 10) ## $hopkins_stat ## [1] 0.907 ## ## $plot Both plots show a strong cluster structure with 4 clusters. 7.5.2.3 Data Without Clustering Tendency data_random &lt;- tibble(x = runif(500), y = runif(500)) ggplot(data_random, aes(x, y)) + geom_point() No point clouds are visible, just noise. d_random &lt;- dist(data_random) VAT(d_random, col = bluered(100)) iVAT(d_random, col = bluered(100)) get_clust_tendency(data_random, n = 10, graph = FALSE) ## $hopkins_stat ## [1] 0.464 ## ## $plot ## NULL There is very little clustering structure visible indicating low clustering tendency and clustering should not be performed on this data. However, k-means can be used to partition the data into \\(k\\) regions of roughly equivalent size. This can be used as a data-driven discretization of the space. 7.5.2.4 k-means on Data Without Clustering Tendency What happens if we perform k-means on data that has no inherent clustering structure? km &lt;- kmeans(data_random, centers = 4) random_clustered&lt;- data_random %&gt;% add_column(cluster = factor(km$cluster)) ggplot(random_clustered, aes(x = x, y = y, color = cluster)) + geom_point() k-means discretizes the space into similarly sized regions. References "],["references.html", "References", " References "]]
