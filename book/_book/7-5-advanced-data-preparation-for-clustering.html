<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta property="og:title" content="7.5 Advanced Data Preparation for Clustering | R Code Companion for the Textbook Introduction to Data Mining" />
<meta property="og:type" content="book" />


<meta property="og:description" content="This book contains documented R examples to accompany several chapters of the popular data mining text book Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach and Vipin Kumar (2006 or 2017 edition)." />


<meta name="author" content="Michael Hahsler" />

<meta name="date" content="2021-07-09" />

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<meta name="description" content="This book contains documented R examples to accompany several chapters of the popular data mining text book Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach and Vipin Kumar (2006 or 2017 edition).">

<title>7.5 Advanced Data Preparation for Clustering | R Code Companion for the Textbook Introduction to Data Mining</title>

<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap/css/united.min.css" rel="stylesheet" />
<script src="libs/bootstrap/js/bootstrap.min.js"></script>
<script src="libs/bootstrap/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="libs/navigation/tabsets.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/plotly-binding/plotly.js"></script>
<script src="libs/typedarray/typedarray.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main/plotly-latest.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider/jquery.nouislider.min.js"></script>
<link href="libs/selectize/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize/selectize.min.js"></script>
<link href="libs/vis/vis.css" rel="stylesheet" />
<script src="libs/vis/vis.min.js"></script>
<script src="libs/visNetwork-binding/visNetwork.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
/* show arrow before summary tag as in bootstrap
TODO: remove if boostrap in updated in html_document (rmarkdown#1485) */
details > summary {
  display: list-item;
  cursor: pointer;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#preface">Preface</a></li>
<li><a href="1-introduction.html#introduction"><span class="toc-section-number">1</span> Introduction</a>
<ul>
<li><a href="1-1-data-manipulation-with-tidyverse.html#data-manipulation-with-tidyverse"><span class="toc-section-number">1.1</span> Data Manipulation with Tidyverse</a></li>
<li><a href="1-2-visualization-with-ggplot2.html#visualization-with-ggplot2"><span class="toc-section-number">1.2</span> Visualization with ggplot2</a></li>
</ul></li>
<li><a href="2-data.html#data"><span class="toc-section-number">2</span> Data</a>
<ul>
<li><a href="2-1-the-iris-dataset.html#the-iris-dataset"><span class="toc-section-number">2.1</span> The Iris Dataset</a></li>
<li><a href="2-2-data-quality.html#data-quality"><span class="toc-section-number">2.2</span> Data Quality</a></li>
<li><a href="2-3-aggregation.html#aggregation"><span class="toc-section-number">2.3</span> Aggregation</a></li>
<li><a href="2-4-sampling.html#sampling"><span class="toc-section-number">2.4</span> Sampling</a>
<ul>
<li><a href="2-4-sampling.html#random-sampling"><span class="toc-section-number">2.4.1</span> Random Sampling</a></li>
<li><a href="2-4-sampling.html#stratified-sampling"><span class="toc-section-number">2.4.2</span> Stratified Sampling</a></li>
</ul></li>
<li><a href="2-5-features.html#features"><span class="toc-section-number">2.5</span> Features</a>
<ul>
<li><a href="2-5-features.html#dimensionality-reduction"><span class="toc-section-number">2.5.1</span> Dimensionality Reduction</a></li>
<li><a href="2-5-features.html#feature-selection"><span class="toc-section-number">2.5.2</span> Feature Selection</a></li>
<li><a href="2-5-features.html#discretize-features"><span class="toc-section-number">2.5.3</span> Discretize Features</a></li>
<li><a href="2-5-features.html#standardize-data-z-scores"><span class="toc-section-number">2.5.4</span> Standardize Data (Z-Scores)</a></li>
</ul></li>
<li><a href="2-6-proximities-similarities-and-distances.html#proximities-similarities-and-distances"><span class="toc-section-number">2.6</span> Proximities: Similarities and Distances</a>
<ul>
<li><a href="2-6-proximities-similarities-and-distances.html#minkowsky-distances"><span class="toc-section-number">2.6.1</span> Minkowsky Distances</a></li>
<li><a href="2-6-proximities-similarities-and-distances.html#distances-for-binary-data"><span class="toc-section-number">2.6.2</span> Distances for Binary Data</a></li>
<li><a href="2-6-proximities-similarities-and-distances.html#distances-for-mixed-data"><span class="toc-section-number">2.6.3</span> Distances for Mixed Data</a></li>
<li><a href="2-6-proximities-similarities-and-distances.html#additional-proximity-measures-available-in-package-proxy"><span class="toc-section-number">2.6.4</span> Additional proximity Measures Available in Package proxy</a></li>
</ul></li>
<li><a href="2-7-relationships-between-features.html#relationships-between-features"><span class="toc-section-number">2.7</span> Relationships Between Features</a>
<ul>
<li><a href="2-7-relationships-between-features.html#correlation"><span class="toc-section-number">2.7.1</span> Correlation</a></li>
<li><a href="2-7-relationships-between-features.html#rank-correlation"><span class="toc-section-number">2.7.2</span> Rank Correlation</a></li>
<li><a href="2-7-relationships-between-features.html#relationship-between-nominal-and-ordinal-features"><span class="toc-section-number">2.7.3</span> Relationship Between Nominal and Ordinal Features</a></li>
</ul></li>
<li><a href="2-8-density-estimation.html#density-estimation"><span class="toc-section-number">2.8</span> Density Estimation</a></li>
<li><a href="2-9-exploring-data.html#exploring-data"><span class="toc-section-number">2.9</span> Exploring Data</a>
<ul>
<li><a href="2-9-exploring-data.html#basic-statistics"><span class="toc-section-number">2.9.1</span> Basic statistics</a></li>
<li><a href="2-9-exploring-data.html#tabulate-data"><span class="toc-section-number">2.9.2</span> Tabulate data</a></li>
<li><a href="2-9-exploring-data.html#percentiles-quantiles"><span class="toc-section-number">2.9.3</span> Percentiles (Quantiles)</a></li>
</ul></li>
<li><a href="2-10-visualization.html#visualization"><span class="toc-section-number">2.10</span> Visualization</a>
<ul>
<li><a href="2-10-visualization.html#histogram"><span class="toc-section-number">2.10.1</span> Histogram</a></li>
<li><a href="2-10-visualization.html#boxplot"><span class="toc-section-number">2.10.2</span> Boxplot</a></li>
<li><a href="2-10-visualization.html#scatter-plot"><span class="toc-section-number">2.10.3</span> Scatter plot</a></li>
<li><a href="2-10-visualization.html#scatter-plot-matrix"><span class="toc-section-number">2.10.4</span> Scatter Plot Matrix</a></li>
<li><a href="2-10-visualization.html#data-matrix-visualization"><span class="toc-section-number">2.10.5</span> Data Matrix Visualization</a></li>
<li><a href="2-10-visualization.html#correlation-matrix"><span class="toc-section-number">2.10.6</span> Correlation Matrix</a></li>
<li><a href="2-10-visualization.html#parallel-coordinates-plot"><span class="toc-section-number">2.10.7</span> Parallel Coordinates Plot</a></li>
</ul></li>
</ul></li>
<li><a href="3-classification-basic-concepts-and-techniques.html#classification-basic-concepts-and-techniques"><span class="toc-section-number">3</span> Classification: Basic Concepts and Techniques</a>
<ul>
<li><a href="3-1-the-zoo-dataset.html#the-zoo-dataset"><span class="toc-section-number">3.1</span> The Zoo Dataset</a></li>
<li><a href="3-2-decision-trees.html#decision-trees"><span class="toc-section-number">3.2</span> Decision Trees</a>
<ul>
<li><a href="3-2-decision-trees.html#create-tree-with-default-settings-uses-pre-pruning"><span class="toc-section-number">3.2.1</span> Create Tree With Default Settings (uses pre-pruning)</a></li>
<li><a href="3-2-decision-trees.html#create-a-full-tree"><span class="toc-section-number">3.2.2</span> Create a Full Tree</a></li>
<li><a href="3-2-decision-trees.html#make-predictions-for-new-data"><span class="toc-section-number">3.2.3</span> Make Predictions for New Data</a></li>
</ul></li>
<li><a href="3-3-model-evaluation-with-caret.html#model-evaluation-with-caret"><span class="toc-section-number">3.3</span> Model Evaluation with Caret</a>
<ul>
<li><a href="3-3-model-evaluation-with-caret.html#hold-out-test-data"><span class="toc-section-number">3.3.1</span> Hold out Test Data</a></li>
<li><a href="3-3-model-evaluation-with-caret.html#learn-a-model-and-tune-hyperparameters-on-the-training-data"><span class="toc-section-number">3.3.2</span> Learn a Model and Tune Hyperparameters on the Training Data</a></li>
</ul></li>
<li><a href="3-4-testing-confusion-matrix-and-confidence-interval-for-accuracy.html#testing-confusion-matrix-and-confidence-interval-for-accuracy"><span class="toc-section-number">3.4</span> Testing: Confusion Matrix and Confidence Interval for Accuracy</a></li>
<li><a href="3-5-model-comparison.html#model-comparison"><span class="toc-section-number">3.5</span> Model Comparison</a></li>
<li><a href="3-6-feature-selection-and-feature-preparation.html#feature-selection-and-feature-preparation"><span class="toc-section-number">3.6</span> Feature Selection and Feature Preparation</a>
<ul>
<li><a href="3-6-feature-selection-and-feature-preparation.html#univariate-feature-importance-score"><span class="toc-section-number">3.6.1</span> Univariate Feature Importance Score</a></li>
<li><a href="3-6-feature-selection-and-feature-preparation.html#feature-subset-selection"><span class="toc-section-number">3.6.2</span> Feature Subset Selection</a></li>
<li><a href="3-6-feature-selection-and-feature-preparation.html#using-dummy-variables-for-factors"><span class="toc-section-number">3.6.3</span> Using Dummy Variables for Factors</a></li>
</ul></li>
<li><a href="3-7-class-imbalance.html#class-imbalance"><span class="toc-section-number">3.7</span> Class Imbalance</a>
<ul>
<li><a href="3-7-class-imbalance.html#option-1-use-the-data-as-is-and-hope-for-the-best"><span class="toc-section-number">3.7.1</span> Option 1: Use the Data As Is and Hope For The Best</a></li>
<li><a href="3-7-class-imbalance.html#option-2-balance-data-with-resampling"><span class="toc-section-number">3.7.2</span> Option 2: Balance Data With Resampling</a></li>
<li><a href="3-7-class-imbalance.html#option-3-build-a-larger-tree-and-use-predicted-probabilities"><span class="toc-section-number">3.7.3</span> Option 3: Build A Larger Tree and use Predicted Probabilities</a></li>
<li><a href="3-7-class-imbalance.html#option-4-use-a-cost-sensitive-classifier"><span class="toc-section-number">3.7.4</span> Option 4: Use a Cost-Sensitive Classifier</a></li>
</ul></li>
</ul></li>
<li><a href="4-classification-alternative-techniques.html#classification-alternative-techniques"><span class="toc-section-number">4</span> Classification: Alternative Techniques</a>
<ul>
<li><a href="4-1-training-and-test-data.html#training-and-test-data"><span class="toc-section-number">4.1</span> Training and Test Data</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#fitting-different-classification-models-to-the-training-data"><span class="toc-section-number">4.2</span> Fitting Different Classification Models to the Training Data</a>
<ul>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#conditional-inference-tree-decision-tree"><span class="toc-section-number">4.2.1</span> Conditional Inference Tree (Decision Tree)</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#c-4.5-decision-tree"><span class="toc-section-number">4.2.2</span> C 4.5 Decision Tree</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#k-nearest-neighbors"><span class="toc-section-number">4.2.3</span> K-Nearest Neighbors</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#part-rule-based-classifier"><span class="toc-section-number">4.2.4</span> PART (Rule-based classifier)</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#linear-support-vector-machines"><span class="toc-section-number">4.2.5</span> Linear Support Vector Machines</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#random-forest"><span class="toc-section-number">4.2.6</span> Random Forest</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#gradient-boosted-decision-trees-xgboost"><span class="toc-section-number">4.2.7</span> Gradient Boosted Decision Trees (xgboost)</a></li>
<li><a href="4-2-fitting-different-classification-models-to-the-training-data.html#artificial-neural-network"><span class="toc-section-number">4.2.8</span> Artificial Neural Network</a></li>
</ul></li>
<li><a href="4-3-comparing-models.html#comparing-models"><span class="toc-section-number">4.3</span> Comparing Models</a></li>
<li><a href="4-4-applying-the-chosen-model-to-the-test-data.html#applying-the-chosen-model-to-the-test-data"><span class="toc-section-number">4.4</span> Applying the Chosen Model to the Test Data</a></li>
<li><a href="4-5-decision-boundaries.html#decision-boundaries"><span class="toc-section-number">4.5</span> Decision Boundaries</a>
<ul>
<li><a href="4-5-decision-boundaries.html#iris-dataset"><span class="toc-section-number">4.5.1</span> Iris Dataset</a></li>
<li><a href="4-5-decision-boundaries.html#circle-dataset"><span class="toc-section-number">4.5.2</span> Circle Dataset</a></li>
</ul></li>
<li><a href="4-6-more-information.html#more-information"><span class="toc-section-number">4.6</span> More Information</a></li>
</ul></li>
<li><a href="5-association-analysis-basic-concepts-and-algorithms.html#association-analysis-basic-concepts-and-algorithms"><span class="toc-section-number">5</span> Association Analysis: Basic Concepts and Algorithms</a>
<ul>
<li><a href="5-1-the-arules-package.html#the-arules-package"><span class="toc-section-number">5.1</span> The arules Package</a></li>
<li><a href="5-2-transactions.html#transactions"><span class="toc-section-number">5.2</span> Transactions</a>
<ul>
<li><a href="5-2-transactions.html#create-transactions"><span class="toc-section-number">5.2.1</span> Create Transactions</a></li>
<li><a href="5-2-transactions.html#inspect-transactions"><span class="toc-section-number">5.2.2</span> Inspect Transactions</a></li>
<li><a href="5-2-transactions.html#vertical-layout-transaction-id-lists"><span class="toc-section-number">5.2.3</span> Vertical Layout (Transaction ID Lists)</a></li>
</ul></li>
<li><a href="5-3-frequent-itemsets.html#frequent-itemsets"><span class="toc-section-number">5.3</span> Frequent Itemsets</a>
<ul>
<li><a href="5-3-frequent-itemsets.html#mine-frequent-itemsets"><span class="toc-section-number">5.3.1</span> Mine Frequent Itemsets</a></li>
<li><a href="5-3-frequent-itemsets.html#concise-representation-of-itemsets"><span class="toc-section-number">5.3.2</span> Concise Representation of Itemsets</a></li>
</ul></li>
<li><a href="5-4-association-rules.html#association-rules"><span class="toc-section-number">5.4</span> Association Rules</a>
<ul>
<li><a href="5-4-association-rules.html#mine-association-rules"><span class="toc-section-number">5.4.1</span> Mine Association Rules</a></li>
<li><a href="5-4-association-rules.html#calculate-additional-interest-measures"><span class="toc-section-number">5.4.2</span> Calculate Additional Interest Measures</a></li>
<li><a href="5-4-association-rules.html#mine-using-templates"><span class="toc-section-number">5.4.3</span> Mine Using Templates</a></li>
</ul></li>
<li><a href="5-5-association-rule-visualization.html#association-rule-visualization"><span class="toc-section-number">5.5</span> Association Rule Visualization</a></li>
<li><a href="5-6-interactive-visualizations.html#interactive-visualizations"><span class="toc-section-number">5.6</span> Interactive Visualizations</a>
<ul>
<li><a href="5-6-interactive-visualizations.html#interactive-inspect-with-sorting-filtering-and-paging"><span class="toc-section-number">5.6.1</span> Interactive Inspect With Sorting, Filtering and Paging</a></li>
<li><a href="5-6-interactive-visualizations.html#scatter-plot-1"><span class="toc-section-number">5.6.2</span> Scatter Plot</a></li>
<li><a href="5-6-interactive-visualizations.html#matrix-visualization"><span class="toc-section-number">5.6.3</span> Matrix Visualization</a></li>
<li><a href="5-6-interactive-visualizations.html#visualization-as-graph"><span class="toc-section-number">5.6.4</span> Visualization as Graph</a></li>
<li><a href="5-6-interactive-visualizations.html#interactive-rule-explorer"><span class="toc-section-number">5.6.5</span> Interactive Rule Explorer</a></li>
</ul></li>
</ul></li>
<li><a href="6-association-analysis-advanced-concepts.html#association-analysis-advanced-concepts"><span class="toc-section-number">6</span> Association Analysis: Advanced Concepts</a></li>
<li><a href="7-clustering-analysis.html#clustering-analysis"><span class="toc-section-number">7</span> Clustering Analysis</a>
<ul>
<li><a href="7-1-data-preparation.html#data-preparation"><span class="toc-section-number">7.1</span> Data Preparation</a>
<ul>
<li><a href="7-1-data-preparation.html#data-cleaning"><span class="toc-section-number">7.1.1</span> Data cleaning</a></li>
<li><a href="7-1-data-preparation.html#scale-data"><span class="toc-section-number">7.1.2</span> Scale data</a></li>
</ul></li>
<li><a href="7-2-clustering-methods.html#clustering-methods"><span class="toc-section-number">7.2</span> Clustering methods</a>
<ul>
<li><a href="7-2-clustering-methods.html#k-means-clustering"><span class="toc-section-number">7.2.1</span> k-means Clustering</a></li>
<li><a href="7-2-clustering-methods.html#hierarchical-clustering"><span class="toc-section-number">7.2.2</span> Hierarchical Clustering</a></li>
<li><a href="7-2-clustering-methods.html#density-based-clustering-with-dbscan"><span class="toc-section-number">7.2.3</span> Density-based clustering with DBSCAN</a></li>
<li><a href="7-2-clustering-methods.html#partitioning-around-medoids-pam"><span class="toc-section-number">7.2.4</span> Partitioning Around Medoids (PAM)</a></li>
<li><a href="7-2-clustering-methods.html#gaussian-mixture-models"><span class="toc-section-number">7.2.5</span> Gaussian Mixture Models</a></li>
<li><a href="7-2-clustering-methods.html#spectral-clustering"><span class="toc-section-number">7.2.6</span> Spectral clustering</a></li>
<li><a href="7-2-clustering-methods.html#fuzzy-c-means-clustering"><span class="toc-section-number">7.2.7</span> Fuzzy C-Means Clustering</a></li>
</ul></li>
<li><a href="7-3-internal-cluster-validation.html#internal-cluster-validation"><span class="toc-section-number">7.3</span> Internal Cluster Validation</a>
<ul>
<li><a href="7-3-internal-cluster-validation.html#compare-the-clustering-quality"><span class="toc-section-number">7.3.1</span> Compare the Clustering Quality</a></li>
<li><a href="7-3-internal-cluster-validation.html#silhouette-plot"><span class="toc-section-number">7.3.2</span> Silhouette plot</a></li>
<li><a href="7-3-internal-cluster-validation.html#find-optimal-number-of-clusters-for-k-means"><span class="toc-section-number">7.3.3</span> Find Optimal Number of Clusters for k-means</a></li>
<li><a href="7-3-internal-cluster-validation.html#visualizing-the-distance-matrix"><span class="toc-section-number">7.3.4</span> Visualizing the Distance Matrix</a></li>
</ul></li>
<li><a href="7-4-external-cluster-validation.html#external-cluster-validation"><span class="toc-section-number">7.4</span> External Cluster Validation</a></li>
<li><a href="7-5-advanced-data-preparation-for-clustering.html#advanced-data-preparation-for-clustering"><span class="toc-section-number">7.5</span> Advanced Data Preparation for Clustering</a>
<ul>
<li><a href="7-5-advanced-data-preparation-for-clustering.html#outlier-removal"><span class="toc-section-number">7.5.1</span> Outlier Removal</a></li>
<li><a href="7-5-advanced-data-preparation-for-clustering.html#clustering-tendency"><span class="toc-section-number">7.5.2</span> Clustering Tendency</a></li>
</ul></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="advanced-data-preparation-for-clustering" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Advanced Data Preparation for Clustering</h2>
<div id="outlier-removal" class="section level3" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Outlier Removal</h3>
<p>Most clustering algorithms perform complete assignment (i.e., all data points need to be assigned to a cluster). Outliers will affect the clustering. It is useful to identify outliers and remove strong outliers prior to clustering.
A density based method to identify outlier is <a href="https://en.wikipedia.org/wiki/Local_outlier_factor">LOF</a> (Local Outlier Factor).
It is related to dbscan and compares the density around a point with the
densities around its neighbors (you have to specify the neighborhood size <span class="math inline">\(k\)</span>).
The LOF value for a regular data point is 1.
The larger the LOF value gets, the more likely the point is an outlier.</p>
<div class="sourceCode" id="cb767"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb767-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb767-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dbscan)</span></code></pre></div>
<p>Add a clear outlier to the scaled Ruspini dataset that is 10 standard deviations above the average for the x axis.</p>
<div class="sourceCode" id="cb768"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb768-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb768-1" aria-hidden="true" tabindex="-1"></a>ruspini_scaled_outlier <span class="ot">&lt;-</span> ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_case</span>(<span class="at">x=</span><span class="dv">10</span>,<span class="at">y=</span><span class="dv">0</span>)</span></code></pre></div>
<div id="visual-inspection-of-the-data" class="section level4" number="7.5.1.1">
<h4><span class="header-section-number">7.5.1.1</span> Visual inspection of the data</h4>
<p>Outliers can be identified using summary statistics, histograms, scatterplots (pairs plots), and boxplots, etc. We use here a pairs plot (the diagonal contains smoothed histograms). The outlier is visible as the single separate point in the scatter plot and as the long tail of the smoothed histogram for <code>x</code> (we would expect most observations to fall in the range [-3,3] in normalized data).</p>
<div class="sourceCode" id="cb769"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb769-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb769-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;GGally&quot;</span>)</span>
<span id="cb769-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb769-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(ruspini_scaled_outlier)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-290-1.png" width="672" /></p>
<p>The outlier is a problem for k-means</p>
<div class="sourceCode" id="cb770"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb770-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb770-1" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(ruspini_scaled_outlier, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb770-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb770-2" aria-hidden="true" tabindex="-1"></a>ruspini_scaled_outlier_km <span class="ot">&lt;-</span> ruspini_scaled_outlier<span class="sc">%&gt;%</span></span>
<span id="cb770-3"><a href="7-5-advanced-data-preparation-for-clustering.html#cb770-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster))</span>
<span id="cb770-4"><a href="7-5-advanced-data-preparation-for-clustering.html#cb770-4" aria-hidden="true" tabindex="-1"></a>centroids <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(km<span class="sc">$</span>centers, <span class="at">rownames =</span> <span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb770-5"><a href="7-5-advanced-data-preparation-for-clustering.html#cb770-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb770-6"><a href="7-5-advanced-data-preparation-for-clustering.html#cb770-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled_outlier_km, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb770-7"><a href="7-5-advanced-data-preparation-for-clustering.html#cb770-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> centroids, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster), <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">size =</span> <span class="dv">10</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-291-1.png" width="672" /></p>
<p>This problem can be fixed by increasing the number of clusters and removing small clusters in a post-processing step or by identifying and removing outliers before clustering.</p>
</div>
<div id="local-outlier-factor-lof" class="section level4" number="7.5.1.2">
<h4><span class="header-section-number">7.5.1.2</span> Local Outlier Factor (LOF)</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Local_outlier_factor">Local Outlier Factor</a> is related to concepts of DBSCAN can help to identify potential outliers.
Calculate the LOF (I choose a neighborhood size of 10 for density estimation),</p>
<div class="sourceCode" id="cb771"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb771-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb771-1" aria-hidden="true" tabindex="-1"></a>lof <span class="ot">&lt;-</span> <span class="fu">lof</span>(ruspini_scaled_outlier, <span class="at">k =</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## Warning in lof(ruspini_scaled_outlier, k = 10): lof: k
## is now deprecated. use minPts = 11 instead .</code></pre>
<div class="sourceCode" id="cb773"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb773-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb773-1" aria-hidden="true" tabindex="-1"></a>lof</span></code></pre></div>
<pre><code>##  [1]  0.978  1.044  0.926  1.024  1.022  0.966  1.154
##  [8]  1.071  1.124  1.058  1.008  0.933  1.000  1.074
## [15]  1.008  0.987  1.184  0.989  0.984  1.080  0.911
## [22]  1.019  1.524  0.979  1.045  0.958  1.022  0.934
## [29]  0.979  1.470  0.964  0.988  0.973  1.236  1.082
## [36]  1.326  1.566  1.018  0.998  1.029  1.378  1.107
## [43]  0.952  1.083  1.091  1.029  1.181  1.009  1.031
## [50]  1.030  1.002  1.201  1.001  1.071  0.968  0.954
## [57]  1.046  0.970  1.066  1.045  0.989  0.966  1.028
## [64]  0.991  1.152  0.942  0.977  1.000  0.984  0.998
## [71]  1.174  0.996  1.116  0.934  1.588 17.027</code></pre>
<div class="sourceCode" id="cb775"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb775-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb775-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled_outlier <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">lof =</span> lof), <span class="fu">aes</span>(x, y, <span class="at">color =</span> lof)) <span class="sc">+</span></span>
<span id="cb775-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb775-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">scale_color_gradient</span>(<span class="at">low =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">high =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-292-1.png" width="672" /></p>
<p>Plot the points sorted by increasing LOF and look for a knee.</p>
<div class="sourceCode" id="cb776"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb776-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb776-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">tibble</span>(<span class="at">index =</span> <span class="fu">seq_len</span>(<span class="fu">length</span>(lof)), <span class="at">lof =</span> <span class="fu">sort</span>(lof)), <span class="fu">aes</span>(index, lof)) <span class="sc">+</span></span>
<span id="cb776-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb776-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb776-3"><a href="7-5-advanced-data-preparation-for-clustering.html#cb776-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">linetype =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-293-1.png" width="672" /></p>
<p>Choose a threshold above 1.</p>
<div class="sourceCode" id="cb777"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb777-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb777-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled_outlier <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">outlier =</span> lof <span class="sc">&gt;=</span> <span class="dv">2</span>), <span class="fu">aes</span>(x, y, <span class="at">color =</span> outlier)) <span class="sc">+</span></span>
<span id="cb777-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb777-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-294-1.png" width="672" /></p>
<div class="sourceCode" id="cb778"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb778-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Analyze the found outliers (they might be interesting data points) and then cluster the data without them.</span></span>
<span id="cb778-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-2" aria-hidden="true" tabindex="-1"></a>ruspini_scaled_clean <span class="ot">&lt;-</span> ruspini_scaled_outlier  <span class="sc">%&gt;%</span> <span class="fu">filter</span>(lof <span class="sc">&lt;</span> <span class="dv">2</span>)</span>
<span id="cb778-3"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb778-4"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-4" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(ruspini_scaled_clean, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb778-5"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-5" aria-hidden="true" tabindex="-1"></a>ruspini_scaled_clean_km <span class="ot">&lt;-</span> ruspini_scaled_clean<span class="sc">%&gt;%</span></span>
<span id="cb778-6"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster))</span>
<span id="cb778-7"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-7" aria-hidden="true" tabindex="-1"></a>centroids <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(km<span class="sc">$</span>centers, <span class="at">rownames =</span> <span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb778-8"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb778-9"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled_clean_km, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb778-10"><a href="7-5-advanced-data-preparation-for-clustering.html#cb778-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> centroids, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster), <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">size =</span> <span class="dv">10</span>)</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-294-2.png" width="672" /></p>
<p>There are many other outlier removal strategies available. See, e.g., package
<a href="https://cran.r-project.org/package=outliers">outliers</a>.</p>
</div>
</div>
<div id="clustering-tendency" class="section level3" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Clustering Tendency</h3>
<p>Most clustering algorithms will always produce a clustering, even if the
data does not contain a cluster structure. It is typically good to check
cluster tendency before attempting to cluster the data.</p>
<p>We use again the smiley data.</p>
<div class="sourceCode" id="cb779"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb779-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb779-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlbench)</span>
<span id="cb779-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb779-2" aria-hidden="true" tabindex="-1"></a>shapes <span class="ot">&lt;-</span> <span class="fu">mlbench.smiley</span>(<span class="at">n =</span> <span class="dv">500</span>, <span class="at">sd1 =</span> <span class="fl">0.1</span>, <span class="at">sd2 =</span> <span class="fl">0.05</span>)<span class="sc">$</span>x</span>
<span id="cb779-3"><a href="7-5-advanced-data-preparation-for-clustering.html#cb779-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(shapes) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>)</span>
<span id="cb779-4"><a href="7-5-advanced-data-preparation-for-clustering.html#cb779-4" aria-hidden="true" tabindex="-1"></a>shapes <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(shapes)</span></code></pre></div>
<div id="scatter-plots" class="section level4" number="7.5.2.1">
<h4><span class="header-section-number">7.5.2.1</span> Scatter plots</h4>
<p>The first step is visual inspection using scatter plots.</p>
<div class="sourceCode" id="cb780"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb780-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb780-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(shapes, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-296-1.png" width="672" /></p>
<p>Cluster tendency is typically indicated by several separated point clouds. Often an appropriate number of clusters can also be visually obtained by counting the number of point clouds. We see four clusters, but the mouth is not convex/spherical and thus will pose a problems to algorithms like k-means.</p>
<p>If the data has more than two features then you can use a pairs plot (scatterplot matrix) or look at a scatterplot of the first two principal components using PCA.
#### Visual Analysis for Cluster Tendency Assessment (VAT)</p>
<p>VAT reorders the
objects to show potential clustering tendency as a block structure
(dark blocks along the main diagonal). We scale the data before using Euclidean distance.</p>
<div class="sourceCode" id="cb781"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb781-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb781-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(seriation)</span>
<span id="cb781-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb781-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb781-3"><a href="7-5-advanced-data-preparation-for-clustering.html#cb781-3" aria-hidden="true" tabindex="-1"></a>d_shapes <span class="ot">&lt;-</span> <span class="fu">dist</span>(<span class="fu">scale</span>(shapes))</span>
<span id="cb781-4"><a href="7-5-advanced-data-preparation-for-clustering.html#cb781-4" aria-hidden="true" tabindex="-1"></a><span class="fu">VAT</span>(d_shapes, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-297-1.png" width="672" /></p>
<p>iVAT uses the largest distances for all possible paths between two objects
instead of the direct distances to make the block structure better visible.</p>
<div class="sourceCode" id="cb782"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb782-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb782-1" aria-hidden="true" tabindex="-1"></a><span class="fu">iVAT</span>(d_shapes, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-298-1.png" width="672" /></p>
</div>
<div id="hopkins-statistic" class="section level4" number="7.5.2.2">
<h4><span class="header-section-number">7.5.2.2</span> Hopkins statistic</h4>
<p><code>factoextra</code> can also create a VAT plot and calculate the <a href="https://en.wikipedia.org/wiki/Hopkins_statistic">Hopkins statistic</a> to assess clustering tendency. For the Hopkins statistic, a sample of size <span class="math inline">\(n\)</span> is drawn from the data and then compares the nearest neighbor distribution with a simulated dataset drawn from a random uniform distribution (see <a href="https://www.datanovia.com/en/lessons/assessing-clustering-tendency/#statistical-methods">detailed explanation</a>). A values &gt;.5 indicates usually a clustering tendency.</p>
<div class="sourceCode" id="cb783"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb783-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb783-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_clust_tendency</span>(shapes, <span class="at">n =</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## $hopkins_stat
## [1] 0.907
## 
## $plot</code></pre>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-299-1.png" width="672" /></p>
<p>Both plots show a strong cluster structure with 4 clusters.</p>
</div>
<div id="data-without-clustering-tendency" class="section level4" number="7.5.2.3">
<h4><span class="header-section-number">7.5.2.3</span> Data Without Clustering Tendency</h4>
<div class="sourceCode" id="cb785"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb785-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb785-1" aria-hidden="true" tabindex="-1"></a>data_random <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">500</span>), <span class="at">y =</span> <span class="fu">runif</span>(<span class="dv">500</span>))</span>
<span id="cb785-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb785-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_random, <span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-300-1.png" width="672" /></p>
<p>No point clouds are visible, just noise.</p>
<div class="sourceCode" id="cb786"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb786-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb786-1" aria-hidden="true" tabindex="-1"></a>d_random <span class="ot">&lt;-</span> <span class="fu">dist</span>(data_random)</span>
<span id="cb786-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb786-2" aria-hidden="true" tabindex="-1"></a><span class="fu">VAT</span>(d_random, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-301-1.png" width="672" /></p>
<div class="sourceCode" id="cb787"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb787-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb787-1" aria-hidden="true" tabindex="-1"></a><span class="fu">iVAT</span>(d_random, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-301-2.png" width="672" /></p>
<div class="sourceCode" id="cb788"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb788-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb788-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_clust_tendency</span>(data_random, <span class="at">n =</span> <span class="dv">10</span>, <span class="at">graph =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## $hopkins_stat
## [1] 0.464
## 
## $plot
## NULL</code></pre>
<p>There is very little clustering structure visible indicating low clustering tendency and clustering should not be performed on this data. However, k-means can be used to partition the data into <span class="math inline">\(k\)</span> regions of roughly equivalent size. This can be used as a data-driven discretization of the space.</p>
</div>
<div id="k-means-on-data-without-clustering-tendency" class="section level4" number="7.5.2.4">
<h4><span class="header-section-number">7.5.2.4</span> k-means on Data Without Clustering Tendency</h4>
<p>What happens if we perform k-means on data that has no inherent clustering structure?</p>
<div class="sourceCode" id="cb790"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb790-1"><a href="7-5-advanced-data-preparation-for-clustering.html#cb790-1" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(data_random, <span class="at">centers =</span> <span class="dv">4</span>)</span>
<span id="cb790-2"><a href="7-5-advanced-data-preparation-for-clustering.html#cb790-2" aria-hidden="true" tabindex="-1"></a>random_clustered<span class="ot">&lt;-</span> data_random <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster))</span>
<span id="cb790-3"><a href="7-5-advanced-data-preparation-for-clustering.html#cb790-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(random_clustered, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="R-Code-Companion-for-Introduction-to-Data-Mining_files/figure-html/unnamed-chunk-302-1.png" width="672" /></p>
<p>k-means discretizes the space into similarly sized regions.</p>

</div>
</div>
</div>
<!-- </div> -->
<p style="text-align: center;">
<a href="7-4-external-cluster-validation.html"><button class="btn btn-default">Previous</button></a>
<a href="references.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
