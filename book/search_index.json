[["index.html", "R Companion for the Textbook Introduction to Data Mining Preface License", " R Companion for the Textbook Introduction to Data Mining Michael Hahsler 2021-07-13 Preface This book contains documented R examples to accompany several chapters of the popular data mining textbook Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach, Anuj Karpatne and Vipin Kumar. The companion book can be used with either edition: 1st edition (Tan et al., 2005) or 2nd edition (Tan et al., 2017). The code examples collected in this book were developed for the course CS 7331 - Data Mining taught at SMU since Spring 2013 and will be regularly updated and improved. The latest update includes the use of the popular packages in the meta-package tidyverse (Wickham, 2021c) including ggplot2 (Wickham, Chang, et al., 2021) for data wrangling and visualization along with caret (Kuhn, 2021) for model building. Please use the edit function within this book or visit the book’s GitHub project page to submit corrections or suggest improvements. To cite this book use: Michael Hahsler (2021). R Companion for the Textbook Introduction to Data Mining. Online Book. https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book/ I hope this book helps you to learn to use R more efficiently for your data mining projects. Michael Hahsler License The online version of this book is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. The cover art is based on “rocks” by stebulus licensed with CC BY 2.0. References Kuhn, M. (2021). Caret: Classification and regression training. https://github.com/topepo/caret/ Tan, P.-N., Steinbach, M. S., Karpatne, A., &amp; Kumar, V. (2017). Introduction to data mining (2nd Edition). Pearson. https://www-users.cs.umn.edu/~kumar001/dmbook Tan, P.-N., Steinbach, M. S., &amp; Kumar, V. (2005). Introduction to data mining (1st Edition). Addison-Wesley. https://www-users.cs.umn.edu/~kumar001/dmbook/firsted.php Wickham, H. (2021c). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., &amp; Dunnington, D. (2021). ggplot2: Create elegant data visualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2 "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Packages used for this chapter: ggplot2 (Wickham, Chang, et al., 2021), tidyverse (Wickham, 2021c) This companion book assumes that you have R and RStudio Desktop installed and that you are familiar with the basics of R, how to run R code and install packages. If you are new to R, then working through the official R manual An Introduction to R (Venables et al., 2021) will get you started. There are many introduction videos for RStudio and a basic video that shows you how to run code and how to install packages will suffice. The code in this book uses tidyverse to manipulate data and ggplot2 for visualization. A great introduction to these very useful tools can be found in the freely available web book R for Data Science by Wickham &amp; Grolemund (2017). library(tidyverse) ## ── Attaching packages ────────────── tidyverse 1.3.1 ── ## ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 ## ✓ tibble 3.1.2 ✓ dplyr 1.0.6 ## ✓ tidyr 1.1.3 ✓ stringr 1.4.0 ## ✓ readr 1.4.0 ✓ forcats 0.5.1 ## ── Conflicts ───────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() tidyverse (Wickham, 2021c) is a collection of many very useful packages that work well together by sharing design principles and data structures. tidyverse also includes ggplot2 (Wickham, Chang, et al., 2021) for visualization. In this book, we will use tidyverse tibbles to replace R’s built-in data.frames, the pipe operator %&gt;% to chain functions together, and data transformation functions like filter(), arrange(), select(), group_by(), and mutate() provided by the tidyverse package dplyr. A good introduction can be found in the Section on Data Wrangling (Wickham &amp; Grolemund, 2017), and a very useful reference resource is the RStudio Data Transformation Cheat Sheet. Here is a short example. We create a tibble with the price in dollars per pound and the vitamin C content in milligrams (mg) per pound for three fruit. fruit &lt;- tibble( name = c(&quot;apple&quot;, &quot;banana&quot;, &quot;orange&quot;), price = c(2.5, 2.0, 3.5), vitamin_c = c(20, 45, 250)) fruit ## # A tibble: 3 x 3 ## name price vitamin_c ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 apple 2.5 20 ## 2 banana 2 45 ## 3 orange 3.5 250 Now we add a column with the vitamin C (in mg) that a dollar buys you, filter only fruit that provides more than 20 mg and then order (arrange) the data by the vitamin C per dollar from largest to smallest. affordable_vitamin_c_sources &lt;- fruit %&gt;% mutate(vitamin_c_per_dollar = vitamin_c / price) %&gt;% filter(vitamin_c_per_dollar &gt; 20) %&gt;% arrange(desc(vitamin_c_per_dollar)) affordable_vitamin_c_sources ## # A tibble: 2 x 4 ## name price vitamin_c vitamin_c_per_dollar ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 orange 3.5 250 71.4 ## 2 banana 2 45 22.5 The pipes operator %&gt;% lets you compose a sequence of function calls in a more readable way by passing the value to the left on as the first argument to the function to the right. For visualization, we will use mainly ggplot2. The gg in ggplot2 stands for The Grammar of Graphics introduced in Wilkinson (2005). The main idea is that every graph is built from the same basic components: the data, a coordinate system, and visual marks representing the data (geoms). In ggplot2, the components are combined using the + operator. ggplot(data, mapping = aes(x = ..., y = ..., color = ...)) + geom_point() Since we typically use a Cartesian coordinate system, ggplot uses that by default. Each geom_ function uses a stat_ function to calculate what is visualizes. For example, geom_bar uses stat_count to create a bar chart by counting how often each value appears in the data (see ? geom_bar). geom_point just uses the stat \"identity\" to display the points using the coordinates as they are. A great introduction can be found in the Chapter on Data Visualization (Wickham &amp; Grolemund, 2017), and very useful is RStudio’s Data Visualization Cheat Sheet. We can visualize our fruit data as a scatter plot. ggplot(fruit, aes(x = price, y = vitamin_c)) + geom_point() Alternatively, we can visualize the vitamin C content of each fruit using bars. ggplot(fruit, aes(x = name, y = vitamin_c)) + geom_bar(stat = &quot;identity&quot;) Note that geom_bar by default tries to aggregate data by counting, but we just want to visualize the value already available in the tibble, so we specify the identity statistic. References Venables, W. N., Smith, D. M., &amp; the R Core Team. (2021). An introduction to R. Wickham, H. (2021c). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., &amp; Dunnington, D. (2021). ggplot2: Create elegant data visualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2 Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. https://r4ds.had.co.nz/ Wilkinson, L. (2005). The grammar of graphics (statistics and computing). Springer-Verlag. https://doi.org/10.1007/0-387-28695-0 "],["data.html", "Chapter 2 Data 2.1 The Iris Dataset 2.2 Data Quality 2.3 Aggregation 2.4 Sampling 2.5 Features 2.6 Proximities: Similarities and Distances 2.7 Relationships Between Features 2.8 Density Estimation 2.9 Exploring Data 2.10 Visualization", " Chapter 2 Data This chapter gives examples for cleaning and preparing data for data mining. Packages used for this chapter: arules (Hahsler, Buchta, Gruen, et al., 2021), caret (Kuhn, 2021), factoextra (Kassambara &amp; Mundt, 2020), GGally (Schloerke et al., 2021), plotly (Sievert et al., 2021), proxy (Meyer &amp; Buchta, 2021), sampling (Tillé &amp; Matei, 2021), seriation (Hahsler, Buchta, &amp; Hornik, 2021), tidyverse (Wickham, 2021c) 2.1 The Iris Dataset We will use a toy dataset that comes with R. Fisher’s iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 150 50 flowers from each of 3 species of iris. The species are Iris Setosa, Iris Versicolor, and Iris Virginica. For more details see: ? iris Load the iris data set and convert the data.frame into a tibble. Note: datasets that come with R or R packages can be loaded with data(). library(tidyverse) data(iris) iris &lt;- as_tibble(iris) iris ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## # … with 140 more rows, and 1 more variable: ## # Species &lt;fct&gt; We see that the data contains 150 rows (flowers) and 5 features. tibbles only show the first few rows and do not show all features, if they do not fit the screen width. We can force print to show all features by changing the width. print(iris, width = Inf) ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## 7 4.6 3.4 1.4 0.3 ## 8 5 3.4 1.5 0.2 ## 9 4.4 2.9 1.4 0.2 ## 10 4.9 3.1 1.5 0.1 ## Species ## &lt;fct&gt; ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa ## 7 setosa ## 8 setosa ## 9 setosa ## 10 setosa ## # … with 140 more rows 2.2 Data Quality Assessing the quality of the available data is crucial before we start using the data. Start with summary statistics for each column to identify outliers and missing values. summary(iris) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.00 Min. :1.00 ## 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 ## Median :5.80 Median :3.00 Median :4.35 ## Mean :5.84 Mean :3.06 Mean :3.76 ## 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 ## Max. :7.90 Max. :4.40 Max. :6.90 ## Petal.Width Species ## Min. :0.1 setosa :50 ## 1st Qu.:0.3 versicolor:50 ## Median :1.3 virginica :50 ## Mean :1.2 ## 3rd Qu.:1.8 ## Max. :2.5 You can also summarize specific columns using a statistic function like mean. iris %&gt;% summarize_if(is.numeric, mean) ## # A tibble: 1 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.84 3.06 3.76 1.20 Another way to inspect your data is to use a scatterplot matrix (we use here ggpairs from package GGally). In this plot, you can visually identify noise data points and ouliers (points that are far from the majority of other points). library(GGally) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ggpairs(iris, aes(color = Species)) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. See if you can spot the one red dot that is far away from all others. We need complete data for many data mining methods. To remove missing values (NA) and duplicates (identical data points which might be a mistake in the data), we often do this: clean.data &lt;- iris %&gt;% drop_na() %&gt;% unique() summary(clean.data) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.00 Min. :1.00 ## 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 ## Median :5.80 Median :3.00 Median :4.30 ## Mean :5.84 Mean :3.06 Mean :3.75 ## 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 ## Max. :7.90 Max. :4.40 Max. :6.90 ## Petal.Width Species ## Min. :0.1 setosa :50 ## 1st Qu.:0.3 versicolor:50 ## Median :1.3 virginica :49 ## Mean :1.2 ## 3rd Qu.:1.8 ## Max. :2.5 Note that one case (non-unique) is gone leaving only 149 flowers. The data did not contain missing values, but if it did, they would also have been dropped. Typically, you should spend more time on data cleaning. 2.3 Aggregation Data often contains groups and we want to compare these groups. We group the iris dataset by species and then calculate a summary statistic for each group. iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 ## 2 versicolor 5.94 2.77 4.26 ## 3 virginica 6.59 2.97 5.55 ## # … with 1 more variable: Petal.Width &lt;dbl&gt; iris %&gt;% group_by(Species) %&gt;% summarize_all(median) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5 3.4 1.5 ## 2 versicolor 5.9 2.8 4.35 ## 3 virginica 6.5 3 5.55 ## # … with 1 more variable: Petal.Width &lt;dbl&gt; Using this information, we can compare how features differ between groups. 2.4 Sampling Sampling is often used in data mining to reduce the dataset size. 2.4.1 Random Sampling The built-in sample function can sample from a vector with replacement. sample(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), size = 10, replace = TRUE) ## [1] &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; &quot;C&quot; We often want to sample rows from a dataset. This can be done by sampling without replacement from a vector with row indices (using the functions seq and nrow). The sample is vector is then used to subset the rows of the dataset. take &lt;- sample(seq(nrow(iris)), size = 15) take ## [1] 116 85 29 21 7 38 111 73 146 88 65 5 ## [13] 141 22 112 iris[take, ] ## # A tibble: 15 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6.4 3.2 5.3 2.3 ## 2 5.4 3 4.5 1.5 ## 3 5.2 3.4 1.4 0.2 ## 4 5.4 3.4 1.7 0.2 ## 5 4.6 3.4 1.4 0.3 ## 6 4.9 3.6 1.4 0.1 ## 7 6.5 3.2 5.1 2 ## 8 6.3 2.5 4.9 1.5 ## 9 6.7 3 5.2 2.3 ## 10 6.3 2.3 4.4 1.3 ## 11 5.6 2.9 3.6 1.3 ## 12 5 3.6 1.4 0.2 ## 13 6.7 3.1 5.6 2.4 ## 14 5.1 3.7 1.5 0.4 ## 15 6.4 2.7 5.3 1.9 ## # … with 1 more variable: Species &lt;fct&gt; dplyr from tidyverse lets us sample rows from tibbles directly. I set the random number generator seed to make the results reproducible. set.seed(1000) s &lt;- iris %&gt;% slice_sample(n = 15) ggpairs(s, aes(color = Species)) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. 2.4.2 Stratified Sampling Stratified sampling is a method of sampling from a population which can be partitioned into subpopulations, while controlling the proportions of the subpopulation in the resulting sample. In the following, the subpopulations are the different types of species and we want to make sure to sample the same number (5) flowers from each. The library sampling provides a function for stratified sampling. The column ID_unit in the resulting data.frame contains the row numbers of the sampled rows. We can use slice from dplyr to select the sampled rows. library(sampling) id2 &lt;- strata(iris, stratanames = &quot;Species&quot;, size = c(5,5,5), method = &quot;srswor&quot;) id2 ## Species ID_unit Prob Stratum ## 7 setosa 7 0.1 1 ## 9 setosa 9 0.1 1 ## 10 setosa 10 0.1 1 ## 24 setosa 24 0.1 1 ## 48 setosa 48 0.1 1 ## 58 versicolor 58 0.1 2 ## 62 versicolor 62 0.1 2 ## 74 versicolor 74 0.1 2 ## 78 versicolor 78 0.1 2 ## 99 versicolor 99 0.1 2 ## 106 virginica 106 0.1 3 ## 107 virginica 107 0.1 3 ## 127 virginica 127 0.1 3 ## 135 virginica 135 0.1 3 ## 145 virginica 145 0.1 3 s2 &lt;- iris %&gt;% slice(id2$ID_unit) ggpairs(s2, aes(color = Species)) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. 2.5 Features 2.5.1 Dimensionality Reduction 2.5.1.1 Principal Components Analysis (PCA) PCA calculates principal components (a new orthonormal basis vectors in the data space) from data points such that the first principal component explains the most variability in the data, the second the next most and so on. In data analysis, PCA is used to project high-dimensional data points onto the first few (typically two) principal components for visualization as a scatter plot and as preprocessing for modeling (e.g., before k-means clustering). Points that are closer together in the high-dimensional space, tend also be closer together in the lower-dimensional space, Look at the 3d data using an interactive 3d plot (needs package plotly). However, 3d plots are hard to print out and the iris data is actually in 4 dimensions. ##library(plotly) # I don&#39;t load the package because it&#39;s namespace clashes with select in dplyr. plotly::plot_ly(iris, x = ~Sepal.Length, y = ~Petal.Length, z = ~Sepal.Width, size = ~Petal.Width, color = ~Species, type=&quot;scatter3d&quot;) ## No scatter3d mode specifed: ## Setting the mode to markers ## Read more about this attribute -&gt; https://plotly.com/r/reference/#scatter-mode ## Warning: `line.width` does not currently support ## multiple values. ## Warning: `line.width` does not currently support ## multiple values. ## Warning: `line.width` does not currently support ## multiple values. The principal components can be calculated from a matrix using the function prcomp(). We select all numeric columns (by removing the species column) and convert the data.frame into a matrix before the calculation. pc &lt;- iris %&gt;% select(-Species) %&gt;% as.matrix() %&gt;% prcomp() summary(pc) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 2.056 0.4926 0.2797 0.15439 ## Proportion of Variance 0.925 0.0531 0.0171 0.00521 ## Cumulative Proportion 0.925 0.9777 0.9948 1.00000 How important is each principal component can also be seen using a scree plot. The plot function for the result of the prcomp function visualizes how much variability in the data is explained by each additional principal component. plot(pc, type = &quot;line&quot;) Note that the first principal component (PC1) explains most of the variability in the iris dataset. To find out what information is stored in the object pc, we can inspect the raw object (display structure). str(pc) ## List of 5 ## $ sdev : num [1:4] 2.056 0.493 0.28 0.154 ## $ rotation: num [1:4, 1:4] 0.3614 -0.0845 0.8567 0.3583 -0.6566 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## .. ..$ : chr [1:4] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## $ center : Named num [1:4] 5.84 3.06 3.76 1.2 ## ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Sepal.Length&quot; &quot;Sepal.Width&quot; &quot;Petal.Length&quot; &quot;Petal.Width&quot; ## $ scale : logi FALSE ## $ x : num [1:150, 1:4] -2.68 -2.71 -2.89 -2.75 -2.73 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:4] &quot;PC1&quot; &quot;PC2&quot; &quot;PC3&quot; &quot;PC4&quot; ## - attr(*, &quot;class&quot;)= chr &quot;prcomp&quot; The element x contains the data points projected on the principal components. We can convert the matrix into a tibble and add the species column from the original dataset to display the data projected on the first two principal components. iris_projected &lt;- as_tibble(pc$x) %&gt;% add_column(Species = iris$Species) ggplot(iris_projected, aes(x = PC1, y = PC2, color = Species)) + geom_point() Since the first principal component represents most of the variability, we can also show the data projected only on PC1. ggplot(iris_projected, aes(x = PC1, y = 0, color = Species)) + geom_point() + scale_y_continuous(expand=c(0,0)) + theme(axis.text.y = element_blank(), axis.title.y = element_blank() ) A plot of the projected data with the original axes added as arrows is called a biplot. If the arrows (original axes) align roughly with the axes of the projection, then they are correlated (linearly dependent). library(factoextra) ## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa fviz_pca(pc) We can also display only the old and new axes. fviz_pca_var(pc) We see Petal.Width and Petal.Length point in the same direction which indicates that they are highly correlated. They are also roughly aligned with PC1 (called Dim1 in the plot) which means that PC1 represents most of the variability of these two variables. Sepal.Width parallel to the y-axis and therefore represented by PC2 (Dim2). Petal.Width/Petal.Length and Sepal.Width are almost at 90 degrees, indicating that they are close to uncorrelated. Sepal.Length correlated to all other variables and represented by both, PC1 and PC2 in the projection. Another popular method to project data into lower dimensions for visualization is t-distributed stochastic neighbor embedding (t-SNE) available in package Rtsne. 2.5.1.2 Multi-Dimensional Scaling (MDS) MDS is similar to PCA. Instead of data points, it takes pairwise distances (i.e., a distance matrix) and produces a space where points are placed to represent these distances as well as possible. The axis in this space are called components and are similar to the principal components in PCA. First, we calculate a distance matrix from the 4-d space of the iris dataset. d &lt;- iris %&gt;% select(-Species) %&gt;% dist() Metric (classic) MDS tries to reconstruct a 2-d space from the distance matrix. Points with smaller distances are closer in the reconstructed space. fit &lt;- cmdscale(d, k = 2) colnames(fit) &lt;- c(&quot;comp1&quot;, &quot;comp2&quot;) fit &lt;- as_tibble(fit) %&gt;% add_column(Species = iris$Species) ggplot(fit, aes(x = comp1, y = comp2, color = Species)) + geom_point() The resulting projection is similar (except for rotation and reflection) to the result of PCA. 2.5.1.3 Non-Parametric Multidimensional Scaling Non-parametric multidimensional scaling performs MDS while relaxing the need of linear relationships. Methods are available in package MASS as functions isoMDS and sammon. 2.5.2 Feature Selection Feature selection is the process of identifying the features that are used to create a model. We will talk about feature selection when we discuss classification models in Chapter 3 in Feature Selection and Feature Preparation. 2.5.3 Discretize Features Some data mining methods require discrete data. Discretization converts continuous features into discrete features. Petal.Width is a continuous feature. Before we perform discretization, we should look at the distribution and see if it gives us an idea how we should group the continuous values into a set of discrete values. A histogram visualizes the distribution of a single continuous feature. ggplot(iris, aes(x = Petal.Width)) + geom_histogram(binwidth = .2) The R function cut performs equal interval width discretization. iris %&gt;% pull(Sepal.Width) %&gt;% cut(breaks = 3) ## [1] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [11] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] (3.6,4.4] ## [16] (3.6,4.4] (3.6,4.4] (2.8,3.6] (3.6,4.4] (3.6,4.4] ## [21] (2.8,3.6] (3.6,4.4] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [26] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [31] (2.8,3.6] (2.8,3.6] (3.6,4.4] (3.6,4.4] (2.8,3.6] ## [36] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [41] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] (3.6,4.4] ## [46] (2.8,3.6] (3.6,4.4] (2.8,3.6] (3.6,4.4] (2.8,3.6] ## [51] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8] (2,2.8] ## [56] (2,2.8] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] ## [61] (2,2.8] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] ## [66] (2.8,3.6] (2.8,3.6] (2,2.8] (2,2.8] (2,2.8] ## [71] (2.8,3.6] (2,2.8] (2,2.8] (2,2.8] (2.8,3.6] ## [76] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] (2,2.8] ## [81] (2,2.8] (2,2.8] (2,2.8] (2,2.8] (2.8,3.6] ## [86] (2.8,3.6] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] ## [91] (2,2.8] (2.8,3.6] (2,2.8] (2,2.8] (2,2.8] ## [96] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2,2.8] (2,2.8] ## [101] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [106] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] (2.8,3.6] ## [111] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] (2,2.8] ## [116] (2.8,3.6] (2.8,3.6] (3.6,4.4] (2,2.8] (2,2.8] ## [121] (2.8,3.6] (2,2.8] (2,2.8] (2,2.8] (2.8,3.6] ## [126] (2.8,3.6] (2,2.8] (2.8,3.6] (2,2.8] (2.8,3.6] ## [131] (2,2.8] (3.6,4.4] (2,2.8] (2,2.8] (2,2.8] ## [136] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## [141] (2.8,3.6] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] ## [146] (2.8,3.6] (2,2.8] (2.8,3.6] (2.8,3.6] (2.8,3.6] ## Levels: (2,2.8] (2.8,3.6] (3.6,4.4] Other discretization methods include equal frequency discretization using k-means clustering. These methods are implemented by several R packages. We use here the implementation in package arules and visualize the results as histograms with blue lines to separate intervals assigned to each discrete value. library(arules) ## Loading required package: Matrix ## ## Attaching package: &#39;Matrix&#39; ## The following objects are masked from &#39;package:tidyr&#39;: ## ## expand, pack, unpack ## ## Attaching package: &#39;arules&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode ## The following objects are masked from &#39;package:base&#39;: ## ## abbreviate, write iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;interval&quot;, breaks = 3) ## [1] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [6] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [11] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [16] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [21] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [26] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [31] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [36] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [41] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [46] [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) [0.1,0.9) ## [51] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [56] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [61] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [66] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [71] [1.7,2.5] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [76] [0.9,1.7) [0.9,1.7) [1.7,2.5] [0.9,1.7) [0.9,1.7) ## [81] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [86] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [91] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [96] [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) [0.9,1.7) ## [101] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [106] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [111] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [116] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) ## [121] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [126] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) ## [131] [1.7,2.5] [1.7,2.5] [1.7,2.5] [0.9,1.7) [0.9,1.7) ## [136] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [141] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## [146] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] [1.7,2.5] ## attr(,&quot;discretized:breaks&quot;) ## [1] 0.1 0.9 1.7 2.5 ## attr(,&quot;discretized:method&quot;) ## [1] interval ## Levels: [0.1,0.9) [0.9,1.7) [1.7,2.5] iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;frequency&quot;, breaks = 3) ## [1] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [5] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [9] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [13] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [17] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [21] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [25] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [29] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [33] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [37] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [41] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [45] [0.1,0.867) [0.1,0.867) [0.1,0.867) [0.1,0.867) ## [49] [0.1,0.867) [0.1,0.867) [0.867,1.6) [0.867,1.6) ## [53] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [57] [1.6,2.5] [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [61] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [65] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [69] [0.867,1.6) [0.867,1.6) [1.6,2.5] [0.867,1.6) ## [73] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [77] [0.867,1.6) [1.6,2.5] [0.867,1.6) [0.867,1.6) ## [81] [0.867,1.6) [0.867,1.6) [0.867,1.6) [1.6,2.5] ## [85] [0.867,1.6) [1.6,2.5] [0.867,1.6) [0.867,1.6) ## [89] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [93] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [97] [0.867,1.6) [0.867,1.6) [0.867,1.6) [0.867,1.6) ## [101] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [105] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [109] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [113] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [117] [1.6,2.5] [1.6,2.5] [1.6,2.5] [0.867,1.6) ## [121] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [125] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [129] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [133] [1.6,2.5] [0.867,1.6) [0.867,1.6) [1.6,2.5] ## [137] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [141] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [145] [1.6,2.5] [1.6,2.5] [1.6,2.5] [1.6,2.5] ## [149] [1.6,2.5] [1.6,2.5] ## attr(,&quot;discretized:breaks&quot;) ## [1] 0.100 0.867 1.600 2.500 ## attr(,&quot;discretized:method&quot;) ## [1] frequency ## Levels: [0.1,0.867) [0.867,1.6) [1.6,2.5] iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;cluster&quot;, breaks = 3) ## [1] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [4] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [7] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [10] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [13] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [16] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [19] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [22] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [25] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [28] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [31] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [34] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [37] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [40] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [43] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [46] [0.1,0.792) [0.1,0.792) [0.1,0.792) ## [49] [0.1,0.792) [0.1,0.792) [0.792,1.71) ## [52] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [55] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [58] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [61] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [64] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [67] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [70] [0.792,1.71) [1.71,2.5] [0.792,1.71) ## [73] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [76] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [79] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [82] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [85] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [88] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [91] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [94] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [97] [0.792,1.71) [0.792,1.71) [0.792,1.71) ## [100] [0.792,1.71) [1.71,2.5] [1.71,2.5] ## [103] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [106] [1.71,2.5] [0.792,1.71) [1.71,2.5] ## [109] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [112] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [115] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [118] [1.71,2.5] [1.71,2.5] [0.792,1.71) ## [121] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [124] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [127] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [130] [0.792,1.71) [1.71,2.5] [1.71,2.5] ## [133] [1.71,2.5] [0.792,1.71) [0.792,1.71) ## [136] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [139] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [142] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [145] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## [148] [1.71,2.5] [1.71,2.5] [1.71,2.5] ## attr(,&quot;discretized:breaks&quot;) ## [1] 0.100 0.792 1.705 2.500 ## attr(,&quot;discretized:method&quot;) ## [1] cluster ## Levels: [0.1,0.792) [0.792,1.71) [1.71,2.5] ggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) + geom_vline(xintercept = iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;interval&quot;, breaks = 3, onlycuts = TRUE), color = &quot;blue&quot;) + labs(title = &quot;Discretization: interval&quot;, subtitle = &quot;Blue lines are boundaries&quot;) ggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) + geom_vline(xintercept = iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;frequency&quot;, breaks = 3, onlycuts = TRUE), color = &quot;blue&quot;) + labs(title = &quot;Discretization: frequency&quot;, subtitle = &quot;Blue lines are boundaries&quot;) ggplot(iris, aes(Petal.Width)) + geom_histogram(binwidth = .2) + geom_vline(xintercept = iris %&gt;% pull(Petal.Width) %&gt;% discretize(method = &quot;cluster&quot;, breaks = 3, onlycuts = TRUE), color = &quot;blue&quot;) + labs(title = &quot;Discretization: cluster&quot;, subtitle = &quot;Blue lines are boundaries&quot;) 2.5.4 Standardize Data Standardizing (scaling, normalizing) the range of features values to make them comparable. The most popular method is to convert the values of each feature to z-scores. by subtracting the mean (centering) and dividing by the standard deviation (scaling). Note: tidyverse currently does not have a simple scale function, so I make one that provides a wrapper for the standard scale function in R: scale_numeric &lt;- function(x) x %&gt;% mutate_if(is.numeric, function(y) as.vector(scale(y))) iris.scaled &lt;- iris %&gt;% scale_numeric() iris.scaled ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.898 1.02 -1.34 -1.31 ## 2 -1.14 -0.132 -1.34 -1.31 ## 3 -1.38 0.327 -1.39 -1.31 ## 4 -1.50 0.0979 -1.28 -1.31 ## 5 -1.02 1.25 -1.34 -1.31 ## 6 -0.535 1.93 -1.17 -1.05 ## 7 -1.50 0.786 -1.34 -1.18 ## 8 -1.02 0.786 -1.28 -1.31 ## 9 -1.74 -0.361 -1.34 -1.31 ## 10 -1.14 0.0979 -1.28 -1.44 ## # … with 140 more rows, and 1 more variable: ## # Species &lt;fct&gt; summary(iris.scaled) ## Sepal.Length Sepal.Width Petal.Length ## Min. :-1.864 Min. :-2.426 Min. :-1.562 ## 1st Qu.:-0.898 1st Qu.:-0.590 1st Qu.:-1.222 ## Median :-0.052 Median :-0.132 Median : 0.335 ## Mean : 0.000 Mean : 0.000 Mean : 0.000 ## 3rd Qu.: 0.672 3rd Qu.: 0.557 3rd Qu.: 0.760 ## Max. : 2.484 Max. : 3.080 Max. : 1.780 ## Petal.Width Species ## Min. :-1.442 setosa :50 ## 1st Qu.:-1.180 versicolor:50 ## Median : 0.132 virginica :50 ## Mean : 0.000 ## 3rd Qu.: 0.788 ## Max. : 1.706 The standardized feature has a mean of zero and is measured in standard deviations away from zero. Most “normal” values will fall in the range [-3,3] (standard deviations). 2.6 Proximities: Similarities and Distances R stores proximity as dissimilarities/distances matrices. Similarities are first converted to dissimilarities. Distances are symmetric, i.e., the distance from A to B is the same as the distance from B to A. R therefore stores only a triangle (typically the lower triangle) of the distance matrix. 2.6.1 Minkowsky Distances The Minkowsky distance is a family of metric distances including Euclidean and Manhattan distance. To avoid one feature to dominate the distance calculation, scaled data is typically used. iris_sample &lt;- iris.scaled %&gt;% select(-Species) %&gt;% slice(1:5) iris_sample ## # A tibble: 5 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.898 1.02 -1.34 -1.31 ## 2 -1.14 -0.132 -1.34 -1.31 ## 3 -1.38 0.327 -1.39 -1.31 ## 4 -1.50 0.0979 -1.28 -1.31 ## 5 -1.02 1.25 -1.34 -1.31 Calculate distances matrices between the first 5 flowers. dist(iris_sample, method=&quot;euclidean&quot;) ## 1 2 3 4 ## 2 1.172 ## 3 0.843 0.522 ## 4 1.100 0.433 0.283 ## 5 0.259 1.382 0.988 1.246 dist(iris_sample, method=&quot;manhattan&quot;) ## 1 2 3 4 ## 2 1.389 ## 3 1.228 0.757 ## 4 1.578 0.648 0.463 ## 5 0.350 1.497 1.337 1.687 dist(iris_sample, method=&quot;maximum&quot;) ## 1 2 3 4 ## 2 1.147 ## 3 0.688 0.459 ## 4 0.918 0.362 0.229 ## 5 0.229 1.377 0.918 1.147 We see that only the lower triangle of the distance matrices are stored (note that rows start with row 2). 2.6.2 Distances for Binary Data Binary data can be encodes as 0 and 1 (numeric) or TRUE and FALSE (logical). b &lt;- rbind( c(0,0,0,1,1,1,1,0,0,1), c(0,0,1,1,1,0,0,1,0,0) ) b ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] ## [1,] 0 0 0 1 1 1 1 0 0 ## [2,] 0 0 1 1 1 0 0 1 0 ## [,10] ## [1,] 1 ## [2,] 0 b_logical &lt;- apply(b, MARGIN = 2, as.logical) b_logical ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] FALSE FALSE FALSE TRUE TRUE TRUE TRUE FALSE ## [2,] FALSE FALSE TRUE TRUE TRUE FALSE FALSE TRUE ## [,9] [,10] ## [1,] FALSE TRUE ## [2,] FALSE FALSE 2.6.2.1 Hamming Distance The Hamming distance is the number of mis-matches between two binary vectors. For 0-1 data this is equivalent to the Manhattan distance and also the squared Euclidean distance. dist(b, method = &quot;manhattan&quot;) ## 1 ## 2 5 dist(b, method = &quot;euclidean&quot;)^2 ## 1 ## 2 5 2.6.2.2 Jaccard Index The Jaccard index is a similarity measure that focuses on matching 1s. R converts the similarity into a dissimilarity using \\(d_{J} = 1 - s_{J}\\). dist(b, method = &quot;binary&quot;) ## 1 ## 2 0.714 2.6.3 Distances for Mixed Data Most distance measures work only on numeric data. Often we have a mixture of numbers and nominal features like this data: people &lt;- tibble( height = c( 160, 185, 170), weight = c( 52, 90, 75), sex = c( &quot;female&quot;, &quot;male&quot;, &quot;male&quot;) ) people ## # A tibble: 3 x 3 ## height weight sex ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 160 52 female ## 2 185 90 male ## 3 170 75 male Note: Nominal features need to be a factor and not character (&lt;chr&gt;). people &lt;- people %&gt;% mutate_if(is.character, factor) people ## # A tibble: 3 x 3 ## height weight sex ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 160 52 female ## 2 185 90 male ## 3 170 75 male 2.6.3.1 Gower’s Coefficient Gower’s coefficient of similarity works with mixed data by calculating the appropriate similarity for each feature and then aggregating them into a single measure. The package proxy implements Gower’s coefficient converted into a distance. library(proxy) ## ## Attaching package: &#39;proxy&#39; ## The following object is masked from &#39;package:Matrix&#39;: ## ## as.matrix ## The following objects are masked from &#39;package:stats&#39;: ## ## as.dist, dist ## The following object is masked from &#39;package:base&#39;: ## ## as.matrix d_Gower &lt;- dist(people, method = &quot;Gower&quot;) d_Gower ## 1 2 ## 2 1.000 ## 3 0.668 0.332 Gower’s coefficient calculation implicitly scales the data because it calculates distances on each feature individualy, so no need to scale the data first. 2.6.3.2 Using Euclidean Distance with Mixed Data Sometimes methods (e.g., k-means) can only use Euclidean distance. In this case, nominal features can be converted into 0-1 dummy variables. After scaling, Euclidean distance will result in a usable distance measure. We use package caret to create dummy variables. library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:sampling&#39;: ## ## cluster ## The following object is masked from &#39;package:purrr&#39;: ## ## lift data_dummy &lt;- dummyVars(~., people) %&gt;% predict(people) data_dummy ## height weight sex.female sex.male ## 1 160 52 1 0 ## 2 185 90 0 1 ## 3 170 75 0 1 Note that feature sex has now two columns. If we want to height, weight and sex to have the same influence on the distance measure, then we need to weight the sex columns by 1/2 after scaling. weight &lt;- matrix(c(1, 1, 1/2, 1/2), ncol = 4, nrow = nrow(data_dummy), byrow = TRUE) data_dummy_scaled &lt;- scale(data_dummy) * weight d_dummy &lt;- dist(data_dummy_scaled) d_dummy ## 1 2 ## 2 3.06 ## 3 1.89 1.43 The distance using dummy variables is (mostly) consistent with Gower’s distance (other than that Gower’s distance is scaled between 0 and 1). ggplot(tibble(d_dummy, d_Gower), aes(x = d_dummy, y = d_Gower)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## Don&#39;t know how to automatically pick scale for object of type dist. Defaulting to continuous. ## Don&#39;t know how to automatically pick scale for object of type dist. Defaulting to continuous. ## `geom_smooth()` using formula &#39;y ~ x&#39; 2.6.4 Additional proximity Measures Available in Package proxy The package proxy implements a wide array of distances. library(proxy) pr_DB$get_entry_names() ## [1] &quot;Jaccard&quot; &quot;Kulczynski1&quot; ## [3] &quot;Kulczynski2&quot; &quot;Mountford&quot; ## [5] &quot;Fager&quot; &quot;Russel&quot; ## [7] &quot;simple matching&quot; &quot;Hamman&quot; ## [9] &quot;Faith&quot; &quot;Tanimoto&quot; ## [11] &quot;Dice&quot; &quot;Phi&quot; ## [13] &quot;Stiles&quot; &quot;Michael&quot; ## [15] &quot;Mozley&quot; &quot;Yule&quot; ## [17] &quot;Yule2&quot; &quot;Ochiai&quot; ## [19] &quot;Simpson&quot; &quot;Braun-Blanquet&quot; ## [21] &quot;cosine&quot; &quot;eJaccard&quot; ## [23] &quot;eDice&quot; &quot;correlation&quot; ## [25] &quot;Chi-squared&quot; &quot;Phi-squared&quot; ## [27] &quot;Tschuprow&quot; &quot;Cramer&quot; ## [29] &quot;Pearson&quot; &quot;Gower&quot; ## [31] &quot;Euclidean&quot; &quot;Mahalanobis&quot; ## [33] &quot;Bhjattacharyya&quot; &quot;Manhattan&quot; ## [35] &quot;supremum&quot; &quot;Minkowski&quot; ## [37] &quot;Canberra&quot; &quot;Wave&quot; ## [39] &quot;divergence&quot; &quot;Kullback&quot; ## [41] &quot;Bray&quot; &quot;Soergel&quot; ## [43] &quot;Levenshtein&quot; &quot;Podani&quot; ## [45] &quot;Chord&quot; &quot;Geodesic&quot; ## [47] &quot;Whittaker&quot; &quot;Hellinger&quot; ## [49] &quot;fJaccard&quot; Note that loading the package proxy replaces the dist function in R. You can specify which dist function to use by specifying the package in the call. For example stats::dist() calls the default function in R (the package stats is part of R) while proxy::dist() calls the version in the package proxy. 2.7 Relationships Between Features 2.7.1 Correlation Correlation can be used for ratio/interval scaled features. We typically think of the Pearson correlation coefficient between features (columns). cc &lt;- iris %&gt;% select(-Species) %&gt;% cor() cc ## Sepal.Length Sepal.Width Petal.Length ## Sepal.Length 1.000 -0.118 0.872 ## Sepal.Width -0.118 1.000 -0.428 ## Petal.Length 0.872 -0.428 1.000 ## Petal.Width 0.818 -0.366 0.963 ## Petal.Width ## Sepal.Length 0.818 ## Sepal.Width -0.366 ## Petal.Length 0.963 ## Petal.Width 1.000 cor calculates a correlation matrix with pairwise correlations between features. The correlation between Petal.Length and Petal.Width can be visualized using a scatter plot. ggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; geom_smooth adds a regression line by fitting a linear model (lm). Most points are close to this line indicating strong linear dependence (i.e., correlation). We can calculate individual correlations by specifying two vectors. with(iris, cor(Petal.Length, Petal.Width)) ## [1] 0.963 Note: with lets you use columns using just their names and with(iris, cor(Petal.Length, Petal.Width)) is the same as cor(iris$Petal.Length, iris$Petal.Width). Finally, we can test if the correlation is significantly different from zero. with(iris, cor.test(Petal.Length, Petal.Width)) ## ## Pearson&#39;s product-moment correlation ## ## data: Petal.Length and Petal.Width ## t = 43, df = 148, p-value &lt;2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.949 0.973 ## sample estimates: ## cor ## 0.963 A small p-value (less than 0.05) indicates that the observed correlation is significantly different from zero. This can also be seen by the fact that the 95% confidence interval does not span zero. Sepal.Length and Sepal.Width show little correlation: { r} ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_point() + geom_smooth(method = \"lm\") with(iris, cor(Sepal.Length, Sepal.Width)) with(iris, cor.test(Sepal.Length, Sepal.Width)) 2.7.2 Rank Correlation Rank correlation is used for ordinal features. To show this, we first convert the continuous features in the Iris dataset into ordered factors (ordinal) with three levels using the function cut. iris_ord &lt;- iris %&gt;% mutate_if(is.numeric, function(x) cut(x, 3, labels = c(&quot;short&quot;, &quot;medium&quot;, &quot;long&quot;), ordered = TRUE)) iris_ord ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; ## 1 short medium short short ## 2 short medium short short ## 3 short medium short short ## 4 short medium short short ## 5 short medium short short ## 6 short long short short ## 7 short medium short short ## 8 short medium short short ## 9 short medium short short ## 10 short medium short short ## # … with 140 more rows, and 1 more variable: ## # Species &lt;fct&gt; summary(iris_ord) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## short :59 short :47 short :50 short :50 ## medium:71 medium:88 medium:54 medium:54 ## long :20 long :15 long :46 long :46 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 iris_ord %&gt;% pull(Sepal.Length) ## [1] short short short short short short short ## [8] short short short short short short short ## [15] medium medium short short medium short short ## [22] short short short short short short short ## [29] short short short short short short short ## [36] short short short short short short short ## [43] short short short short short short short ## [50] short long medium long short medium medium ## [57] medium short medium short short medium medium ## [64] medium medium medium medium medium medium medium ## [71] medium medium medium medium medium medium long ## [78] medium medium medium short short medium medium ## [85] short medium medium medium medium short short ## [92] medium medium short medium medium medium medium ## [99] short medium medium medium long medium medium ## [106] long short long medium long medium medium ## [113] long medium medium medium medium long long ## [120] medium long medium long medium medium long ## [127] medium medium medium long long long medium ## [134] medium medium long medium medium medium long ## [141] medium long medium long medium medium medium ## [148] medium medium medium ## Levels: short &lt; medium &lt; long Two measures for rank correlation are Kendall’s Tau and Spearman’s Rho. Kendall’s Tau Rank Correlation Coefficient measures the agreement between two rankings (i.e., ordinal features). iris_ord %&gt;% select(-Species) %&gt;% sapply(xtfrm) %&gt;% cor(method = &quot;kendall&quot;) ## Sepal.Length Sepal.Width Petal.Length ## Sepal.Length 1.000 -0.144 0.742 ## Sepal.Width -0.144 1.000 -0.330 ## Petal.Length 0.742 -0.330 1.000 ## Petal.Width 0.730 -0.315 0.920 ## Petal.Width ## Sepal.Length 0.730 ## Sepal.Width -0.315 ## Petal.Length 0.920 ## Petal.Width 1.000 Note: We have to use xtfrm to transform the ordered factors into ranks, i.e., numbers representing the order. Spearman’s Rho is equal to the Pearson correlation between the rank values of those two features. iris_ord %&gt;% select(-Species) %&gt;% sapply(xtfrm) %&gt;% cor(method = &quot;spearman&quot;) ## Sepal.Length Sepal.Width Petal.Length ## Sepal.Length 1.000 -0.157 0.794 ## Sepal.Width -0.157 1.000 -0.366 ## Petal.Length 0.794 -0.366 1.000 ## Petal.Width 0.784 -0.352 0.940 ## Petal.Width ## Sepal.Length 0.784 ## Sepal.Width -0.352 ## Petal.Length 0.940 ## Petal.Width 1.000 Spearman’s Rho is much faster to compute on large datasets then Kendall’s Tau. Comparing the rank correlation results with the Pearson correlation on the original data shows that they are very similar. This indicates that discretizing data does not result in the loss of too much information. iris %&gt;% select(-Species) %&gt;% cor() ## Sepal.Length Sepal.Width Petal.Length ## Sepal.Length 1.000 -0.118 0.872 ## Sepal.Width -0.118 1.000 -0.428 ## Petal.Length 0.872 -0.428 1.000 ## Petal.Width 0.818 -0.366 0.963 ## Petal.Width ## Sepal.Length 0.818 ## Sepal.Width -0.366 ## Petal.Length 0.963 ## Petal.Width 1.000 2.8 Density Estimation Density estimation constructions an estimate of an unobservable probability density function (a distribution) based on observed data. Just plotting the data is not very helpful for a single feature. ggplot(iris, aes(x = Petal.Length, y = 0)) + geom_point() 2.8.1 Histograms A histograms shows more about the distribution by counting how many values fall within a bucket and visualizing the counts as a bar chart. We use geom_rug to place marks for the original data points at the bottom of the histogram. ggplot(iris, aes(x = Petal.Length)) + geom_histogram() + geom_rug(alpha = 1/2) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. Two-dimensional distributions can be visualized using 2-d binning or hexagonal bins. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_bin2d(bins = 10) + geom_jitter(color = &quot;red&quot;) ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_hex(bins = 10) + geom_jitter(color = &quot;red&quot;) 2.8.2 Kernel Density Estimate (KDE) Kernel density estimation is used to estimate the probability density function of a feature. It works by replacing each value with a kernel function (often a Gaussian) and then adding them up. The result is an estimated probability density function that looks like a smoothed version of the histogram. The bandwidth (bw) of the kernel controls the amount of smoothing. ggplot(iris, aes(Petal.Length)) + geom_density(bw = .2) + geom_rug(alpha = 1/2) Kernel density estimates can also be done in two dimensions. The lines are called contour lines and indicate density similar to elevation lines on maps. ggplot(iris, aes(Sepal.Length, Sepal.Width)) + geom_density2d() + geom_jitter() 2.9 Exploring Data 2.9.1 Basic statistics Get summary statistics (using base R) summary(iris) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.00 Min. :1.00 ## 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 ## Median :5.80 Median :3.00 Median :4.35 ## Mean :5.84 Mean :3.06 Mean :3.76 ## 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 ## Max. :7.90 Max. :4.40 Max. :6.90 ## Petal.Width Species ## Min. :0.1 setosa :50 ## 1st Qu.:0.3 versicolor:50 ## Median :1.3 virginica :50 ## Mean :1.2 ## 3rd Qu.:1.8 ## Max. :2.5 Get mean and standard deviation for sepal length iris %&gt;% pull(Sepal.Length) %&gt;% mean() ## [1] 5.84 iris %&gt;% pull(Sepal.Length) %&gt;% sd() ## [1] 0.828 Ignore missing values (Note: this data does not contain any, but this is what you would do) iris %&gt;% pull(Sepal.Length) %&gt;% mean(na.rm = TRUE) ## [1] 5.84 Robust mean (trim 10% of observations from each end of the distribution) iris %&gt;% pull(Sepal.Length) %&gt;% mean(trim = .1) ## [1] 5.81 Calculate a summary for all numeric columns iris %&gt;% summarize_if(is.numeric, mean) ## # A tibble: 1 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 5.84 3.06 3.76 1.20 iris %&gt;% summarize_if(is.numeric, sd) ## # A tibble: 1 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.828 0.436 1.77 0.762 iris %&gt;% summarize_if(is.numeric, list(min = min, median = median, max = max)) ## # A tibble: 1 x 12 ## Sepal.Length_min Sepal.Width_min Petal.Length_min ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.3 2 1 ## # … with 9 more variables: Petal.Width_min &lt;dbl&gt;, ## # Sepal.Length_median &lt;dbl&gt;, ## # Sepal.Width_median &lt;dbl&gt;, ## # Petal.Length_median &lt;dbl&gt;, ## # Petal.Width_median &lt;dbl&gt;, Sepal.Length_max &lt;dbl&gt;, ## # Sepal.Width_max &lt;dbl&gt;, Petal.Length_max &lt;dbl&gt;, ## # Petal.Width_max &lt;dbl&gt; MAD (median absolute deviation) iris %&gt;% summarize_if(is.numeric, mad) ## # A tibble: 1 x 4 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1.04 0.445 1.85 1.04 2.9.2 Tabulate data Count the number of flowers for each species. iris %&gt;% group_by(Species) %&gt;% summarize(n()) ## # A tibble: 3 x 2 ## Species `n()` ## &lt;fct&gt; &lt;int&gt; ## 1 setosa 50 ## 2 versicolor 50 ## 3 virginica 50 In base R, this can be also done using count(iris$Species). We discretize the data using cut. iris_ord &lt;- iris %&gt;% mutate_if(is.numeric, function(x) cut(x, 3, labels = c(&quot;short&quot;, &quot;medium&quot;, &quot;long&quot;), ordered = TRUE)) iris_ord ## # A tibble: 150 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; ## 1 short medium short short ## 2 short medium short short ## 3 short medium short short ## 4 short medium short short ## 5 short medium short short ## 6 short long short short ## 7 short medium short short ## 8 short medium short short ## 9 short medium short short ## 10 short medium short short ## # … with 140 more rows, and 1 more variable: ## # Species &lt;fct&gt; summary(iris_ord) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## short :59 short :47 short :50 short :50 ## medium:71 medium:88 medium:54 medium:54 ## long :20 long :15 long :46 long :46 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 Cross tabulation is used to find out if two discrete features are related. tbl &lt;- iris_ord %&gt;% select(Sepal.Length, Species) %&gt;% table() tbl ## Species ## Sepal.Length setosa versicolor virginica ## short 47 11 1 ## medium 3 36 32 ## long 0 3 17 The table contains the number of rows that contain the combination of values (e.g., the number of flowers with a short Sepal.Length and species setosa is 47). If a few cells have very large counts and most others have very low counts, then there might be a relationship. For the iris data, we see that species setosa has mostly a short Sepal.Length, while versicolor and virginica have longer sepals. Creating a cross table with tidyverse is a little more involved and uses pivot operations and grouping. iris_ord %&gt;% select(Species, Sepal.Length) %&gt;% ### Relationship Between Nominal and Ordinal Features pivot_longer(cols = Sepal.Length) %&gt;% group_by(Species, value) %&gt;% count() %&gt;% ungroup() %&gt;% pivot_wider(names_from = Species, values_from = n) ## # A tibble: 3 x 4 ## value setosa versicolor virginica ## &lt;ord&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 short 47 11 1 ## 2 medium 3 36 32 ## 3 long NA 3 17 We can use a statistical test to determine if there is a significant relationship between the two features. Pearson’s chi-squared test for independence is performed with the null hypothesis that the joint distribution of the cell counts in a 2-dimensional contingency table is the product of the row and column marginals. The null hypothesis h0 is independence between rows and columns. tbl %&gt;% chisq.test() ## ## Pearson&#39;s Chi-squared test ## ## data: . ## X-squared = 112, df = 4, p-value &lt;2e-16 The small p-value indicates that the null hypothesis of independence needs to be rejected. For small counts (cells with counts &lt;5), Fisher’s exact test is better. fisher.test(tbl) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: tbl ## p-value &lt;2e-16 ## alternative hypothesis: two.sided We can use the nominal feature to form groups and then calculate group-wise statistics for the continuous features. We often use group-wise averages to see if they differ between groups. iris %&gt;% group_by(Species) %&gt;% summarize(across(Sepal.Length, mean)) ## # A tibble: 3 x 2 ## Species Sepal.Length ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 iris %&gt;% group_by(Species) %&gt;% summarize_all(mean) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 1.46 ## 2 versicolor 5.94 2.77 4.26 ## 3 virginica 6.59 2.97 5.55 ## # … with 1 more variable: Petal.Width &lt;dbl&gt; We see that the species viginica has the highest average for all, but Sepal.Width. 2.9.3 Percentiles (Quantiles) Quantiles are cutting points dividing the range of a probability distribution into continuous intervals with equal probability. For example, the median is the empirical 50% quantile dividing the observations into 50% of the observations being smaller than the median and the other 50% being larger than the median. By default quartiles are calculated. 25% is typically called Q1, 50% is called Q2 or the median and 75% is called Q3. iris %&gt;% pull(Petal.Length) %&gt;% quantile() ## 0% 25% 50% 75% 100% ## 1.00 1.60 4.35 5.10 6.90 The interquartile range is a measure for variability that is robust against outliers. It is defined the length Q3 - Q2 which covers the 50% of the data in the middle. iris %&gt;% summarize(IQR = quantile(Petal.Length, probs = 0.75) - quantile(Petal.Length, probs = 0.25)) ## # A tibble: 1 x 1 ## IQR ## &lt;dbl&gt; ## 1 3.5 2.10 Visualization 2.10.1 Histogram Histograms show the distribution of a single continuous feature. ggplot(iris, aes(Petal.Width)) + geom_histogram(bins = 20) 2.10.2 Boxplot Boxplots are used to compare the distribution of a feature between different groups. The horizontal line in the middle of the boxes are the group-wise medians, the boxes span the interquartile range. The whiskers (vertical lines) span typically 1.4 times the interquartile range. Points that fall outside that range are typically outliers shown as dots. ggplot(iris, aes(Species, Sepal.Length)) + geom_boxplot() The group-wise medians can also be calculated directly. iris %&gt;% group_by(Species) %&gt;% summarize_if(is.numeric, median) ## # A tibble: 3 x 5 ## Species Sepal.Length Sepal.Width Petal.Length ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5 3.4 1.5 ## 2 versicolor 5.9 2.8 4.35 ## 3 virginica 6.5 3 5.55 ## # … with 1 more variable: Petal.Width &lt;dbl&gt; To compare the distribution of the four features using a ggplot boxplot, we first have to transform the data into long format (i.e., all feature values are combined into a single column). library(tidyr) iris_long &lt;- iris %&gt;% mutate(id = row_number()) %&gt;% pivot_longer(1:4) ggplot(iris_long, aes(name, value)) + geom_boxplot() This visualization is only useful if all features have roughly the same range. 2.10.3 Scatter plot Scatter plots show the relationship between two continuous features. ggplot(iris, aes(x = Petal.Length, y = Petal.Width, color = Species)) + geom_point() 2.10.4 Scatter Plot Matrix A scatter plot matrix show the relationship between several features library(&quot;GGally&quot;) ggpairs(iris, aes(color = Species)) ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. The implementation in package GGally also shows additional plots (histograms, density estimates and box plots) and correlation coefficients. 2.10.5 Data Matrix Visualization Matrix visualization shows the values in the matrix using a color scale. iris_matrix &lt;- iris %&gt;% select(-Species) %&gt;% as.matrix() We need the long format for tidyverse. iris_long &lt;- as_tibble(iris_matrix) %&gt;% mutate(id = row_number()) %&gt;% pivot_longer(1:4) head(iris_long) ## # A tibble: 6 x 3 ## id name value ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 Sepal.Length 5.1 ## 2 1 Sepal.Width 3.5 ## 3 1 Petal.Length 1.4 ## 4 1 Petal.Width 0.2 ## 5 2 Sepal.Length 4.9 ## 6 2 Sepal.Width 3 ggplot(iris_long, aes(x = name, y = id, fill = value)) + geom_tile() Smaller values are darker. Package seriation provides a simpler plotting function. library(seriation) ## Registered S3 methods overwritten by &#39;registry&#39;: ## method from ## print.registry_field proxy ## print.registry_entry proxy ## ## Attaching package: &#39;seriation&#39; ## The following object is masked from &#39;package:lattice&#39;: ## ## panel.lines ggpimage(iris_matrix, prop = FALSE) We can scale the features to z-scores to make them better comparable. iris_scaled &lt;- scale(iris_matrix) ggpimage(iris_scaled, prop = FALSE) This reveals red and blue blocks. Each row is a flower and the flowers in the Iris dataset are sorted by species. The blue blocks for the top 50 flowers shiw that these flowers are smaller than average for all but Speal.Width and the red blocks show that the bottom 50 flowers are larger for most features. Often, reordering data matrices help with visualization. A reordering technique is called seriation. Ir reorders rows and columns to place more similar points closer together. ggpimage(iris_scaled, order = seriate(iris_scaled), prop = FALSE) We see that the rows (flowers) are organized from very blue to very red and the features are reordered to move Sepal.Width all the way to the right because it is very different from the other features. 2.10.6 Correlation Matrix A correlation matrix contains the correlation between features. cm1 &lt;- iris %&gt;% select(-Species) %&gt;% as.matrix %&gt;% cor() cm1 ## Sepal.Length Sepal.Width Petal.Length ## Sepal.Length 1.000 -0.118 0.872 ## Sepal.Width -0.118 1.000 -0.428 ## Petal.Length 0.872 -0.428 1.000 ## Petal.Width 0.818 -0.366 0.963 ## Petal.Width ## Sepal.Length 0.818 ## Sepal.Width -0.366 ## Petal.Length 0.963 ## Petal.Width 1.000 Package ggcorrplot provides a visualization for correlation matrices. library(ggcorrplot) ggcorrplot(cm1) Package seriation provides a reordered version for this plot using a heatmap. gghmap(cm1) Correlations can also be calculates between objects by trnsposing the data matrix. cm2 &lt;- iris %&gt;% select(-Species) %&gt;% as.matrix() %&gt;% t() %&gt;% cor() ggcorrplot(cm2) Object-to-object correlations can be used as a measure of similarity. The dark red blocks indicate different species. 2.10.7 Parallel Coordinates Plot Parallel coordinate plots can visualize several featues in a single plot. Lines connect the values for each object (flower). library(GGally) ggparcoord(as_tibble(iris), columns = 1:4, groupColumn = 5) The plot can be improved by reordering the variables to place correlated features next to each other. o &lt;- seriate(as.dist(1-cor(iris[,1:4])), method = &quot;BBURCG&quot;) get_order(o) ## Petal.Length Petal.Width Sepal.Length Sepal.Width ## 3 4 1 2 ggparcoord(as_tibble(iris), columns = get_order(o), groupColumn = 5) 2.10.8 More Visulizations A well organized collection of visualizations with code can be found at The R Graph Gallery. References Hahsler, M., Buchta, C., Gruen, B., &amp; Hornik, K. (2021). Arules: Mining association rules and frequent itemsets. https://github.com/mhahsler/arules Hahsler, M., Buchta, C., &amp; Hornik, K. (2021). Seriation: Infrastructure for ordering objects using seriation. https://github.com/mhahsler/seriation Kassambara, A., &amp; Mundt, F. (2020). Factoextra: Extract and visualize the results of multivariate data analyses. http://www.sthda.com/english/rpkgs/factoextra Kuhn, M. (2021). Caret: Classification and regression training. https://github.com/topepo/caret/ Meyer, D., &amp; Buchta, C. (2021). Proxy: Distance and similarity measures. https://CRAN.R-project.org/package=proxy Schloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp; Crowley, J. (2021). GGally: Extension to ggplot2. https://CRAN.R-project.org/package=GGally Sievert, C., Parmer, C., Hocking, T., Chamberlain, S., Ram, K., Corvellec, M., &amp; Despouy, P. (2021). Plotly: Create interactive web graphics via plotly.js. https://CRAN.R-project.org/package=plotly Tillé, Y., &amp; Matei, A. (2021). Sampling: Survey sampling. https://CRAN.R-project.org/package=sampling Wickham, H. (2021c). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse "],["classification-basic-concepts-and-techniques.html", "Chapter 3 Classification: Basic Concepts and Techniques 3.1 The Zoo Dataset 3.2 Decision Trees 3.3 Model Evaluation with Caret 3.4 Testing: Confusion Matrix and Confidence Interval for Accuracy 3.5 Model Comparison 3.6 Feature Selection and Feature Preparation 3.7 Class Imbalance", " Chapter 3 Classification: Basic Concepts and Techniques Packages used for this chapter: caret (Kuhn, 2021), FSelector (Romanski et al., 2021), lattice (Sarkar, 2021), mlbench (Leisch &amp; Dimitriadou., 2021), pROC (Robin et al., 2021), rpart (Therneau &amp; Atkinson, 2019), rpart.plot (Milborrow, 2020), sampling (Tillé &amp; Matei, 2021), tidyverse (Wickham, 2021c) You can read the free sample chapter from the textbook (Tan et al., 2005): Chapter 3. Classification: Basic Concepts and Techniques 3.1 The Zoo Dataset We will use the Zoo dataset which is included in the R package mlbench (you may have to install it). The Zoo dataset containing 17 (mostly logical) variables on different 101 animals as a data frame with 17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize, type). We convert the data frame into a tidyverse tibble (optional). data(Zoo, package=&quot;mlbench&quot;) head(Zoo) ## hair feathers eggs milk airborne aquatic ## aardvark TRUE FALSE FALSE TRUE FALSE FALSE ## antelope TRUE FALSE FALSE TRUE FALSE FALSE ## bass FALSE FALSE TRUE FALSE FALSE TRUE ## bear TRUE FALSE FALSE TRUE FALSE FALSE ## boar TRUE FALSE FALSE TRUE FALSE FALSE ## buffalo TRUE FALSE FALSE TRUE FALSE FALSE ## predator toothed backbone breathes venomous ## aardvark TRUE TRUE TRUE TRUE FALSE ## antelope FALSE TRUE TRUE TRUE FALSE ## bass TRUE TRUE TRUE FALSE FALSE ## bear TRUE TRUE TRUE TRUE FALSE ## boar TRUE TRUE TRUE TRUE FALSE ## buffalo FALSE TRUE TRUE TRUE FALSE ## fins legs tail domestic catsize type ## aardvark FALSE 4 FALSE FALSE TRUE mammal ## antelope FALSE 4 TRUE FALSE TRUE mammal ## bass TRUE 0 TRUE FALSE FALSE fish ## bear FALSE 4 FALSE FALSE TRUE mammal ## boar FALSE 4 TRUE FALSE TRUE mammal ## buffalo FALSE 4 TRUE FALSE TRUE mammal Note: data.frames in R can have row names. The Zoo data set uses the animal name as the row names. tibbles from tidyverse do not support row names. To keep the animal name you can add a column with the animal name. library(tidyverse) as_tibble(Zoo, rownames = &quot;animal&quot;) ## # A tibble: 101 x 18 ## animal hair feathers eggs milk airborne aquatic ## &lt;chr&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 aardvark TRUE FALSE FALSE TRUE FALSE FALSE ## 2 antelope TRUE FALSE FALSE TRUE FALSE FALSE ## 3 bass FALSE FALSE TRUE FALSE FALSE TRUE ## 4 bear TRUE FALSE FALSE TRUE FALSE FALSE ## 5 boar TRUE FALSE FALSE TRUE FALSE FALSE ## 6 buffalo TRUE FALSE FALSE TRUE FALSE FALSE ## 7 calf TRUE FALSE FALSE TRUE FALSE FALSE ## 8 carp FALSE FALSE TRUE FALSE FALSE TRUE ## 9 catfish FALSE FALSE TRUE FALSE FALSE TRUE ## 10 cavy TRUE FALSE FALSE TRUE FALSE FALSE ## # … with 91 more rows, and 11 more variables: ## # predator &lt;lgl&gt;, toothed &lt;lgl&gt;, backbone &lt;lgl&gt;, ## # breathes &lt;lgl&gt;, venomous &lt;lgl&gt;, fins &lt;lgl&gt;, ## # legs &lt;int&gt;, tail &lt;lgl&gt;, domestic &lt;lgl&gt;, ## # catsize &lt;lgl&gt;, type &lt;fct&gt; You will have to remove the animal column before learning a model! In the following I use the data.frame. I translate all the TRUE/FALSE values into factors (nominal). This is often needed for building models. Always check summary() to make sure the data is ready for model learning. Zoo &lt;- Zoo %&gt;% modify_if(is.logical, factor, levels = c(TRUE, FALSE)) %&gt;% modify_if(is.character, factor) Zoo %&gt;% summary() ## hair feathers eggs milk ## TRUE :43 TRUE :20 TRUE :59 TRUE :41 ## FALSE:58 FALSE:81 FALSE:42 FALSE:60 ## ## ## ## ## ## airborne aquatic predator toothed ## TRUE :24 TRUE :36 TRUE :56 TRUE :61 ## FALSE:77 FALSE:65 FALSE:45 FALSE:40 ## ## ## ## ## ## backbone breathes venomous fins ## TRUE :83 TRUE :80 TRUE : 8 TRUE :17 ## FALSE:18 FALSE:21 FALSE:93 FALSE:84 ## ## ## ## ## ## legs tail domestic catsize ## Min. :0.00 TRUE :75 TRUE :13 TRUE :44 ## 1st Qu.:2.00 FALSE:26 FALSE:88 FALSE:57 ## Median :4.00 ## Mean :2.84 ## 3rd Qu.:4.00 ## Max. :8.00 ## ## type ## mammal :41 ## bird :20 ## reptile : 5 ## fish :13 ## amphibian : 4 ## insect : 8 ## mollusc.et.al:10 3.2 Decision Trees Recursive Partitioning (similar to CART) uses the Gini index to make splitting decisions and early stopping (pre-pruning). library(rpart) 3.2.1 Create Tree With Default Settings (uses pre-pruning) tree_default &lt;- Zoo %&gt;% rpart(type ~ ., data = .) tree_default ## n= 101 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099) ## 2) milk=TRUE 41 0 mammal (1 0 0 0 0 0 0) * ## 3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17) ## 6) feathers=TRUE 20 0 bird (0 1 0 0 0 0 0) * ## 7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25) ## 14) fins=TRUE 13 0 fish (0 0 0 1 0 0 0) * ## 15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37) ## 30) backbone=TRUE 9 4 reptile (0 0 0.56 0 0.44 0 0) * ## 31) backbone=FALSE 18 8 mollusc.et.al (0 0 0 0 0 0.44 0.56) * Notes: - %&gt;% supplies the data for rpart. Since data is not the first argument of rpart, the syntax data = . is used to specify where the data in Zoo goes. The call is equivalent to tree_default &lt;- rpart(type ~ ., data = Zoo). - The formula models the type variable by all other features represented by .. data = . means that the data provided by the pipe (%&gt;%) will be passed to rpart as the argument data. the class variable needs a factor (nominal) or rpart will create a regression tree instead of a decision tree. Use as.factor() if necessary. Plotting library(rpart.plot) rpart.plot(tree_default, extra = 2) ## Warning: Cannot retrieve the data used to build the model (so cannot determine roundint and is.binary for the variables). ## To silence this warning: ## Call rpart.plot with roundint=FALSE, ## or rebuild the rpart model with model=TRUE. Note: extra=2 prints for each leaf node the number of correctly classified objects from data and the total number of objects from the training data falling into that node (correct/total). 3.2.2 Create a Full Tree To create a full tree, we set the complexity parameter cp to 0 (split even if it does not improve the tree) and we set the minimum number of observations in a node needed to split to the smallest value of 2 (see: ?rpart.control). Note: full trees overfit the training data! tree_full &lt;- Zoo %&gt;% rpart(type ~., data = ., control = rpart.control(minsplit = 2, cp = 0)) rpart.plot(tree_full, extra = 2, roundint=FALSE, box.palette = list(&quot;Gy&quot;, &quot;Gn&quot;, &quot;Bu&quot;, &quot;Bn&quot;, &quot;Or&quot;, &quot;Rd&quot;, &quot;Pu&quot;)) # specify 7 colors tree_full ## n= 101 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 101 60 mammal (0.41 0.2 0.05 0.13 0.04 0.079 0.099) ## 2) milk=TRUE 41 0 mammal (1 0 0 0 0 0 0) * ## 3) milk=FALSE 60 40 bird (0 0.33 0.083 0.22 0.067 0.13 0.17) ## 6) feathers=TRUE 20 0 bird (0 1 0 0 0 0 0) * ## 7) feathers=FALSE 40 27 fish (0 0 0.12 0.33 0.1 0.2 0.25) ## 14) fins=TRUE 13 0 fish (0 0 0 1 0 0 0) * ## 15) fins=FALSE 27 17 mollusc.et.al (0 0 0.19 0 0.15 0.3 0.37) ## 30) backbone=TRUE 9 4 reptile (0 0 0.56 0 0.44 0 0) ## 60) aquatic=FALSE 4 0 reptile (0 0 1 0 0 0 0) * ## 61) aquatic=TRUE 5 1 amphibian (0 0 0.2 0 0.8 0 0) ## 122) eggs=FALSE 1 0 reptile (0 0 1 0 0 0 0) * ## 123) eggs=TRUE 4 0 amphibian (0 0 0 0 1 0 0) * ## 31) backbone=FALSE 18 8 mollusc.et.al (0 0 0 0 0 0.44 0.56) ## 62) airborne=TRUE 6 0 insect (0 0 0 0 0 1 0) * ## 63) airborne=FALSE 12 2 mollusc.et.al (0 0 0 0 0 0.17 0.83) ## 126) predator=FALSE 4 2 insect (0 0 0 0 0 0.5 0.5) ## 252) legs&gt;=3 2 0 insect (0 0 0 0 0 1 0) * ## 253) legs&lt; 3 2 0 mollusc.et.al (0 0 0 0 0 0 1) * ## 127) predator=TRUE 8 0 mollusc.et.al (0 0 0 0 0 0 1) * Training error on tree with pre-pruning predict(tree_default, Zoo) %&gt;% head () ## mammal bird reptile fish amphibian insect ## aardvark 1 0 0 0 0 0 ## antelope 1 0 0 0 0 0 ## bass 0 0 0 1 0 0 ## bear 1 0 0 0 0 0 ## boar 1 0 0 0 0 0 ## buffalo 1 0 0 0 0 0 ## mollusc.et.al ## aardvark 0 ## antelope 0 ## bass 0 ## bear 0 ## boar 0 ## buffalo 0 pred &lt;- predict(tree_default, Zoo, type=&quot;class&quot;) head(pred) ## aardvark antelope bass bear boar buffalo ## mammal mammal fish mammal mammal mammal ## 7 Levels: mammal bird reptile fish ... mollusc.et.al confusion_table &lt;- with(Zoo, table(type, pred)) confusion_table ## pred ## type mammal bird reptile fish amphibian ## mammal 41 0 0 0 0 ## bird 0 20 0 0 0 ## reptile 0 0 5 0 0 ## fish 0 0 0 13 0 ## amphibian 0 0 4 0 0 ## insect 0 0 0 0 0 ## mollusc.et.al 0 0 0 0 0 ## pred ## type insect mollusc.et.al ## mammal 0 0 ## bird 0 0 ## reptile 0 0 ## fish 0 0 ## amphibian 0 0 ## insect 0 8 ## mollusc.et.al 0 10 correct &lt;- confusion_table %&gt;% diag() %&gt;% sum() correct ## [1] 89 error &lt;- confusion_table %&gt;% sum() - correct error ## [1] 12 accuracy &lt;- correct / (correct + error) accuracy ## [1] 0.881 Use a function for accuracy accuracy &lt;- function(truth, prediction) { tbl &lt;- table(truth, prediction) sum(diag(tbl))/sum(tbl) } accuracy(Zoo %&gt;% pull(type), pred) ## [1] 0.881 Training error of the full tree accuracy(Zoo %&gt;% pull(type), predict(tree_full, Zoo, type=&quot;class&quot;)) ## [1] 1 Get a confusion table with more statistics (using caret) library(caret) confusionMatrix(data = pred, reference = Zoo %&gt;% pull(type)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction mammal bird reptile fish amphibian ## mammal 41 0 0 0 0 ## bird 0 20 0 0 0 ## reptile 0 0 5 0 4 ## fish 0 0 0 13 0 ## amphibian 0 0 0 0 0 ## insect 0 0 0 0 0 ## mollusc.et.al 0 0 0 0 0 ## Reference ## Prediction insect mollusc.et.al ## mammal 0 0 ## bird 0 0 ## reptile 0 0 ## fish 0 0 ## amphibian 0 0 ## insect 0 0 ## mollusc.et.al 8 10 ## ## Overall Statistics ## ## Accuracy : 0.881 ## 95% CI : (0.802, 0.937) ## No Information Rate : 0.406 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.843 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: mammal Class: bird ## Sensitivity 1.000 1.000 ## Specificity 1.000 1.000 ## Pos Pred Value 1.000 1.000 ## Neg Pred Value 1.000 1.000 ## Prevalence 0.406 0.198 ## Detection Rate 0.406 0.198 ## Detection Prevalence 0.406 0.198 ## Balanced Accuracy 1.000 1.000 ## Class: reptile Class: fish ## Sensitivity 1.0000 1.000 ## Specificity 0.9583 1.000 ## Pos Pred Value 0.5556 1.000 ## Neg Pred Value 1.0000 1.000 ## Prevalence 0.0495 0.129 ## Detection Rate 0.0495 0.129 ## Detection Prevalence 0.0891 0.129 ## Balanced Accuracy 0.9792 1.000 ## Class: amphibian Class: insect ## Sensitivity 0.0000 0.0000 ## Specificity 1.0000 1.0000 ## Pos Pred Value NaN NaN ## Neg Pred Value 0.9604 0.9208 ## Prevalence 0.0396 0.0792 ## Detection Rate 0.0000 0.0000 ## Detection Prevalence 0.0000 0.0000 ## Balanced Accuracy 0.5000 0.5000 ## Class: mollusc.et.al ## Sensitivity 1.000 ## Specificity 0.912 ## Pos Pred Value 0.556 ## Neg Pred Value 1.000 ## Prevalence 0.099 ## Detection Rate 0.099 ## Detection Prevalence 0.178 ## Balanced Accuracy 0.956 3.2.3 Make Predictions for New Data Make up my own animal: A lion with feathered wings my_animal &lt;- tibble(hair = TRUE, feathers = TRUE, eggs = FALSE, milk = TRUE, airborne = TRUE, aquatic = FALSE, predator = TRUE, toothed = TRUE, backbone = TRUE, breathes = TRUE, venomous = FALSE, fins = FALSE, legs = 4, tail = TRUE, domestic = FALSE, catsize = FALSE, type = NA) Fix columns to be factors like in the training set. my_animal &lt;- my_animal %&gt;% modify_if(is.logical, factor, levels = c(TRUE, FALSE)) my_animal ## # A tibble: 1 x 17 ## hair feathers eggs milk airborne aquatic predator ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 TRUE TRUE FALSE TRUE TRUE FALSE TRUE ## # … with 10 more variables: toothed &lt;fct&gt;, ## # backbone &lt;fct&gt;, breathes &lt;fct&gt;, venomous &lt;fct&gt;, ## # fins &lt;fct&gt;, legs &lt;dbl&gt;, tail &lt;fct&gt;, ## # domestic &lt;fct&gt;, catsize &lt;fct&gt;, type &lt;fct&gt; Make a prediction using the default tree predict(tree_default , my_animal, type = &quot;class&quot;) ## 1 ## mammal ## 7 Levels: mammal bird reptile fish ... mollusc.et.al 3.3 Model Evaluation with Caret The package caret makes preparing training sets, building classification (and regression) models and evaluation easier. A great cheat sheet can be found here. library(caret) Cross-validation runs are independent and can be done faster in parallel. To enable multi-core support, caret uses the package foreach and you need to load a do backend. For Linux, you can use doMC with 4 cores. Windows needs different backend like doParallel (see caret cheat sheet above). ## Linux backend # library(doMC) # registerDoMC(cores = 4) # getDoParWorkers() ## Windows backend # library(doParallel) # cl &lt;- makeCluster(4, type=&quot;SOCK&quot;) # registerDoParallel(cl) Set random number generator seed to make results reproducible set.seed(2000) 3.3.1 Hold out Test Data Test data is not used in the model building process and set aside purely for testing the model. Here, we partition data the 80% training and 20% testing. inTrain &lt;- createDataPartition(y = Zoo$type, p = .8, list = FALSE) Zoo_train &lt;- Zoo %&gt;% slice(inTrain) Zoo_test &lt;- Zoo %&gt;% slice(-inTrain) 3.3.2 Learn a Model and Tune Hyperparameters on the Training Data The package caret combines training and validation for hyperparameter tuning into a single function called train(). It internally splits the data into training and validation sets and thus will provide you with error estimates for different hyperparameter settings. trainControl is used to choose how testing is performed. For rpart, train tries to tune the cp parameter (tree complexity) using accuracy to chose the best model. I set minsplit to 2 since we have not much data. Note: Parameters used for tuning (in this case cp) need to be set using a data.frame in the argument tuneGrid! Setting it in control will be ignored. fit &lt;- Zoo_train %&gt;% train(type ~ ., data = . , method = &quot;rpart&quot;, control = rpart.control(minsplit = 2), trControl = trainControl(method = &quot;cv&quot;, number = 10), tuneLength = 5) fit ## CART ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 77, 74, 75, 73, 74, 76, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.00 0.938 0.919 ## 0.08 0.897 0.868 ## 0.16 0.745 0.664 ## 0.22 0.666 0.554 ## 0.32 0.474 0.190 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was cp = 0. Note: Train has built 10 trees using the training folds for each value of cp and the reported values for accuracy and Kappa are the averages on the validation folds. A model using the best tuning parameters and using all the data supplied to train() is available as fit$finalModel. rpart.plot(fit$finalModel, extra = 2, box.palette = list(&quot;Gy&quot;, &quot;Gn&quot;, &quot;Bu&quot;, &quot;Bn&quot;, &quot;Or&quot;, &quot;Rd&quot;, &quot;Pu&quot;)) caret also computes variable importance. By default it uses competing splits (splits which would be runners up, but do not get chosen by the tree) for rpart models (see ? varImp). Toothed is the runner up for many splits, but it never gets chosen! varImp(fit) ## rpart variable importance ## ## Overall ## toothedFALSE 100.00 ## feathersFALSE 69.81 ## backboneFALSE 63.08 ## milkFALSE 55.56 ## eggsFALSE 53.61 ## hairFALSE 50.52 ## finsFALSE 46.98 ## tailFALSE 28.45 ## breathesFALSE 28.13 ## airborneFALSE 26.27 ## legs 25.86 ## aquaticFALSE 5.96 ## predatorFALSE 2.35 ## venomousFALSE 1.39 ## catsizeFALSE 0.00 ## domesticFALSE 0.00 Here is the variable importance without competing splits. imp &lt;- varImp(fit, compete = FALSE) imp ## rpart variable importance ## ## Overall ## milkFALSE 100.00 ## feathersFALSE 55.69 ## finsFALSE 39.45 ## toothedFALSE 22.96 ## airborneFALSE 22.48 ## aquaticFALSE 9.99 ## eggsFALSE 6.66 ## legs 5.55 ## predatorFALSE 1.85 ## domesticFALSE 0.00 ## breathesFALSE 0.00 ## catsizeFALSE 0.00 ## tailFALSE 0.00 ## hairFALSE 0.00 ## backboneFALSE 0.00 ## venomousFALSE 0.00 ggplot(imp) Note: Not all models provide a variable importance function. In this case caret might calculate varImp by itself and ignore the model (see ? varImp)! 3.4 Testing: Confusion Matrix and Confidence Interval for Accuracy Use the best model on the test data pred &lt;- predict(fit, newdata = Zoo_test) pred ## [1] mammal mammal mollusc.et.al ## [4] insect mammal mammal ## [7] mammal bird mammal ## [10] mammal bird fish ## [13] fish mammal mollusc.et.al ## [16] bird insect bird ## 7 Levels: mammal bird reptile fish ... mollusc.et.al Caret’s confusionMatrix() function calculates accuracy, confidence intervals, kappa and many more evaluation metrics. You need to use separate test data to create a confusion matrix based on the generalization error. confusionMatrix(data = pred, ref = Zoo_test$type) ## Confusion Matrix and Statistics ## ## Reference ## Prediction mammal bird reptile fish amphibian ## mammal 8 0 0 0 0 ## bird 0 4 0 0 0 ## reptile 0 0 0 0 0 ## fish 0 0 0 2 0 ## amphibian 0 0 0 0 0 ## insect 0 0 1 0 0 ## mollusc.et.al 0 0 0 0 0 ## Reference ## Prediction insect mollusc.et.al ## mammal 0 0 ## bird 0 0 ## reptile 0 0 ## fish 0 0 ## amphibian 0 0 ## insect 1 0 ## mollusc.et.al 0 2 ## ## Overall Statistics ## ## Accuracy : 0.944 ## 95% CI : (0.727, 0.999) ## No Information Rate : 0.444 ## P-Value [Acc &gt; NIR] : 1.08e-05 ## ## Kappa : 0.923 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: mammal Class: bird ## Sensitivity 1.000 1.000 ## Specificity 1.000 1.000 ## Pos Pred Value 1.000 1.000 ## Neg Pred Value 1.000 1.000 ## Prevalence 0.444 0.222 ## Detection Rate 0.444 0.222 ## Detection Prevalence 0.444 0.222 ## Balanced Accuracy 1.000 1.000 ## Class: reptile Class: fish ## Sensitivity 0.0000 1.000 ## Specificity 1.0000 1.000 ## Pos Pred Value NaN 1.000 ## Neg Pred Value 0.9444 1.000 ## Prevalence 0.0556 0.111 ## Detection Rate 0.0000 0.111 ## Detection Prevalence 0.0000 0.111 ## Balanced Accuracy 0.5000 1.000 ## Class: amphibian Class: insect ## Sensitivity NA 1.0000 ## Specificity 1 0.9412 ## Pos Pred Value NA 0.5000 ## Neg Pred Value NA 1.0000 ## Prevalence 0 0.0556 ## Detection Rate 0 0.0556 ## Detection Prevalence 0 0.1111 ## Balanced Accuracy NA 0.9706 ## Class: mollusc.et.al ## Sensitivity 1.000 ## Specificity 1.000 ## Pos Pred Value 1.000 ## Neg Pred Value 1.000 ## Prevalence 0.111 ## Detection Rate 0.111 ## Detection Prevalence 0.111 ## Balanced Accuracy 1.000 Some notes Many classification algorithms and train in caret do not deal well with missing values. If your classification model can deal with missing values (e.g., rpart) then use na.action = na.pass when you call train and predict. Otherwise, you need to remove observations with missing values with na.omit or use imputation to replace the missing values before you train the model. Make sure that you still have enough observations left. Make sure that nominal variables (this includes logical variables) are coded as factors. The class variable for train in caret cannot have level names that are keywords in R (e.g., TRUE and FALSE). Rename them to, for example, “yes” and “no.” Make sure that nominal variables (factors) have examples for all possible values. Some methods might have problems with variable values without examples. You can drop empty levels using droplevels or factor. Sampling in train might create a sample that does not contain examples for all values in a nominal (factor) variable. You will get an error message. This most likely happens for variables which have one very rare value. You may have to remove the variable. 3.5 Model Comparison We will compare decision trees with a k-nearest neighbors (kNN) classifier. We will create fixed sampling scheme (10-folds) so we compare the different models using exactly the same folds. It is specified as trControl during training. train_index &lt;- createFolds(Zoo_train$type, k = 10) Build models rpartFit &lt;- Zoo_train %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, tuneLength = 10, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index) ) Note: for kNN we ask train to scale the data using preProcess = \"scale\". Logicals will be used as 0-1 variables in Euclidean distance calculation. knnFit &lt;- Zoo_train %&gt;% train(type ~ ., data = ., method = &quot;knn&quot;, preProcess = &quot;scale&quot;, tuneLength = 10, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index) ) Compare accuracy over all folds. resamps &lt;- resamples(list( CART = rpartFit, kNearestNeighbors = knnFit )) summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: CART, kNearestNeighbors ## Number of resamples: 10 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. ## CART 0.667 0.875 0.889 0.872 0.889 ## kNearestNeighbors 0.875 0.917 1.000 0.965 1.000 ## Max. NA&#39;s ## CART 1 0 ## kNearestNeighbors 1 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. ## CART 0.591 0.833 0.847 0.834 0.857 ## kNearestNeighbors 0.833 0.898 1.000 0.955 1.000 ## Max. NA&#39;s ## CART 1 0 ## kNearestNeighbors 1 0 caret provides some visualizations using the package lattice. For example, a boxplot to compare the accuracy and kappa distribution (over the 10 folds). library(lattice) bwplot(resamps, layout = c(3, 1)) We see that kNN is performing consistently better on the folds than CART (except for some outlier folds). Find out if one models is statistically better than the other (is the difference in accuracy is not zero). difs &lt;- diff(resamps) difs ## ## Call: ## diff.resamples(x = resamps) ## ## Models: CART, kNearestNeighbors ## Metrics: Accuracy, Kappa ## Number of differences: 1 ## p-value adjustment: bonferroni summary(difs) ## ## Call: ## summary.diff.resamples(object = difs) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## Accuracy ## CART kNearestNeighbors ## CART -0.0931 ## kNearestNeighbors 0.0115 ## ## Kappa ## CART kNearestNeighbors ## CART -0.121 ## kNearestNeighbors 0.0104 p-values tells you the probability of seeing an even more extreme value (difference between accuracy) given that the null hypothesis (difference = 0) is true. For a better classifier, the p-value should be less than .05 or 0.01. diff automatically applies Bonferroni correction for multiple comparisons. In this case, kNN seems better but the classifiers do not perform statistically differently. 3.6 Feature Selection and Feature Preparation Decision trees implicitly select features for splitting, but we can also select features manually. library(FSelector) see: http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Feature_Selection#The_Feature_Ranking_Approach 3.6.1 Univariate Feature Importance Score These scores measure how related each feature is to the class variable. For discrete features (as in our case), the chi-square statistic can be used to derive a score. weights &lt;- Zoo_train %&gt;% chi.squared(type ~ ., data = .) %&gt;% as_tibble(rownames = &quot;feature&quot;) %&gt;% arrange(desc(attr_importance)) weights ## # A tibble: 16 x 2 ## feature attr_importance ## &lt;chr&gt; &lt;dbl&gt; ## 1 feathers 1 ## 2 milk 1 ## 3 backbone 1 ## 4 toothed 0.975 ## 5 eggs 0.933 ## 6 hair 0.907 ## 7 breathes 0.898 ## 8 airborne 0.848 ## 9 fins 0.845 ## 10 legs 0.828 ## 11 tail 0.779 ## 12 catsize 0.664 ## 13 aquatic 0.655 ## 14 venomous 0.475 ## 15 predator 0.385 ## 16 domestic 0.231 plot importance in descending order (using reorder to order factor levels used by ggplot). ggplot(weights, aes(x = attr_importance, y = reorder(feature, attr_importance))) + geom_bar(stat = &quot;identity&quot;) + xlab(&quot;Importance score&quot;) + ylab(&quot;Feature&quot;) Get the 5 best features subset &lt;- cutoff.k(weights %&gt;% column_to_rownames(&quot;feature&quot;), 5) subset ## [1] &quot;feathers&quot; &quot;milk&quot; &quot;backbone&quot; &quot;toothed&quot; ## [5] &quot;eggs&quot; Use only the best 5 features to build a model (Fselector provides as.simple.formula) f &lt;- as.simple.formula(subset, &quot;type&quot;) f ## type ~ feathers + milk + backbone + toothed + eggs ## &lt;environment: 0x558770ce3638&gt; m &lt;- Zoo_train %&gt;% rpart(f, data = .) rpart.plot(m, extra = 2, roundint = FALSE) There are many alternative ways to calculate univariate importance scores (see package FSelector). Some of them (also) work for continuous features. One example is the information gain ratio based on entropy as used in decision tree induction. Zoo_train %&gt;% gain.ratio(type ~ ., data = .) %&gt;% as_tibble(rownames = &quot;feature&quot;) %&gt;% arrange(desc(attr_importance)) ## # A tibble: 16 x 2 ## feature attr_importance ## &lt;chr&gt; &lt;dbl&gt; ## 1 milk 1 ## 2 backbone 1 ## 3 feathers 1 ## 4 toothed 0.919 ## 5 eggs 0.827 ## 6 breathes 0.821 ## 7 hair 0.782 ## 8 fins 0.689 ## 9 legs 0.682 ## 10 airborne 0.671 ## 11 tail 0.573 ## 12 aquatic 0.391 ## 13 catsize 0.383 ## 14 venomous 0.351 ## 15 predator 0.125 ## 16 domestic 0.0975 3.6.2 Feature Subset Selection Often features are related and calculating importance for each feature independently is not optimal. We can use greedy search heuristics. For example cfs uses correlation/entropy with best first search. Zoo_train %&gt;% cfs(type ~ ., data = .) ## [1] &quot;hair&quot; &quot;feathers&quot; &quot;eggs&quot; &quot;milk&quot; ## [5] &quot;toothed&quot; &quot;backbone&quot; &quot;breathes&quot; &quot;fins&quot; ## [9] &quot;legs&quot; &quot;tail&quot; Black-box feature selection uses an evaluator function (the black box) to calculate a score to be maximized. First, we define an evaluation function that builds a model given a subset of features and calculates a quality score. We use here the average for 5 bootstrap samples (method = \"cv\" can also be used instead), no tuning (to be faster), and the average accuracy as the score. evaluator &lt;- function(subset) { model &lt;- Zoo_train %&gt;% train(as.simple.formula(subset, &quot;type&quot;), data = ., method = &quot;rpart&quot;, trControl = trainControl(method = &quot;boot&quot;, number = 5), tuneLength = 0) results &lt;- model$resample$Accuracy cat(&quot;Trying features:&quot;, paste(subset, collapse = &quot; + &quot;), &quot;\\n&quot;) m &lt;- mean(results) cat(&quot;Accuracy:&quot;, round(m, 2), &quot;\\n\\n&quot;) m } Start with all features (but not the class variable type) features &lt;- Zoo_train %&gt;% colnames() %&gt;% setdiff(&quot;type&quot;) There are several (greedy) search strategies available. These run for a while! ##subset &lt;- backward.search(features, evaluator) ##subset &lt;- forward.search(features, evaluator) ##subset &lt;- best.first.search(features, evaluator) ##subset &lt;- hill.climbing.search(features, evaluator) ##subset 3.6.3 Using Dummy Variables for Factors Nominal features (factors) are often encoded as a series of 0-1 dummy variables. For example, let us try to predict if an animal is a predator given the type. First we use the original encoding of type as a factor with several values. tree_predator &lt;- Zoo_train %&gt;% rpart(predator ~ type, data = .) rpart.plot(tree_predator, extra = 2, roundint = FALSE) Note: Some splits use multiple values. Building the tree will become extremely slow if a factor has many levels (different values) since the tree has to check all possible splits into two subsets. This situation should be avoided. Recode type as a set of 0-1 dummy variables using class2ind. See also ? dummyVars in package caret. Zoo_train_dummy &lt;- as_tibble(class2ind(Zoo_train$type)) %&gt;% mutate_all(as.factor) %&gt;% add_column(predator = Zoo_train$predator) Zoo_train_dummy ## # A tibble: 83 x 8 ## mammal bird reptile fish amphibian insect ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 1 0 0 0 0 0 ## 2 1 0 0 0 0 0 ## 3 0 0 0 1 0 0 ## 4 1 0 0 0 0 0 ## 5 1 0 0 0 0 0 ## 6 1 0 0 0 0 0 ## 7 0 0 0 1 0 0 ## 8 0 0 0 1 0 0 ## 9 1 0 0 0 0 0 ## 10 0 1 0 0 0 0 ## # … with 73 more rows, and 2 more variables: ## # mollusc.et.al &lt;fct&gt;, predator &lt;fct&gt; tree_predator &lt;- Zoo_train_dummy %&gt;% rpart(predator ~ ., data = ., control = rpart.control(minsplit = 2, cp = 0.01)) rpart.plot(tree_predator, roundint = FALSE) Using caret on the original factor encoding automatically translates factors (here type) into 0-1 dummy variables (e.g., typeinsect = 0). The reason is that some models cannot directly use factors and caret tries to consistently work with all of them. fit &lt;- Zoo_train %&gt;% train(predator ~ type, data = ., method = &quot;rpart&quot;, control = rpart.control(minsplit = 2), tuneGrid = data.frame(cp = 0.01)) fit ## CART ## ## 83 samples ## 1 predictor ## 2 classes: &#39;TRUE&#39;, &#39;FALSE&#39; ## ## No pre-processing ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 83, 83, 83, 83, 83, 83, ... ## Resampling results: ## ## Accuracy Kappa ## 0.606 0.203 ## ## Tuning parameter &#39;cp&#39; was held constant at a value ## of 0.01 rpart.plot(fit$finalModel, extra = 2) Note: To use a fixed value for the tuning parameter cp, we have to create a tuning grid that only icontains that value. 3.7 Class Imbalance Classifiers have a hard time to learn from data where we have much more observations for one class (called the majority class). This is called the class imbalance problem. Here is a very good article about the problem and solutions. library(rpart) library(rpart.plot) data(Zoo, package=&quot;mlbench&quot;) Class distribution ggplot(Zoo, aes(y = type)) + geom_bar() To create an imbalanced problem, we want to decide if an animal is an reptile. First, we change the class variable to make it into a binary reptile/no reptile classification problem. Note: We use here the training data for testing. You should use a separate testing data set! Zoo_reptile &lt;- Zoo %&gt;% mutate( type = factor(Zoo$type == &quot;reptile&quot;, levels = c(FALSE, TRUE), labels = c(&quot;nonreptile&quot;, &quot;reptile&quot;))) Do not forget to make the class variable a factor (a nominal variable) or you will get a regression tree instead of a classification tree. summary(Zoo_reptile) ## hair feathers eggs ## Mode :logical Mode :logical Mode :logical ## FALSE:58 FALSE:81 FALSE:42 ## TRUE :43 TRUE :20 TRUE :59 ## ## ## ## milk airborne aquatic ## Mode :logical Mode :logical Mode :logical ## FALSE:60 FALSE:77 FALSE:65 ## TRUE :41 TRUE :24 TRUE :36 ## ## ## ## predator toothed backbone ## Mode :logical Mode :logical Mode :logical ## FALSE:45 FALSE:40 FALSE:18 ## TRUE :56 TRUE :61 TRUE :83 ## ## ## ## breathes venomous fins ## Mode :logical Mode :logical Mode :logical ## FALSE:21 FALSE:93 FALSE:84 ## TRUE :80 TRUE :8 TRUE :17 ## ## ## ## legs tail domestic ## Min. :0.00 Mode :logical Mode :logical ## 1st Qu.:2.00 FALSE:26 FALSE:88 ## Median :4.00 TRUE :75 TRUE :13 ## Mean :2.84 ## 3rd Qu.:4.00 ## Max. :8.00 ## catsize type ## Mode :logical nonreptile:96 ## FALSE:57 reptile : 5 ## TRUE :44 ## ## ## See if we have a class imbalance problem. ggplot(Zoo_reptile, aes(y = type)) + geom_bar() Create test and training data. I use here a 50/50 split to make sure that the test set has some samples of the rare reptile class. set.seed(1234) inTrain &lt;- createDataPartition(y = Zoo_reptile$type, p = .5, list = FALSE) training_reptile &lt;- Zoo_reptile %&gt;% slice(inTrain) testing_reptile &lt;- Zoo_reptile %&gt;% slice(-inTrain) the new class variable is clearly not balanced. This is a problem for building a tree! 3.7.1 Option 1: Use the Data As Is and Hope For The Best fit &lt;- training_reptile %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, trControl = trainControl(method = &quot;cv&quot;)) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = ## weights, info = trainInfo, : There were missing values ## in resampled performance measures. Warnings: “There were missing values in resampled performance measures.” means that some test folds did not contain examples of both classes. This is very likely with class imbalance and small datasets. fit ## CART ## ## 51 samples ## 16 predictors ## 2 classes: &#39;nonreptile&#39;, &#39;reptile&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 46, 47, 46, 46, 45, 46, ... ## Resampling results: ## ## Accuracy Kappa ## 0.947 0 ## ## Tuning parameter &#39;cp&#39; was held constant at a value of 0 rpart.plot(fit$finalModel, extra = 2) the tree predicts everything as non-reptile. Have a look at the error on the test set. confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 48 2 ## reptile 0 0 ## ## Accuracy : 0.96 ## 95% CI : (0.863, 0.995) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 0.677 ## ## Kappa : 0 ## ## Mcnemar&#39;s Test P-Value : 0.480 ## ## Sensitivity : 0.00 ## Specificity : 1.00 ## Pos Pred Value : NaN ## Neg Pred Value : 0.96 ## Prevalence : 0.04 ## Detection Rate : 0.00 ## Detection Prevalence : 0.00 ## Balanced Accuracy : 0.50 ## ## &#39;Positive&#39; Class : reptile ## Accuracy is high, but it is exactly the same as the no-information rate and kappa is zero. Sensitivity is also zero, meaning that we do not identify any positive (reptile). If the cost of missing a positive is much larger than the cost associated with misclassifying a negative, then accuracy is not a good measure! By dealing with imbalance, we are not concerned with accuracy, but we want to increase the sensitivity, i.e., the chance to identify positive examples. Note: The positive class value (the one that you want to detect) is set manually to reptile using positive = \"reptile\". Otherwise sensitivity/specificity will not be correctly calculated. 3.7.2 Option 2: Balance Data With Resampling We use stratified sampling with replacement (to oversample the minority/positive class). You could also use SMOTE (in package DMwR) or other sampling strategies (e.g., from package unbalanced). We use 50+50 observations here (Note: many samples will be chosen several times). library(sampling) set.seed(1000) # for repeatability id &lt;- strata(training_reptile, stratanames = &quot;type&quot;, size = c(50, 50), method = &quot;srswr&quot;) training_reptile_balanced &lt;- training_reptile %&gt;% slice(id$ID_unit) table(training_reptile_balanced$type) ## ## nonreptile reptile ## 50 50 fit &lt;- training_reptile_balanced %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, trControl = trainControl(method = &quot;cv&quot;), control = rpart.control(minsplit = 5)) fit ## CART ## ## 100 samples ## 16 predictor ## 2 classes: &#39;nonreptile&#39;, &#39;reptile&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 90, 90, 90, 90, 90, 90, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.18 0.81 0.62 ## 0.30 0.63 0.26 ## 0.34 0.53 0.06 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was cp = 0.18. rpart.plot(fit$finalModel, extra = 2) Check on the unbalanced testing data. confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 19 0 ## reptile 29 2 ## ## Accuracy : 0.42 ## 95% CI : (0.282, 0.568) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.05 ## ## Mcnemar&#39;s Test P-Value : 2e-07 ## ## Sensitivity : 1.0000 ## Specificity : 0.3958 ## Pos Pred Value : 0.0645 ## Neg Pred Value : 1.0000 ## Prevalence : 0.0400 ## Detection Rate : 0.0400 ## Detection Prevalence : 0.6200 ## Balanced Accuracy : 0.6979 ## ## &#39;Positive&#39; Class : reptile ## Note that the accuracy is below the no information rate! However, kappa (improvement of accuracy over randomness) and sensitivity (the ability to identify reptiles) have increased. There is a tradeoff between sensitivity and specificity (how many of the identified animals are really reptiles) The tradeoff can be controlled using the sample proportions. We can sample more reptiles to increase sensitivity at the cost of lower specificity (this effect cannot be seen in the data since the test set has only a few reptiles). id &lt;- strata(training_reptile, stratanames = &quot;type&quot;, size = c(50, 100), method = &quot;srswr&quot;) training_reptile_balanced &lt;- training_reptile %&gt;% slice(id$ID_unit) table(training_reptile_balanced$type) ## ## nonreptile reptile ## 50 100 fit &lt;- training_reptile_balanced %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, trControl = trainControl(method = &quot;cv&quot;), control = rpart.control(minsplit = 5)) confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 33 0 ## reptile 15 2 ## ## Accuracy : 0.7 ## 95% CI : (0.554, 0.821) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 1.000000 ## ## Kappa : 0.15 ## ## Mcnemar&#39;s Test P-Value : 0.000301 ## ## Sensitivity : 1.000 ## Specificity : 0.688 ## Pos Pred Value : 0.118 ## Neg Pred Value : 1.000 ## Prevalence : 0.040 ## Detection Rate : 0.040 ## Detection Prevalence : 0.340 ## Balanced Accuracy : 0.844 ## ## &#39;Positive&#39; Class : reptile ## 3.7.3 Option 3: Build A Larger Tree and use Predicted Probabilities Increase complexity and require less data for splitting a node. Here I also use AUC (area under the ROC) as the tuning metric. You need to specify the two class summary function. Note that the tree still trying to improve accuracy on the data and not AUC! I also enable class probabilities since I want to predict probabilities later. fit &lt;- training_reptile %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, tuneLength = 10, trControl = trainControl(method = &quot;cv&quot;, classProbs = TRUE, ## necessary for predict with type=&quot;prob&quot; summaryFunction=twoClassSummary), ## necessary for ROC metric = &quot;ROC&quot;, control = rpart.control(minsplit = 3)) ## Warning in nominalTrainWorkflow(x = x, y = y, wts = ## weights, info = trainInfo, : There were missing values ## in resampled performance measures. fit ## CART ## ## 51 samples ## 16 predictors ## 2 classes: &#39;nonreptile&#39;, &#39;reptile&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 46, 47, 46, 46, 46, 45, ... ## Resampling results: ## ## ROC Sens Spec ## 0.358 0.975 0 ## ## Tuning parameter &#39;cp&#39; was held constant at a value of 0 rpart.plot(fit$finalModel, extra = 2) confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 48 2 ## reptile 0 0 ## ## Accuracy : 0.96 ## 95% CI : (0.863, 0.995) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 0.677 ## ## Kappa : 0 ## ## Mcnemar&#39;s Test P-Value : 0.480 ## ## Sensitivity : 0.00 ## Specificity : 1.00 ## Pos Pred Value : NaN ## Neg Pred Value : 0.96 ## Prevalence : 0.04 ## Detection Rate : 0.00 ## Detection Prevalence : 0.00 ## Balanced Accuracy : 0.50 ## ## &#39;Positive&#39; Class : reptile ## Note: Accuracy is high, but it is close or below to the no-information rate! 3.7.3.1 Create A Biased Classifier We can create a classifier which will detect more reptiles at the expense of misclassifying non-reptiles. This is equivalent to increasing the cost of misclassifying a reptile as a non-reptile. The usual rule is to predict in each node the majority class from the test data in the node. For a binary classification problem that means a probability of &gt;50%. In the following, we reduce this threshold to 1% or more. This means that if the new observation ends up in a leaf node with 1% or more reptiles from training then the observation will be classified as a reptile. The data set is small and this works better with more data. prob &lt;- predict(fit, testing_reptile, type = &quot;prob&quot;) tail(prob) ## nonreptile reptile ## tuna 1.000 0.0000 ## vole 0.962 0.0385 ## wasp 0.500 0.5000 ## wolf 0.962 0.0385 ## worm 1.000 0.0000 ## wren 0.962 0.0385 pred &lt;- as.factor(ifelse(prob[,&quot;reptile&quot;]&gt;=0.01, &quot;reptile&quot;, &quot;nonreptile&quot;)) confusionMatrix(data = pred, ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 13 0 ## reptile 35 2 ## ## Accuracy : 0.3 ## 95% CI : (0.179, 0.446) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.029 ## ## Mcnemar&#39;s Test P-Value : 9.08e-09 ## ## Sensitivity : 1.0000 ## Specificity : 0.2708 ## Pos Pred Value : 0.0541 ## Neg Pred Value : 1.0000 ## Prevalence : 0.0400 ## Detection Rate : 0.0400 ## Detection Prevalence : 0.7400 ## Balanced Accuracy : 0.6354 ## ## &#39;Positive&#39; Class : reptile ## Note that accuracy goes down and is below the no information rate. However, both measures are based on the idea that all errors have the same cost. What is important is that we are now able to find more reptiles. 3.7.3.2 Plot the ROC Curve Since we have a binary classification problem and a classifier that predicts a probability for an observation to be a reptile, we can also use a receiver operating characteristic (ROC) curve. For the ROC curve all different cutoff thresholds for the probability are used and then connected with a line. The area under the curve represents a single number for how well the classifier works (the closer to one, the better). library(&quot;pROC&quot;) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var r &lt;- roc(testing_reptile$type == &quot;reptile&quot;, prob[,&quot;reptile&quot;]) ## Setting levels: control = FALSE, case = TRUE ## Setting direction: controls &lt; cases r ## ## Call: ## roc.default(response = testing_reptile$type == &quot;reptile&quot;, predictor = prob[, &quot;reptile&quot;]) ## ## Data: prob[, &quot;reptile&quot;] in 48 controls (testing_reptile$type == &quot;reptile&quot; FALSE) &lt; 2 cases (testing_reptile$type == &quot;reptile&quot; TRUE). ## Area under the curve: 0.766 ggroc(r) + geom_abline(intercept = 1, slope = 1, color = &quot;darkgrey&quot;) 3.7.4 Option 4: Use a Cost-Sensitive Classifier The implementation of CART in rpart can use a cost matrix for making splitting decisions (as parameter loss). The matrix has the form TP FP FN TN TP and TN have to be 0. We make FN very expensive (100). cost &lt;- matrix(c( 0, 1, 100, 0 ), byrow = TRUE, nrow = 2) cost ## [,1] [,2] ## [1,] 0 1 ## [2,] 100 0 fit &lt;- training_reptile %&gt;% train(type ~ ., data = ., method = &quot;rpart&quot;, parms = list(loss = cost), trControl = trainControl(method = &quot;cv&quot;)) The warning “There were missing values in resampled performance measures” means that some folds did not contain any reptiles (because of the class imbalance) and thus the performance measures could not be calculates. fit ## CART ## ## 51 samples ## 16 predictors ## 2 classes: &#39;nonreptile&#39;, &#39;reptile&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 46, 46, 46, 45, 46, 45, ... ## Resampling results: ## ## Accuracy Kappa ## 0.477 -0.0304 ## ## Tuning parameter &#39;cp&#39; was held constant at a value of 0 rpart.plot(fit$finalModel, extra = 2) confusionMatrix(data = predict(fit, testing_reptile), ref = testing_reptile$type, positive = &quot;reptile&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction nonreptile reptile ## nonreptile 39 0 ## reptile 9 2 ## ## Accuracy : 0.82 ## 95% CI : (0.686, 0.914) ## No Information Rate : 0.96 ## P-Value [Acc &gt; NIR] : 0.99998 ## ## Kappa : 0.257 ## ## Mcnemar&#39;s Test P-Value : 0.00766 ## ## Sensitivity : 1.000 ## Specificity : 0.812 ## Pos Pred Value : 0.182 ## Neg Pred Value : 1.000 ## Prevalence : 0.040 ## Detection Rate : 0.040 ## Detection Prevalence : 0.220 ## Balanced Accuracy : 0.906 ## ## &#39;Positive&#39; Class : reptile ## The high cost for false negatives results in a classifier that does not miss any reptile. Note: Using a cost-sensitive classifier is often the best option. Unfortunately, the most classification algorithms (or their implementation) do not have the ability to consider misclassification cost. References Kuhn, M. (2021). Caret: Classification and regression training. https://github.com/topepo/caret/ Leisch, F., &amp; Dimitriadou., E. (2021). Mlbench: Machine learning benchmark problems. https://CRAN.R-project.org/package=mlbench Milborrow, S. (2020). Rpart.plot: Plot rpart models: An enhanced version of plot.rpart. http://www.milbo.org/rpart-plot/index.html Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., &amp; Müller, M. (2021). pROC: Display and analyze ROC curves. http://expasy.org/tools/pROC/ Romanski, P., Kotthoff, L., &amp; Schratz, P. (2021). FSelector: Selecting attributes. https://github.com/larskotthoff/fselector Sarkar, D. (2021). Lattice: Trellis graphics for r. http://lattice.r-forge.r-project.org/ Tan, P.-N., Steinbach, M. S., &amp; Kumar, V. (2005). Introduction to data mining (1st Edition). Addison-Wesley. https://www-users.cs.umn.edu/~kumar001/dmbook/firsted.php Therneau, T., &amp; Atkinson, B. (2019). Rpart: Recursive partitioning and regression trees. https://CRAN.R-project.org/package=rpart Tillé, Y., &amp; Matei, A. (2021). Sampling: Survey sampling. https://CRAN.R-project.org/package=sampling Wickham, H. (2021c). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse "],["classification-alternative-techniques.html", "Chapter 4 Classification: Alternative Techniques 4.1 Training and Test Data 4.2 Fitting Different Classification Models to the Training Data 4.3 Comparing Models 4.4 Applying the Chosen Model to the Test Data 4.5 Decision Boundaries 4.6 More Information", " Chapter 4 Classification: Alternative Techniques Packages used for this chapter: C50 (Kuhn &amp; Quinlan, 2021), caret (Kuhn, 2021), e1071 (Meyer et al., 2021), keras (Allaire &amp; Chollet, 2021), lattice (Sarkar, 2021), MASS (Ripley, 2021a), mlbench (Leisch &amp; Dimitriadou., 2021), nnet (Ripley, 2021b), randomForest (Breiman et al., 2018), rpart (Therneau &amp; Atkinson, 2019), RWeka (Hornik, 2020), scales (Wickham &amp; Seidel, 2020), tidyverse (Wickham, 2021c) We will use tidyverse to prepare the data. library(tidyverse) Show fewer digits options(digits=3) 4.1 Training and Test Data We will use the Zoo dataset which is included in the R package mlbench (you may have to install it). The Zoo dataset containing 17 (mostly logical) variables on different 101 animals as a data frame with 17 columns (hair, feathers, eggs, milk, airborne, aquatic, predator, toothed, backbone, breathes, venomous, fins, legs, tail, domestic, catsize, type). We convert the data frame into a tidyverse tibble (optional). data(Zoo, package=&quot;mlbench&quot;) Zoo &lt;- as_tibble(Zoo) Zoo ## # A tibble: 101 x 17 ## hair feathers eggs milk airborne aquatic predator ## &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 TRUE FALSE FALSE TRUE FALSE FALSE TRUE ## 2 TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## 3 FALSE FALSE TRUE FALSE FALSE TRUE TRUE ## 4 TRUE FALSE FALSE TRUE FALSE FALSE TRUE ## 5 TRUE FALSE FALSE TRUE FALSE FALSE TRUE ## 6 TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## 7 TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## 8 FALSE FALSE TRUE FALSE FALSE TRUE FALSE ## 9 FALSE FALSE TRUE FALSE FALSE TRUE TRUE ## 10 TRUE FALSE FALSE TRUE FALSE FALSE FALSE ## # … with 91 more rows, and 10 more variables: ## # toothed &lt;lgl&gt;, backbone &lt;lgl&gt;, breathes &lt;lgl&gt;, ## # venomous &lt;lgl&gt;, fins &lt;lgl&gt;, legs &lt;int&gt;, ## # tail &lt;lgl&gt;, domestic &lt;lgl&gt;, catsize &lt;lgl&gt;, ## # type &lt;fct&gt; We will use the package caret to make preparing training sets and building classification (and regression) models easier. A great cheat sheet can be found here. library(caret) Use multi-core support for cross-validation. Note: It is commented out because it does not work with rJava used in RWeka below. ##library(doMC, quietly = TRUE) ##registerDoMC(cores = 4) ##getDoParWorkers() Test data is not used in the model building process and needs to be set aside purely for testing the model after it is completely built. Here I use 80% for training. inTrain &lt;- createDataPartition(y = Zoo$type, p = .8, list = FALSE) Zoo_train &lt;- Zoo %&gt;% slice(inTrain) Zoo_test &lt;- Zoo %&gt;% slice(-inTrain) 4.2 Fitting Different Classification Models to the Training Data Create a fixed sampling scheme (10-folds) so we can compare the fitted models later. train_index &lt;- createFolds(Zoo_train$type, k = 10) The fixed folds are used in train() with the argument trControl = trainControl(method = \"cv\", indexOut = train_index)). If you don’t need fixed folds, then remove indexOut = train_index in the code below. For help with building models in caret see: ? train Note: Be careful if you have many NA values in your data. train() and cross-validation many fail in some cases. If that is the case then you can remove features (columns) which have many NAs, omit NAs using na.omit() or use imputation to replace them with reasonable values (e.g., by the feature mean or via kNN). Highly imbalanced datasets are also problematic since there is a chance that a fold does not contain examples of each class leading to a hard to understand error message. 4.2.1 Conditional Inference Tree (Decision Tree) ctreeFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;ctree&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) ctreeFit ## Conditional Inference Tree ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 75, 76, 74, 74, 76, 74, ... ## Resampling results across tuning parameters: ## ## mincriterion Accuracy Kappa ## 0.010 0.808 0.747 ## 0.255 0.808 0.747 ## 0.500 0.808 0.747 ## 0.745 0.808 0.747 ## 0.990 0.808 0.747 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was mincriterion ## = 0.99. plot(ctreeFit$finalModel) The final model can be directly used for predict() predict(ctreeFit, head(Zoo_test)) ## [1] mammal mollusc.et.al bird ## [4] mammal mollusc.et.al bird ## 7 Levels: mammal bird reptile fish ... mollusc.et.al 4.2.2 C 4.5 Decision Tree library(RWeka) C45Fit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;J48&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) C45Fit ## C4.5-like Trees ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 76, 73, 74, 74, 76, 76, ... ## Resampling results across tuning parameters: ## ## C M Accuracy Kappa ## 0.010 1 0.978 0.971 ## 0.010 2 0.978 0.971 ## 0.010 3 0.978 0.971 ## 0.010 4 0.907 0.879 ## 0.010 5 0.918 0.893 ## 0.133 1 0.978 0.971 ## 0.133 2 0.978 0.971 ## 0.133 3 0.978 0.971 ## 0.133 4 0.907 0.879 ## 0.133 5 0.918 0.893 ## 0.255 1 0.989 0.985 ## 0.255 2 0.989 0.985 ## 0.255 3 0.978 0.971 ## 0.255 4 0.907 0.879 ## 0.255 5 0.918 0.893 ## 0.378 1 0.989 0.985 ## 0.378 2 0.989 0.985 ## 0.378 3 0.978 0.971 ## 0.378 4 0.907 0.879 ## 0.378 5 0.918 0.893 ## 0.500 1 0.989 0.985 ## 0.500 2 0.989 0.985 ## 0.500 3 0.978 0.971 ## 0.500 4 0.907 0.879 ## 0.500 5 0.918 0.893 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final values used for the model were C = 0.255 ## and M = 1. C45Fit$finalModel ## J48 pruned tree ## ------------------ ## ## feathersTRUE &lt;= 0 ## | milkTRUE &lt;= 0 ## | | toothedTRUE &lt;= 0 ## | | | airborneTRUE &lt;= 0 ## | | | | predatorTRUE &lt;= 0 ## | | | | | legs &lt;= 2: mollusc.et.al (2.0) ## | | | | | legs &gt; 2: insect (2.0) ## | | | | predatorTRUE &gt; 0: mollusc.et.al (6.0) ## | | | airborneTRUE &gt; 0: insect (5.0) ## | | toothedTRUE &gt; 0 ## | | | finsTRUE &lt;= 0 ## | | | | aquaticTRUE &lt;= 0: reptile (3.0) ## | | | | aquaticTRUE &gt; 0 ## | | | | | eggsTRUE &lt;= 0: reptile (1.0) ## | | | | | eggsTRUE &gt; 0: amphibian (4.0) ## | | | finsTRUE &gt; 0: fish (11.0) ## | milkTRUE &gt; 0: mammal (33.0) ## feathersTRUE &gt; 0: bird (16.0) ## ## Number of Leaves : 10 ## ## Size of the tree : 19 4.2.3 K-Nearest Neighbors Note: kNN uses Euclidean distance, so data should be standardized (scaled) first. Here legs are measured between 0 and 6 while all other variables are between 0 and 1. Scaling can be directly performed as preprocessing in train using the parameter preProcess = \"scale\". knnFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;knn&quot;, data = ., preProcess = &quot;scale&quot;, tuneLength = 5, tuneGrid=data.frame(k = 1:10), trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) knnFit ## k-Nearest Neighbors ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## Pre-processing: scaled (16) ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 74, 73, 76, 74, 74, 75, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 1.000 1.000 ## 2 0.978 0.971 ## 3 0.967 0.957 ## 4 0.943 0.926 ## 5 0.965 0.954 ## 6 0.916 0.891 ## 7 0.883 0.850 ## 8 0.872 0.835 ## 9 0.883 0.848 ## 10 0.908 0.881 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was k = 1. knnFit$finalModel ## 1-nearest neighbor model ## Training set outcome distribution: ## ## mammal bird reptile ## 33 16 4 ## fish amphibian insect ## 11 4 7 ## mollusc.et.al ## 8 4.2.4 PART (Rule-based classifier) rulesFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;PART&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) rulesFit ## Rule-Based Classifier ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 77, 72, 77, 74, 74, 73, ... ## Resampling results across tuning parameters: ## ## threshold pruned Accuracy Kappa ## 0.010 yes 0.965 0.955 ## 0.010 no 0.988 0.984 ## 0.133 yes 0.965 0.955 ## 0.133 no 0.988 0.984 ## 0.255 yes 0.965 0.955 ## 0.255 no 0.988 0.984 ## 0.378 yes 0.965 0.955 ## 0.378 no 0.988 0.984 ## 0.500 yes 0.965 0.955 ## 0.500 no 0.988 0.984 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final values used for the model were threshold ## = 0.5 and pruned = no. rulesFit$finalModel ## PART decision list ## ------------------ ## ## feathersTRUE &lt;= 0 AND ## milkTRUE &gt; 0: mammal (33.0) ## ## feathersTRUE &gt; 0: bird (16.0) ## ## toothedTRUE &lt;= 0 AND ## airborneTRUE &lt;= 0 AND ## predatorTRUE &gt; 0: mollusc.et.al (6.0) ## ## toothedTRUE &lt;= 0 AND ## legs &gt; 2: insect (7.0) ## ## finsTRUE &gt; 0: fish (11.0) ## ## toothedTRUE &gt; 0 AND ## aquaticTRUE &lt;= 0: reptile (3.0) ## ## aquaticTRUE &gt; 0 AND ## venomousTRUE &lt;= 0: amphibian (3.0) ## ## aquaticTRUE &lt;= 0: mollusc.et.al (2.0) ## ## : reptile (2.0/1.0) ## ## Number of Rules : 9 4.2.5 Linear Support Vector Machines svmFit &lt;- Zoo_train %&gt;% train(type ~., method = &quot;svmLinear&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) svmFit ## Support Vector Machines with Linear Kernel ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 73, 75, 75, 74, 74, 76, ... ## Resampling results: ## ## Accuracy Kappa ## 1 1 ## ## Tuning parameter &#39;C&#39; was held constant at a value of 1 svmFit$finalModel ## Support Vector Machine object of class &quot;ksvm&quot; ## ## SV type: C-svc (classification) ## parameter : cost C = 1 ## ## Linear (vanilla) kernel function. ## ## Number of Support Vectors : 44 ## ## Objective Function Value : -0.143 -0.198 -0.148 -0.175 -0.0945 -0.104 -0.19 -0.0814 -0.154 -0.0917 -0.115 -0.177 -0.568 -0.104 -0.15 -0.119 -0.0478 -0.083 -0.123 -0.148 -0.58 ## Training error : 0 4.2.6 Random Forest randomForestFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;rf&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index)) randomForestFit ## Random Forest ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 74, 76, 75, 74, 73, 76, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.976 0.968 ## 5 0.976 0.968 ## 9 0.976 0.968 ## 12 0.965 0.954 ## 16 0.976 0.969 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final value used for the model was mtry = 2. randomForestFit$finalModel ## ## Call: ## randomForest(x = x, y = y, mtry = min(param$mtry, ncol(x))) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 3.61% ## Confusion matrix: ## mammal bird reptile fish amphibian ## mammal 33 0 0 0 0 ## bird 0 16 0 0 0 ## reptile 0 0 2 1 1 ## fish 0 0 0 11 0 ## amphibian 0 0 0 0 4 ## insect 0 0 0 0 0 ## mollusc.et.al 0 0 0 0 0 ## insect mollusc.et.al class.error ## mammal 0 0 0.000 ## bird 0 0 0.000 ## reptile 0 0 0.500 ## fish 0 0 0.000 ## amphibian 0 0 0.000 ## insect 7 0 0.000 ## mollusc.et.al 1 7 0.125 4.2.7 Gradient Boosted Decision Trees (xgboost) xgboostFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;xgbTree&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index), tuneGrid = expand.grid( nrounds = 20, max_depth = 3, colsample_bytree = .6, eta = 0.1, gamma=0, min_child_weight = 1, subsample = .5 )) xgboostFit ## eXtreme Gradient Boosting ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 76, 75, 75, 74, 76, 74, ... ## Resampling results: ## ## Accuracy Kappa ## 0.976 0.969 ## ## Tuning parameter &#39;nrounds&#39; was held constant at ## a value of 1 ## Tuning parameter &#39;subsample&#39; was ## held constant at a value of 0.5 xgboostFit$finalModel ## ##### xgb.Booster ## raw: 83.5 Kb ## call: ## xgboost::xgb.train(params = list(eta = param$eta, max_depth = param$max_depth, ## gamma = param$gamma, colsample_bytree = param$colsample_bytree, ## min_child_weight = param$min_child_weight, subsample = param$subsample), ## data = x, nrounds = param$nrounds, num_class = length(lev), ## objective = &quot;multi:softprob&quot;) ## params (as set within xgb.train): ## eta = &quot;0.1&quot;, max_depth = &quot;3&quot;, gamma = &quot;0&quot;, colsample_bytree = &quot;0.6&quot;, min_child_weight = &quot;1&quot;, subsample = &quot;0.5&quot;, num_class = &quot;7&quot;, objective = &quot;multi:softprob&quot;, validate_parameters = &quot;TRUE&quot; ## xgb.attributes: ## niter ## callbacks: ## cb.print.evaluation(period = print_every_n) ## # of features: 16 ## niter: 20 ## nfeatures : 16 ## xNames : hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE ## problemType : Classification ## tuneValue : ## nrounds max_depth eta gamma colsample_bytree ## 1 20 3 0.1 0 0.6 ## min_child_weight subsample ## 1 1 0.5 ## obsLevels : mammal bird reptile fish amphibian insect mollusc.et.al ## param : ## list() 4.2.8 Artificial Neural Network nnetFit &lt;- Zoo_train %&gt;% train(type ~ ., method = &quot;nnet&quot;, data = ., tuneLength = 5, trControl = trainControl(method = &quot;cv&quot;, indexOut = train_index), trace = FALSE) nnetFit ## Neural Network ## ## 83 samples ## 16 predictors ## 7 classes: &#39;mammal&#39;, &#39;bird&#39;, &#39;reptile&#39;, &#39;fish&#39;, &#39;amphibian&#39;, &#39;insect&#39;, &#39;mollusc.et.al&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 75, 77, 72, 75, 75, 76, ... ## Resampling results across tuning parameters: ## ## size decay Accuracy Kappa ## 1 0e+00 0.694 0.558 ## 1 1e-04 0.807 0.728 ## 1 1e-03 0.892 0.852 ## 1 1e-02 0.825 0.766 ## 1 1e-01 0.727 0.633 ## 3 0e+00 0.954 0.939 ## 3 1e-04 0.989 0.986 ## 3 1e-03 0.989 0.986 ## 3 1e-02 0.989 0.986 ## 3 1e-01 0.989 0.986 ## 5 0e+00 0.939 0.917 ## 5 1e-04 0.965 0.954 ## 5 1e-03 0.989 0.986 ## 5 1e-02 0.989 0.986 ## 5 1e-01 0.989 0.986 ## 7 0e+00 0.989 0.986 ## 7 1e-04 0.989 0.986 ## 7 1e-03 0.989 0.986 ## 7 1e-02 1.000 1.000 ## 7 1e-01 0.989 0.986 ## 9 0e+00 0.989 0.986 ## 9 1e-04 0.989 0.986 ## 9 1e-03 0.989 0.986 ## 9 1e-02 0.989 0.986 ## 9 1e-01 1.000 1.000 ## ## Accuracy was used to select the optimal model ## using the largest value. ## The final values used for the model were size = 7 ## and decay = 0.01. nnetFit$finalModel ## a 16-7-7 network with 175 weights ## inputs: hairTRUE feathersTRUE eggsTRUE milkTRUE airborneTRUE aquaticTRUE predatorTRUE toothedTRUE backboneTRUE breathesTRUE venomousTRUE finsTRUE legs tailTRUE domesticTRUE catsizeTRUE ## output(s): .outcome ## options were - softmax modelling decay=0.01 4.3 Comparing Models Collect the performance metrics from the models trained on the same data. resamps &lt;- resamples(list( ctree = ctreeFit, C45 = C45Fit, SVM = svmFit, KNN = knnFit, rules = rulesFit, randomForest = randomForestFit, xgboost = xgboostFit, NeuralNet = nnetFit )) resamps ## ## Call: ## resamples.default(x = list(ctree = ctreeFit, C45 ## = rulesFit, randomForest = randomForestFit, xgboost ## = xgboostFit, NeuralNet = nnetFit)) ## ## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet ## Number of resamples: 10 ## Performance metrics: Accuracy, Kappa ## Time estimates for: everything, final model fit Calculate summary statistics summary(resamps) ## ## Call: ## summary.resamples(object = resamps) ## ## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet ## Number of resamples: 10 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## ctree 0.750 0.778 0.778 0.808 0.851 0.889 ## C45 0.889 1.000 1.000 0.989 1.000 1.000 ## SVM 1.000 1.000 1.000 1.000 1.000 1.000 ## KNN 1.000 1.000 1.000 1.000 1.000 1.000 ## rules 0.875 1.000 1.000 0.988 1.000 1.000 ## randomForest 0.875 1.000 1.000 0.976 1.000 1.000 ## xgboost 0.875 1.000 1.000 0.976 1.000 1.000 ## NeuralNet 1.000 1.000 1.000 1.000 1.000 1.000 ## NA&#39;s ## ctree 0 ## C45 0 ## SVM 0 ## KNN 0 ## rules 0 ## randomForest 0 ## xgboost 0 ## NeuralNet 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## ctree 0.673 0.701 0.723 0.747 0.798 0.852 ## C45 0.850 1.000 1.000 0.985 1.000 1.000 ## SVM 1.000 1.000 1.000 1.000 1.000 1.000 ## KNN 1.000 1.000 1.000 1.000 1.000 1.000 ## rules 0.837 1.000 1.000 0.984 1.000 1.000 ## randomForest 0.833 1.000 1.000 0.968 1.000 1.000 ## xgboost 0.833 1.000 1.000 0.969 1.000 1.000 ## NeuralNet 1.000 1.000 1.000 1.000 1.000 1.000 ## NA&#39;s ## ctree 0 ## C45 0 ## SVM 0 ## KNN 0 ## rules 0 ## randomForest 0 ## xgboost 0 ## NeuralNet 0 library(lattice) bwplot(resamps, layout = c(3, 1)) Perform inference about differences between models. For each metric, all pair-wise differences are computed and tested to assess if the difference is equal to zero. By default Bonferroni correction for multiple comparison is used. Differences are shown in the upper triangle and p-values are in the lower triangle. difs &lt;- diff(resamps) difs ## ## Call: ## diff.resamples(x = resamps) ## ## Models: ctree, C45, SVM, KNN, rules, randomForest, xgboost, NeuralNet ## Metrics: Accuracy, Kappa ## Number of differences: 28 ## p-value adjustment: bonferroni summary(difs) ## ## Call: ## summary.diff.resamples(object = difs) ## ## p-value adjustment: bonferroni ## Upper diagonal: estimates of the difference ## Lower diagonal: p-value for H0: difference = 0 ## ## Accuracy ## ctree C45 SVM KNN ## ctree -0.18095 -0.19206 -0.19206 ## C45 0.000109 -0.01111 -0.01111 ## SVM 3.49e-05 1.000000 0.00000 ## KNN 3.49e-05 1.000000 NA ## rules 5.75e-05 1.000000 1.000000 1.000000 ## randomForest 0.000126 1.000000 1.000000 1.000000 ## xgboost 0.001617 1.000000 1.000000 1.000000 ## NeuralNet 3.49e-05 1.000000 NA NA ## rules randomForest xgboost NeuralNet ## ctree -0.17956 -0.16845 -0.16845 -0.19206 ## C45 0.00139 0.01250 0.01250 -0.01111 ## SVM 0.01250 0.02361 0.02361 0.00000 ## KNN 0.01250 0.02361 0.02361 0.00000 ## rules 0.01111 0.01111 -0.01250 ## randomForest 1.000000 0.00000 -0.02361 ## xgboost 1.000000 1.000000 -0.02361 ## NeuralNet 1.000000 1.000000 1.000000 ## ## Kappa ## ctree C45 SVM KNN ## ctree -0.238389 -0.253389 -0.253389 ## C45 6.36e-05 -0.015000 -0.015000 ## SVM 2.08e-05 1.00000 0.000000 ## KNN 2.08e-05 1.00000 NA ## rules 3.70e-05 1.00000 1.00000 1.00000 ## randomForest 7.76e-05 1.00000 1.00000 1.00000 ## xgboost 0.00124 1.00000 1.00000 1.00000 ## NeuralNet 2.08e-05 1.00000 NA NA ## rules randomForest xgboost ## ctree -0.237063 -0.221723 -0.222437 ## C45 0.001327 0.016667 0.015952 ## SVM 0.016327 0.031667 0.030952 ## KNN 0.016327 0.031667 0.030952 ## rules 0.015340 0.014626 ## randomForest 1.00000 -0.000714 ## xgboost 1.00000 1.00000 ## NeuralNet 1.00000 1.00000 1.00000 ## NeuralNet ## ctree -0.253389 ## C45 -0.015000 ## SVM 0.000000 ## KNN 0.000000 ## rules -0.016327 ## randomForest -0.031667 ## xgboost -0.030952 ## NeuralNet All perform similarly well except ctree (differences in the first row are negative and the p-values in the first column are &lt;.05 indicating that the null-hypothesis of a difference of 0 can be rejected). 4.4 Applying the Chosen Model to the Test Data Most models do similarly well on the data. We choose here the random forest model. pr &lt;- predict(randomForestFit, Zoo_test) pr ## [1] mammal mollusc.et.al bird ## [4] mammal insect bird ## [7] mammal mollusc.et.al mammal ## [10] mammal bird bird ## [13] fish mammal fish ## [16] mammal bird mammal ## 7 Levels: mammal bird reptile fish ... mollusc.et.al Calculate the confusion matrix for the held-out test data. confusionMatrix(pr, reference = Zoo_test$type) ## Confusion Matrix and Statistics ## ## Reference ## Prediction mammal bird reptile fish amphibian ## mammal 8 0 0 0 0 ## bird 0 4 1 0 0 ## reptile 0 0 0 0 0 ## fish 0 0 0 2 0 ## amphibian 0 0 0 0 0 ## insect 0 0 0 0 0 ## mollusc.et.al 0 0 0 0 0 ## Reference ## Prediction insect mollusc.et.al ## mammal 0 0 ## bird 0 0 ## reptile 0 0 ## fish 0 0 ## amphibian 0 0 ## insect 1 0 ## mollusc.et.al 0 2 ## ## Overall Statistics ## ## Accuracy : 0.944 ## 95% CI : (0.727, 0.999) ## No Information Rate : 0.444 ## P-Value [Acc &gt; NIR] : 1.08e-05 ## ## Kappa : 0.922 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: mammal Class: bird ## Sensitivity 1.000 1.000 ## Specificity 1.000 0.929 ## Pos Pred Value 1.000 0.800 ## Neg Pred Value 1.000 1.000 ## Prevalence 0.444 0.222 ## Detection Rate 0.444 0.222 ## Detection Prevalence 0.444 0.278 ## Balanced Accuracy 1.000 0.964 ## Class: reptile Class: fish ## Sensitivity 0.0000 1.000 ## Specificity 1.0000 1.000 ## Pos Pred Value NaN 1.000 ## Neg Pred Value 0.9444 1.000 ## Prevalence 0.0556 0.111 ## Detection Rate 0.0000 0.111 ## Detection Prevalence 0.0000 0.111 ## Balanced Accuracy 0.5000 1.000 ## Class: amphibian Class: insect ## Sensitivity NA 1.0000 ## Specificity 1 1.0000 ## Pos Pred Value NA 1.0000 ## Neg Pred Value NA 1.0000 ## Prevalence 0 0.0556 ## Detection Rate 0 0.0556 ## Detection Prevalence 0 0.0556 ## Balanced Accuracy NA 1.0000 ## Class: mollusc.et.al ## Sensitivity 1.000 ## Specificity 1.000 ## Pos Pred Value 1.000 ## Neg Pred Value 1.000 ## Prevalence 0.111 ## Detection Rate 0.111 ## Detection Prevalence 0.111 ## Balanced Accuracy 1.000 4.5 Decision Boundaries Classifiers create decision boundaries to discriminate between classes. Different classifiers are able to create different shapes of decision boundaries (e.g., some are strictly linear) and thus some classifiers may perform better for certain datasets. This page visualizes the decision boundaries found by several popular classification methods. The following plot adds the decision boundary by evaluating the classifier at evenly spaced grid points. Note that low resolution (to make evaluation faster) will make the decision boundary look like it has small steps even if it is a (straight) line. library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor library(tidyverse) library(ggplot2) library(caret) decisionplot &lt;- function(model, x, cl = NULL, predict_type = &quot;class&quot;, resolution = 100) { if(!is.null(cl)) { x_data &lt;- x %&gt;% dplyr::select(-all_of(cl)) cl &lt;- x %&gt;% pull(cl) } else cl &lt;- 1 k &lt;- length(unique(cl)) # resubstitution accuracy prediction &lt;- predict(model, x_data, type = predict_type) if(is.list(prediction)) prediction &lt;- prediction$class if(is.numeric(prediction)) prediction &lt;- factor(prediction, labels = levels(cl)) else prediction &lt;- factor(prediction, levels = levels(cl)) cm &lt;- confusionMatrix(data = prediction, reference = cl) acc &lt;- cm$overall[&quot;Accuracy&quot;] # evaluate model on a grid r &lt;- sapply(x[, 1:2], range, na.rm = TRUE) xs &lt;- seq(r[1,1], r[2,1], length.out = resolution) ys &lt;- seq(r[1,2], r[2,2], length.out = resolution) g &lt;- cbind(rep(xs, each = resolution), rep(ys, time = resolution)) colnames(g) &lt;- colnames(r) g &lt;- as_tibble(g) ### guess how to get class labels from predict ### (unfortunately not very consistent between models) prediction &lt;- predict(model, g, type = predict_type) if(is.list(prediction)) prediction &lt;- prediction$class if(is.numeric(prediction)) prediction &lt;- factor(prediction, labels = levels(cl)) else prediction &lt;- factor(prediction, levels = levels(cl)) g &lt;- g %&gt;% add_column(prediction) ggplot(g, mapping = aes_string( x = colnames(g)[1], y = colnames(g)[2])) + geom_tile(mapping = aes(fill = prediction)) + geom_point(data = x, mapping = aes_string( x = colnames(x)[1], y = colnames(x)[2], shape = colnames(x)[3]), alpha = .5) + labs(subtitle = paste(&quot;Training accuracy:&quot;, round(acc, 2))) } 4.5.1 Iris Dataset For easier visualization, we use on two dimensions of the Iris dataset. set.seed(1000) data(iris) iris &lt;- as_tibble(iris) ### Three classes (MASS also has a select function) x &lt;- iris %&gt;% dplyr::select(Sepal.Length, Sepal.Width, Species) x ## # A tibble: 150 x 3 ## Sepal.Length Sepal.Width Species ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 5.1 3.5 setosa ## 2 4.9 3 setosa ## 3 4.7 3.2 setosa ## 4 4.6 3.1 setosa ## 5 5 3.6 setosa ## 6 5.4 3.9 setosa ## 7 4.6 3.4 setosa ## 8 5 3.4 setosa ## 9 4.4 2.9 setosa ## 10 4.9 3.1 setosa ## # … with 140 more rows ggplot(x, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_point() Note: There is some overplotting and you could use geom_jitter() instead of geom_point(). 4.5.1.1 K-Nearest Neighbors Classifier library(caret) model &lt;- x %&gt;% knn3(Species ~ ., data = ., k = 1) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;kNN (1 neighbor)&quot;) model &lt;- x %&gt;% knn3(Species ~ ., data = ., k = 10) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;kNN (10 neighbor)&quot;) 4.5.1.2 Naive Bayes Classifier library(e1071) model &lt;- x %&gt;% naiveBayes(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;Naive Bayes&quot;) 4.5.1.3 Linear Discriminant Analysis library(MASS) ## ## Attaching package: &#39;MASS&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## select model &lt;- x %&gt;% lda(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;LDA&quot;) 4.5.1.4 Multinomial Logistic Regression (implemented in nnet) Multinomial logistic regression is an extension of logistic regression to problems with more than two classes. library(nnet) model &lt;- x %&gt;% multinom(Species ~., data = .) ## # weights: 12 (6 variable) ## initial value 164.791843 ## iter 10 value 62.715967 ## iter 20 value 59.808291 ## iter 30 value 55.445984 ## iter 40 value 55.375704 ## iter 50 value 55.346472 ## iter 60 value 55.301707 ## iter 70 value 55.253532 ## iter 80 value 55.243230 ## iter 90 value 55.230241 ## iter 100 value 55.212479 ## final value 55.212479 ## stopped after 100 iterations decisionplot(model, x, cl = &quot;Species&quot;) + labs(titel = &quot;Multinomial Logistic Regression&quot;) 4.5.1.5 Decision Trees library(&quot;rpart&quot;) model &lt;- x %&gt;% rpart(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;CART&quot;) model &lt;- x %&gt;% rpart(Species ~ ., data = ., control = rpart.control(cp = 0.001, minsplit = 1)) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;CART (overfitting)&quot;) library(C50) model &lt;- x %&gt;% C5.0(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;C5.0&quot;) library(randomForest) ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin model &lt;- x %&gt;% randomForest(Species ~ ., data = .) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;Random Forest&quot;) 4.5.1.6 SVM library(e1071) model &lt;- x %&gt;% svm(Species ~ ., data = ., kernel = &quot;linear&quot;) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;SVM (linear kernel)&quot;) model &lt;- x %&gt;% svm(Species ~ ., data = ., kernel = &quot;radial&quot;) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;SVM (radial kernel)&quot;) model &lt;- x %&gt;% svm(Species ~ ., data = ., kernel = &quot;polynomial&quot;) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;SVM (polynomial kernel)&quot;) model &lt;- x %&gt;% svm(Species ~ ., data = ., kernel = &quot;sigmoid&quot;) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;SVM (sigmoid kernel)&quot;) 4.5.1.7 Single Layer Feed-forward Neural Networks library(nnet) model &lt;-x %&gt;% nnet(Species ~ ., data = ., size = 1, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;NN (1 neuron)&quot;) model &lt;-x %&gt;% nnet(Species ~ ., data = ., size = 2, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;NN (2 neurons)&quot;) model &lt;-x %&gt;% nnet(Species ~ ., data = ., size = 4, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;NN (4 neurons)&quot;) model &lt;-x %&gt;% nnet(Species ~ ., data = ., size = 10, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;NN (10 neurons)&quot;) 4.5.1.8 Deep Learning with keras library(keras) define predict so it works with decision plot predict.keras.engine.training.Model &lt;- function(object, newdata, ...) predict_classes(object, as.matrix(newdata)) Choices are the activation function, number of layers, number of units per layer and the optimizer. A L2 regularizer is used for the dense layer weights to reduce overfitting. The output is a categorical class value, therefore the output layer uses the softmax activation function, the loss is categorical crossentropy, and the metric is accuracy. model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 10, activation = &#39;relu&#39;, input_shape = c(2), kernel_regularizer=regularizer_l2(l=0.01)) %&gt;% layer_dense(units = 4, activation = &#39;softmax&#39;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39;) history &lt;- model %&gt;% fit( as.matrix(x[,1:2]), x %&gt;% pull(3) %&gt;% as.integer %&gt;% to_categorical(), epochs = 100, batch_size = 10 ) history ## ## Final epoch (plot to see history): ## loss: 0.6864 ## accuracy: 0.6933 decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;keras (relu activation)&quot;) model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 10, activation = &#39;tanh&#39;, input_shape = c(2), kernel_regularizer = regularizer_l2(l = 0.01)) %&gt;% layer_dense(units = 4, activation = &#39;softmax&#39;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39;) history &lt;- model %&gt;% fit( as.matrix(x[,1:2]), x %&gt;% pull(3) %&gt;% as.integer %&gt;% to_categorical(), epochs = 100, batch_size = 10 ) history ## ## Final epoch (plot to see history): ## loss: 0.6111 ## accuracy: 0.78 decisionplot(model, x, cl = &quot;Species&quot;) + labs(title = &quot;keras (tanh activation)&quot;) 4.5.2 Circle Dataset This set is not linearly separable! set.seed(1000) library(mlbench) x &lt;- mlbench.circle(500) ###x &lt;- mlbench.cassini(500) ###x &lt;- mlbench.spirals(500, sd = .1) ###x &lt;- mlbench.smiley(500) x &lt;- cbind(as.data.frame(x$x), factor(x$classes)) colnames(x) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;class&quot;) x &lt;- as_tibble(x) x ## # A tibble: 500 x 3 ## x y class ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.344 0.448 1 ## 2 0.518 0.915 2 ## 3 -0.772 -0.0913 1 ## 4 0.382 0.412 1 ## 5 0.0328 0.438 1 ## 6 -0.865 -0.354 2 ## 7 0.477 0.640 2 ## 8 0.167 -0.809 2 ## 9 -0.568 -0.281 1 ## 10 -0.488 0.638 2 ## # … with 490 more rows ggplot(x, aes(x = x, y = y, color = class)) + geom_point() 4.5.2.1 K-Nearest Neighbors Classifier library(caret) model &lt;- x %&gt;% knn3(class ~ ., data = ., k = 1) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;kNN (1 neighbor)&quot;) model &lt;- x %&gt;% knn3(class ~ ., data = ., k = 10) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;kNN (10 neighbor)&quot;) 4.5.2.2 Naive Bayes Classifier library(e1071) model &lt;- x %&gt;% naiveBayes(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;naive Bayes&quot;) 4.5.2.3 Linear Discriminant Analysis library(MASS) model &lt;- x %&gt;% lda(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;LDA&quot;) 4.5.2.4 Multinomial Logistic Regression (implemented in nnet) Multinomial logistic regression is an extension of logistic regression to problems with more than two classes. library(nnet) model &lt;- x %&gt;% multinom(class ~., data = .) ## # weights: 4 (3 variable) ## initial value 346.573590 ## final value 346.308371 ## converged decisionplot(model, x, cl = &quot;class&quot;) + labs(titel = &quot;Multinomial Logistic Regression&quot;) 4.5.2.5 Decision Trees library(&quot;rpart&quot;) model &lt;- x %&gt;% rpart(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;CART&quot;) model &lt;- x %&gt;% rpart(class ~ ., data = ., control = rpart.control(cp = 0.001, minsplit = 1)) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;CART (overfitting)&quot;) library(C50) model &lt;- x %&gt;% C5.0(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;C5.0&quot;) library(randomForest) model &lt;- x %&gt;% randomForest(class ~ ., data = .) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;Random Forest&quot;) 4.5.2.6 SVM library(e1071) model &lt;- x %&gt;% svm(class ~ ., data = ., kernel = &quot;linear&quot;) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;SVM (linear kernel)&quot;) model &lt;- x %&gt;% svm(class ~ ., data = ., kernel = &quot;radial&quot;) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;SVM (radial kernel)&quot;) model &lt;- x %&gt;% svm(class ~ ., data = ., kernel = &quot;polynomial&quot;) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;SVM (polynomial kernel)&quot;) model &lt;- x %&gt;% svm(class ~ ., data = ., kernel = &quot;sigmoid&quot;) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;SVM (sigmoid kernel)&quot;) 4.5.2.7 Single Layer Feed-forward Neural Networks library(nnet) model &lt;-x %&gt;% nnet(class ~ ., data = ., size = 1, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;NN (1 neuron)&quot;) model &lt;-x %&gt;% nnet(class ~ ., data = ., size = 2, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;NN (2 neurons)&quot;) model &lt;-x %&gt;% nnet(class ~ ., data = ., size = 4, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;NN (4 neurons)&quot;) model &lt;-x %&gt;% nnet(class ~ ., data = ., size = 10, maxit = 1000, trace = FALSE) decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;NN (10 neurons)&quot;) 4.5.2.8 Deep Learning with keras library(keras) redefine predict so it works with decision plot predict.keras.engine.training.Model &lt;- function(object, newdata, ...) predict_classes(object, as.matrix(newdata)) Choices are the activation function, number of layers, number of units per layer and the optimizer. A L2 regularizer is used for the dense layer weights to reduce overfitting. The output is a categorical class value, therefore the output layer uses the softmax activation function, the loss is categorical crossentropy, and the metric is accuracy. model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 10, activation = &#39;relu&#39;, input_shape = c(2), kernel_regularizer=regularizer_l2(l = 0.0001)) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39;) history &lt;- model %&gt;% fit( as.matrix(x[,1:2]), x %&gt;% pull(3) %&gt;% as.integer %&gt;% to_categorical(), epochs = 100, batch_size = 10 ) history ## ## Final epoch (plot to see history): ## loss: 0.1675 ## accuracy: 0.972 decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;keras (relu activation)&quot;) model &lt;- keras_model_sequential() %&gt;% layer_dense(units = 10, activation = &#39;tanh&#39;, input_shape = c(2), kernel_regularizer = regularizer_l2(l = 0.0001)) %&gt;% layer_dense(units = 3, activation = &#39;softmax&#39;) %&gt;% compile(loss = &#39;categorical_crossentropy&#39;, optimizer = &#39;adam&#39;, metrics = &#39;accuracy&#39;) history &lt;- model %&gt;% fit( as.matrix(x[,1:2]), x %&gt;% pull(3) %&gt;% as.integer %&gt;% to_categorical(), epochs = 100, batch_size = 10 ) history ## ## Final epoch (plot to see history): ## loss: 0.4902 ## accuracy: 0.808 decisionplot(model, x, cl = &quot;class&quot;) + labs(title = &quot;keras (tanh activation)&quot;) 4.6 More Information Example using deep learning with keras. Package caret: http://topepo.github.io/caret/index.html Tidymodels (machine learning with tidyverse): https://www.tidymodels.org/ R taskview on machine learning: http://cran.r-project.org/web/views/MachineLearning.html References Allaire, J., &amp; Chollet, F. (2021). Keras: R interface to keras. https://keras.rstudio.com Breiman, L., Cutler, A., Liaw, A., &amp; Wiener, M. (2018). randomForest: Breiman and cutler’s random forests for classification and regression. https://www.stat.berkeley.edu/~breiman/RandomForests/ Hornik, K. (2020). RWeka: R/weka interface. https://CRAN.R-project.org/package=RWeka Kuhn, M. (2021). Caret: Classification and regression training. https://github.com/topepo/caret/ Kuhn, M., &amp; Quinlan, R. (2021). C50: C5.0 decision trees and rule-based models. https://topepo.github.io/C5.0/ Leisch, F., &amp; Dimitriadou., E. (2021). Mlbench: Machine learning benchmark problems. https://CRAN.R-project.org/package=mlbench Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., &amp; Leisch, F. (2021). e1071: Misc functions of the department of statistics, probability theory group (formerly: E1071), TU wien. https://CRAN.R-project.org/package=e1071 Ripley, B. (2021a). MASS: Support functions and datasets for venables and ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/ Ripley, B. (2021b). Nnet: Feed-forward neural networks and multinomial log-linear models. http://www.stats.ox.ac.uk/pub/MASS4/ Sarkar, D. (2021). Lattice: Trellis graphics for r. http://lattice.r-forge.r-project.org/ Therneau, T., &amp; Atkinson, B. (2019). Rpart: Recursive partitioning and regression trees. https://CRAN.R-project.org/package=rpart Wickham, H. (2021c). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse Wickham, H., &amp; Seidel, D. (2020). Scales: Scale functions for visualization. https://CRAN.R-project.org/package=scales "],["association-analysis-basic-concepts-and-algorithms.html", "Chapter 5 Association Analysis: Basic Concepts and Algorithms 5.1 The arules Package 5.2 Transactions 5.3 Frequent Itemsets 5.4 Association Rules 5.5 Association Rule Visualization 5.6 Interactive Visualizations", " Chapter 5 Association Analysis: Basic Concepts and Algorithms Packages used for this chapter: arules (Hahsler, Buchta, Gruen, et al., 2021), arulesViz (Hahsler, 2021a), mlbench (Leisch &amp; Dimitriadou., 2021), tidyverse (Wickham, 2021c) You can read the free sample chapter from the textbook (Tan et al., 2005): Chapter 5. Association Analysis: Basic Concepts and Algorithms 5.1 The arules Package Association rule mining in R is implemented in the package arules. library(tidyverse) library(arules) library(arulesViz) For information about the arules package try: help(package=\"arules\") and vignette(\"arules\") (also available at CRAN) arules uses the S4 object system to implement classes and methods. Standard R objects use the S3 object system which do not use formal class definitions and are usually implemented as a list with a class attribute. arules and many other R packages use the S4 object system which is based on formal class definitions with member variables and methods (similar to object-oriented programming languages like Java and C++). Some important differences of using S4 objects compared to the usual S3 objects are: coercion (casting): as(from, \"class_name\") help for classes: class? class_name 5.2 Transactions 5.2.1 Create Transactions We will use the Zoo dataset from mlbench. data(Zoo, package = &quot;mlbench&quot;) head(Zoo) ## hair feathers eggs milk airborne aquatic ## aardvark TRUE FALSE FALSE TRUE FALSE FALSE ## antelope TRUE FALSE FALSE TRUE FALSE FALSE ## bass FALSE FALSE TRUE FALSE FALSE TRUE ## bear TRUE FALSE FALSE TRUE FALSE FALSE ## boar TRUE FALSE FALSE TRUE FALSE FALSE ## buffalo TRUE FALSE FALSE TRUE FALSE FALSE ## predator toothed backbone breathes venomous ## aardvark TRUE TRUE TRUE TRUE FALSE ## antelope FALSE TRUE TRUE TRUE FALSE ## bass TRUE TRUE TRUE FALSE FALSE ## bear TRUE TRUE TRUE TRUE FALSE ## boar TRUE TRUE TRUE TRUE FALSE ## buffalo FALSE TRUE TRUE TRUE FALSE ## fins legs tail domestic catsize type ## aardvark FALSE 4 FALSE FALSE TRUE mammal ## antelope FALSE 4 TRUE FALSE TRUE mammal ## bass TRUE 0 TRUE FALSE FALSE fish ## bear FALSE 4 FALSE FALSE TRUE mammal ## boar FALSE 4 TRUE FALSE TRUE mammal ## buffalo FALSE 4 TRUE FALSE TRUE mammal The data in the data.frame need to be converted into a set of transactions where each row represents a transaction and each column is translated into items. This is done using the contructor transactions(). For the Zoo data set this means that we consider animals as transactions and the different traits (features) will become items that each animal has. For example the animal antelope has the item hair in its transaction. trans &lt;- transactions(Zoo) ## Warning: Column(s) 13 not logical or factor. Applying ## default discretization (see &#39;? discretizeDF&#39;). The conversion gives a warning because only discrete features (factor and logical) can be directly translated into items. Continuous features need to be discretized first. What is column 13? summary(Zoo[13]) ## legs ## Min. :0.00 ## 1st Qu.:2.00 ## Median :4.00 ## Mean :2.84 ## 3rd Qu.:4.00 ## Max. :8.00 ggplot(Zoo, aes(legs)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better value ## with `binwidth`. table(Zoo$legs) ## ## 0 2 4 5 6 8 ## 23 27 38 1 10 2 Possible solution: Make legs into has/does not have legs Zoo_has_legs &lt;- Zoo %&gt;% mutate(legs = legs &gt; 0) ggplot(Zoo_has_legs, aes(legs)) + geom_bar() table(Zoo_has_legs$legs) ## ## FALSE TRUE ## 23 78 Alternatives: use each unique value as an item: Zoo_unique_leg_values &lt;- Zoo %&gt;% mutate(legs = factor(legs)) head(Zoo_unique_leg_values$legs) ## [1] 4 4 0 4 4 4 ## Levels: 0 2 4 5 6 8 discretize (see ? discretize and discretization in the code for Chapter 2): Zoo_discretized_legs &lt;- Zoo %&gt;% mutate( legs = discretize(legs, breaks = 2, method=&quot;interval&quot;) ) table(Zoo_discretized_legs$legs) ## ## [0,4) [4,8] ## 50 51 Convert data into a set of transactions trans &lt;- transactions(Zoo_has_legs) trans ## transactions in sparse format with ## 101 transactions (rows) and ## 23 items (columns) 5.2.2 Inspect Transactions summary(trans) ## transactions as itemMatrix in sparse format with ## 101 rows (elements/itemsets/transactions) and ## 23 columns (items) and a density of 0.361 ## ## most frequent items: ## backbone breathes legs tail toothed (Other) ## 83 80 78 75 61 462 ## ## element (itemset/transaction) length distribution: ## sizes ## 3 4 5 6 7 8 9 10 11 12 ## 3 2 6 5 8 21 27 25 3 1 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 3.00 8.00 9.00 8.31 10.00 12.00 ## ## includes extended item information - examples: ## labels variables levels ## 1 hair hair TRUE ## 2 feathers feathers TRUE ## 3 eggs eggs TRUE ## ## includes extended transaction information - examples: ## transactionID ## 1 aardvark ## 2 antelope ## 3 bass Look at created items. They are still called column names since the transactions are actually stored as a large sparse logical matrix (see below). colnames(trans) ## [1] &quot;hair&quot; &quot;feathers&quot; ## [3] &quot;eggs&quot; &quot;milk&quot; ## [5] &quot;airborne&quot; &quot;aquatic&quot; ## [7] &quot;predator&quot; &quot;toothed&quot; ## [9] &quot;backbone&quot; &quot;breathes&quot; ## [11] &quot;venomous&quot; &quot;fins&quot; ## [13] &quot;legs&quot; &quot;tail&quot; ## [15] &quot;domestic&quot; &quot;catsize&quot; ## [17] &quot;type=mammal&quot; &quot;type=bird&quot; ## [19] &quot;type=reptile&quot; &quot;type=fish&quot; ## [21] &quot;type=amphibian&quot; &quot;type=insect&quot; ## [23] &quot;type=mollusc.et.al&quot; Compare with the original features (column names) from Zoo colnames(Zoo) ## [1] &quot;hair&quot; &quot;feathers&quot; &quot;eggs&quot; &quot;milk&quot; ## [5] &quot;airborne&quot; &quot;aquatic&quot; &quot;predator&quot; &quot;toothed&quot; ## [9] &quot;backbone&quot; &quot;breathes&quot; &quot;venomous&quot; &quot;fins&quot; ## [13] &quot;legs&quot; &quot;tail&quot; &quot;domestic&quot; &quot;catsize&quot; ## [17] &quot;type&quot; Look at a (first) few transactions as a matrix. 1 indicates the presence of an item. as(trans, &quot;matrix&quot;)[1:3,] ## hair feathers eggs milk airborne aquatic ## aardvark TRUE FALSE FALSE TRUE FALSE FALSE ## antelope TRUE FALSE FALSE TRUE FALSE FALSE ## bass FALSE FALSE TRUE FALSE FALSE TRUE ## predator toothed backbone breathes venomous ## aardvark TRUE TRUE TRUE TRUE FALSE ## antelope FALSE TRUE TRUE TRUE FALSE ## bass TRUE TRUE TRUE FALSE FALSE ## fins legs tail domestic catsize ## aardvark FALSE TRUE FALSE FALSE TRUE ## antelope FALSE TRUE TRUE FALSE TRUE ## bass TRUE FALSE TRUE FALSE FALSE ## type=mammal type=bird type=reptile type=fish ## aardvark TRUE FALSE FALSE FALSE ## antelope TRUE FALSE FALSE FALSE ## bass FALSE FALSE FALSE TRUE ## type=amphibian type=insect type=mollusc.et.al ## aardvark FALSE FALSE FALSE ## antelope FALSE FALSE FALSE ## bass FALSE FALSE FALSE Look at the transactions as sets of items inspect(trans[1:3]) ## items transactionID ## [1] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## catsize, ## type=mammal} aardvark ## [2] {hair, ## milk, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} antelope ## [3] {eggs, ## aquatic, ## predator, ## toothed, ## backbone, ## fins, ## tail, ## type=fish} bass Plot the binary matrix. Dark dots represent 1s. image(trans) Look at the relative frequency (=support) of items in the data set. Here we look at the 10 most frequent items. itemFrequencyPlot(trans,topN = 20) ggplot( tibble( Support = sort(itemFrequency(trans, type = &quot;absolute&quot;), decreasing = TRUE), Item = seq_len(ncol(trans)) ), aes(x = Item, y = Support)) + geom_line() Alternative encoding: Also create items for FALSE (use factor) sapply(Zoo_has_legs, class) ## hair feathers eggs milk airborne ## &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; ## aquatic predator toothed backbone breathes ## &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; ## venomous fins legs tail domestic ## &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; &quot;logical&quot; ## catsize type ## &quot;logical&quot; &quot;factor&quot; Zoo_factors &lt;- Zoo_has_legs %&gt;% mutate_if(is.logical, factor) sapply(Zoo_factors, class) ## hair feathers eggs milk airborne aquatic ## &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; ## predator toothed backbone breathes venomous fins ## &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; ## legs tail domestic catsize type ## &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; summary(Zoo_factors) ## hair feathers eggs milk ## FALSE:58 FALSE:81 FALSE:42 FALSE:60 ## TRUE :43 TRUE :20 TRUE :59 TRUE :41 ## ## ## ## ## ## airborne aquatic predator toothed ## FALSE:77 FALSE:65 FALSE:45 FALSE:40 ## TRUE :24 TRUE :36 TRUE :56 TRUE :61 ## ## ## ## ## ## backbone breathes venomous fins ## FALSE:18 FALSE:21 FALSE:93 FALSE:84 ## TRUE :83 TRUE :80 TRUE : 8 TRUE :17 ## ## ## ## ## ## legs tail domestic catsize ## FALSE:23 FALSE:26 FALSE:88 FALSE:57 ## TRUE :78 TRUE :75 TRUE :13 TRUE :44 ## ## ## ## ## ## type ## mammal :41 ## bird :20 ## reptile : 5 ## fish :13 ## amphibian : 4 ## insect : 8 ## mollusc.et.al:10 trans_factors &lt;- transactions(Zoo_factors) trans_factors ## transactions in sparse format with ## 101 transactions (rows) and ## 39 items (columns) itemFrequencyPlot(trans_factors, topN = 20) ## Select transactions that contain a certain item trans_insects &lt;- trans_factors[trans %in% &quot;type=insect&quot;] trans_insects ## transactions in sparse format with ## 8 transactions (rows) and ## 39 items (columns) inspect(trans_insects) ## items transactionID ## [1] {hair=FALSE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=FALSE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} flea ## [2] {hair=FALSE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} gnat ## [3] {hair=TRUE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=TRUE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=TRUE, ## catsize=FALSE, ## type=insect} honeybee ## [4] {hair=TRUE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} housefly ## [5] {hair=FALSE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=TRUE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} ladybird ## [6] {hair=TRUE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} moth ## [7] {hair=FALSE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=FALSE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=FALSE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} termite ## [8] {hair=TRUE, ## feathers=FALSE, ## eggs=TRUE, ## milk=FALSE, ## airborne=TRUE, ## aquatic=FALSE, ## predator=FALSE, ## toothed=FALSE, ## backbone=FALSE, ## breathes=TRUE, ## venomous=TRUE, ## fins=FALSE, ## legs=TRUE, ## tail=FALSE, ## domestic=FALSE, ## catsize=FALSE, ## type=insect} wasp 5.2.3 Vertical Layout (Transaction ID Lists) The default layout for transactions is horizontal layout (i.e. each transaction is a row). The vertical layout represents transaction data as a list of transaction IDs for each item (= transaction ID lists). vertical &lt;- as(trans, &quot;tidLists&quot;) as(vertical, &quot;matrix&quot;)[1:10, 1:5] ## aardvark antelope bass bear boar ## hair TRUE TRUE FALSE TRUE TRUE ## feathers FALSE FALSE FALSE FALSE FALSE ## eggs FALSE FALSE TRUE FALSE FALSE ## milk TRUE TRUE FALSE TRUE TRUE ## airborne FALSE FALSE FALSE FALSE FALSE ## aquatic FALSE FALSE TRUE FALSE FALSE ## predator TRUE FALSE TRUE TRUE TRUE ## toothed TRUE TRUE TRUE TRUE TRUE ## backbone TRUE TRUE TRUE TRUE TRUE ## breathes TRUE TRUE FALSE TRUE TRUE 5.3 Frequent Itemsets 5.3.1 Mine Frequent Itemsets For this dataset we have already a huge number of possible itemsets 2^ncol(trans) ## [1] 8388608 Find frequent itemsets (target=“frequent”) with the default settings. its &lt;- apriori(trans, parameter=list(target = &quot;frequent&quot;)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## NA 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.1 1 10 frequent itemsets TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 10 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [18 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans, parameter = list(target = ## &quot;frequent&quot;)): Mining stopped (maxlen reached). Only ## patterns up to a length of 10 returned! ## done [0.00s]. ## sorting transactions ... done [0.00s]. ## writing ... [1465 set(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. its ## set of 1465 itemsets Default minimum support is .1 (10%). Note: We use here a very small data set. For larger datasets the default minimum support might be to low and you may run out of memory. You probably want to start out with a higher minimum support like .5 (50%) and then work your way down. 5/nrow(trans) ## [1] 0.0495 In order to find itemsets that effect 5 animals I need to go down to a support of about 5%. its &lt;- apriori(trans, parameter=list(target = &quot;frequent&quot;, support = 0.05)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## NA 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.05 1 10 frequent itemsets TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 5 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [21 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans, parameter = list(target = ## &quot;frequent&quot;, support = 0.05)): Mining stopped (maxlen ## reached). Only patterns up to a length of 10 returned! ## done [0.00s]. ## sorting transactions ... done [0.00s]. ## writing ... [2537 set(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. its ## set of 2537 itemsets Sort by support its &lt;- sort(its, by = &quot;support&quot;) inspect(head(its, n = 10)) ## items support count ## [1] {backbone} 0.822 83 ## [2] {breathes} 0.792 80 ## [3] {legs} 0.772 78 ## [4] {tail} 0.743 75 ## [5] {backbone, tail} 0.733 74 ## [6] {breathes, legs} 0.723 73 ## [7] {backbone, breathes} 0.683 69 ## [8] {backbone, legs} 0.634 64 ## [9] {backbone, breathes, legs} 0.634 64 ## [10] {toothed} 0.604 61 Look at frequent itemsets with many items (set breaks manually since Automatically chosen breaks look bad) ggplot(tibble(`Itemset Size` = factor(size(its))), aes(`Itemset Size`)) + geom_bar() inspect(its[size(its) &gt; 8]) ## items support count ## [1] {hair, ## milk, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.2376 24 ## [2] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## catsize, ## type=mammal} 0.1584 16 ## [3] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## type=mammal} 0.1485 15 ## [4] {hair, ## milk, ## predator, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1386 14 ## [5] {hair, ## milk, ## predator, ## toothed, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [6] {hair, ## milk, ## predator, ## toothed, ## backbone, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [7] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [8] {milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [9] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize} 0.1287 13 ## [10] {hair, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [11] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [12] {hair, ## milk, ## toothed, ## backbone, ## breathes, ## legs, ## domestic, ## catsize, ## type=mammal} 0.0594 6 ## [13] {hair, ## milk, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## domestic, ## type=mammal} 0.0594 6 ## [14] {feathers, ## eggs, ## airborne, ## predator, ## backbone, ## breathes, ## legs, ## tail, ## type=bird} 0.0594 6 5.3.2 Concise Representation of Itemsets Find maximal frequent itemsets (no superset if frequent) its_max &lt;- its[is.maximal(its)] its_max ## set of 22 itemsets inspect(head(its_max, by = &quot;support&quot;)) ## items support count ## [1] {hair, ## milk, ## predator, ## toothed, ## backbone, ## breathes, ## legs, ## tail, ## catsize, ## type=mammal} 0.1287 13 ## [2] {eggs, ## aquatic, ## predator, ## toothed, ## backbone, ## fins, ## tail, ## type=fish} 0.0891 9 ## [3] {aquatic, ## predator, ## toothed, ## backbone, ## breathes} 0.0792 8 ## [4] {aquatic, ## predator, ## toothed, ## backbone, ## fins, ## tail, ## catsize} 0.0693 7 ## [5] {eggs, ## venomous} 0.0594 6 ## [6] {predator, ## venomous} 0.0594 6 Find closed frequent itemsets (no superset if frequent) its_closed &lt;- its[is.closed(its)] its_closed ## set of 230 itemsets inspect(head(its_closed, by = &quot;support&quot;)) ## items support count ## [1] {backbone} 0.822 83 ## [2] {breathes} 0.792 80 ## [3] {legs} 0.772 78 ## [4] {tail} 0.743 75 ## [5] {backbone, tail} 0.733 74 ## [6] {breathes, legs} 0.723 73 counts &lt;- c( frequent=length(its), closed=length(its_closed), maximal=length(its_max) ) ggplot(as_tibble(counts, rownames = &quot;Itemsets&quot;), aes(Itemsets, counts)) + geom_bar(stat = &quot;identity&quot;) 5.4 Association Rules 5.4.1 Mine Association Rules We use the APRIORI algorithm (see ? apriori) rules &lt;- apriori(trans, parameter = list(support = 0.05, confidence = 0.9)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.9 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.05 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 5 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [21 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans, parameter = list(support ## = 0.05, confidence = 0.9)): Mining stopped (maxlen ## reached). Only patterns up to a length of 10 returned! ## done [0.00s]. ## writing ... [7174 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. length(rules) ## [1] 7174 inspect(head(rules)) ## lhs rhs support ## [1] {type=insect} =&gt; {eggs} 0.0792 ## [2] {type=insect} =&gt; {legs} 0.0792 ## [3] {type=insect} =&gt; {breathes} 0.0792 ## [4] {type=mollusc.et.al} =&gt; {eggs} 0.0891 ## [5] {type=fish} =&gt; {fins} 0.1287 ## [6] {type=fish} =&gt; {aquatic} 0.1287 ## confidence coverage lift count ## [1] 1.0 0.0792 1.71 8 ## [2] 1.0 0.0792 1.29 8 ## [3] 1.0 0.0792 1.26 8 ## [4] 0.9 0.0990 1.54 9 ## [5] 1.0 0.1287 5.94 13 ## [6] 1.0 0.1287 2.81 13 quality(head(rules)) ## support confidence coverage lift count ## 1 0.0792 1.0 0.0792 1.71 8 ## 2 0.0792 1.0 0.0792 1.29 8 ## 3 0.0792 1.0 0.0792 1.26 8 ## 4 0.0891 0.9 0.0990 1.54 9 ## 5 0.1287 1.0 0.1287 5.94 13 ## 6 0.1287 1.0 0.1287 2.81 13 Look at rules with highest lift rules &lt;- sort(rules, by = &quot;lift&quot;) inspect(head(rules, n = 10)) ## lhs rhs support confidence coverage lift count ## [1] {eggs, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [2] {eggs, ## aquatic, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [3] {eggs, ## predator, ## fins} =&gt; {type=fish} 0.0891 1 0.0891 7.77 9 ## [4] {eggs, ## toothed, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [5] {eggs, ## fins, ## tail} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [6] {eggs, ## backbone, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [7] {eggs, ## aquatic, ## predator, ## fins} =&gt; {type=fish} 0.0891 1 0.0891 7.77 9 ## [8] {eggs, ## aquatic, ## toothed, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [9] {eggs, ## aquatic, ## fins, ## tail} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 ## [10] {eggs, ## aquatic, ## backbone, ## fins} =&gt; {type=fish} 0.1287 1 0.1287 7.77 13 Create rules using the alternative encoding (with “FALSE” item) r &lt;- apriori(trans_factors) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.8 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.1 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 10 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[39 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [34 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans_factors): Mining stopped ## (maxlen reached). Only patterns up to a length of 10 ## returned! ## done [0.15s]. ## writing ... [1517191 rule(s)] done [0.34s]. ## creating S4 object ... done [0.76s]. r ## set of 1517191 rules print(object.size(r), unit = &quot;Mb&quot;) ## 110.2 Mb inspect(r[1:10]) ## lhs rhs support ## [1] {} =&gt; {feathers=FALSE} 0.802 ## [2] {} =&gt; {backbone=TRUE} 0.822 ## [3] {} =&gt; {fins=FALSE} 0.832 ## [4] {} =&gt; {domestic=FALSE} 0.871 ## [5] {} =&gt; {venomous=FALSE} 0.921 ## [6] {domestic=TRUE} =&gt; {predator=FALSE} 0.109 ## [7] {domestic=TRUE} =&gt; {aquatic=FALSE} 0.119 ## [8] {domestic=TRUE} =&gt; {legs=TRUE} 0.119 ## [9] {domestic=TRUE} =&gt; {breathes=TRUE} 0.119 ## [10] {domestic=TRUE} =&gt; {backbone=TRUE} 0.119 ## confidence coverage lift count ## [1] 0.802 1.000 1.00 81 ## [2] 0.822 1.000 1.00 83 ## [3] 0.832 1.000 1.00 84 ## [4] 0.871 1.000 1.00 88 ## [5] 0.921 1.000 1.00 93 ## [6] 0.846 0.129 1.90 11 ## [7] 0.923 0.129 1.43 12 ## [8] 0.923 0.129 1.20 12 ## [9] 0.923 0.129 1.17 12 ## [10] 0.923 0.129 1.12 12 inspect(head(r, n = 10, by = &quot;lift&quot;)) ## lhs rhs support confidence coverage lift count ## [1] {breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [2] {eggs=TRUE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [3] {milk=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [4] {breathes=FALSE, ## fins=TRUE, ## legs=FALSE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [5] {aquatic=TRUE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [6] {hair=FALSE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [7] {eggs=TRUE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [8] {milk=FALSE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [9] {toothed=TRUE, ## breathes=FALSE, ## fins=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [10] {breathes=FALSE, ## fins=TRUE, ## tail=TRUE} =&gt; {type=fish} 0.129 1 0.129 7.77 13 5.4.2 Calculate Additional Interest Measures interestMeasure(rules[1:10], measure = c(&quot;phi&quot;, &quot;gini&quot;), trans = trans) ## phi gini ## 1 1.000 0.224 ## 2 1.000 0.224 ## 3 0.814 0.149 ## 4 1.000 0.224 ## 5 1.000 0.224 ## 6 1.000 0.224 ## 7 0.814 0.149 ## 8 1.000 0.224 ## 9 1.000 0.224 ## 10 1.000 0.224 Add measures to the rules quality(rules) &lt;- cbind(quality(rules), interestMeasure(rules, measure = c(&quot;phi&quot;, &quot;gini&quot;), trans = trans)) Find rules which score high for Phi correlation inspect(head(rules, by = &quot;phi&quot;)) ## lhs rhs support confidence coverage lift count phi gini ## [1] {eggs, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [2] {eggs, ## aquatic, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [3] {eggs, ## toothed, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [4] {eggs, ## fins, ## tail} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [5] {eggs, ## backbone, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 ## [6] {eggs, ## aquatic, ## toothed, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 1 0.224 5.4.3 Mine Using Templates Sometimes it is beneficial to specify what items should be where in the rule. For apriori we can use the parameter appearance to specify this (see ? APappearance). In the following we restrict rules to an animal type in the RHS and any item in the LHS. type &lt;- grep(&quot;type=&quot;, itemLabels(trans), value = TRUE) type ## [1] &quot;type=mammal&quot; &quot;type=bird&quot; ## [3] &quot;type=reptile&quot; &quot;type=fish&quot; ## [5] &quot;type=amphibian&quot; &quot;type=insect&quot; ## [7] &quot;type=mollusc.et.al&quot; rules_type &lt;- apriori(trans, appearance= list(rhs = type)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.8 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.1 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 10 ## ## set item appearances ...[7 item(s)] done [0.00s]. ## set transactions ...[23 item(s), 101 transaction(s)] done [0.00s]. ## sorting and recoding items ... [18 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 9 10 ## Warning in apriori(trans, appearance = list(rhs = ## type)): Mining stopped (maxlen reached). Only patterns ## up to a length of 10 returned! ## done [0.00s]. ## writing ... [571 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. inspect(head(sort(rules_type, by = &quot;lift&quot;))) ## lhs rhs support confidence coverage lift count ## [1] {eggs, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [2] {eggs, ## aquatic, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [3] {eggs, ## toothed, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [4] {eggs, ## fins, ## tail} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [5] {eggs, ## backbone, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 ## [6] {eggs, ## aquatic, ## toothed, ## fins} =&gt; {type=fish} 0.129 1 0.129 7.77 13 Saving rules as a CSV-file to be opened with Excel or other tools. write(rules, file = \"rules.csv\", quote = TRUE) 5.5 Association Rule Visualization library(arulesViz) Default scatterplot plot(rules) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. Note that some jitter (randomly move points) was added to show how many rules have the same confidence and support value. Without jitter: plot(rules, control = list(jitter = 0)) plot(rules, shading = &quot;order&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. ##plot(rules, interactive = TRUE) Grouped plot plot(rules, method = &quot;grouped&quot;) ##plot(rules, method = &quot;grouped&quot;, engine = &quot;interactive&quot;) As a graph plot(rules, method = &quot;graph&quot;) ## Warning: Too many rules supplied. Only plotting the ## best 100 rules using lift (change control parameter max ## if needed) plot(head(rules, by = &quot;phi&quot;, n = 100), method = &quot;graph&quot;) 5.6 Interactive Visualizations We will use the association rules mined from the Iris dataset for the following examples. data(iris) summary(iris) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.00 Min. :1.00 ## 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 ## Median :5.80 Median :3.00 Median :4.35 ## Mean :5.84 Mean :3.06 Mean :3.76 ## 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 ## Max. :7.90 Max. :4.40 Max. :6.90 ## Petal.Width Species ## Min. :0.1 setosa :50 ## 1st Qu.:0.3 versicolor:50 ## Median :1.3 virginica :50 ## Mean :1.2 ## 3rd Qu.:1.8 ## Max. :2.5 Convert the data to transactions. Note that the features are numeric and need to be discretized. The conversion automatically applies frequency-based discretization with 3 classes to each numeric feature (with a warning). iris_trans &lt;- transactions(iris) ## Warning: Column(s) 1, 2, 3, 4 not logical or factor. ## Applying default discretization (see &#39;? discretizeDF&#39;). inspect(head(iris_trans)) ## items transactionID ## [1] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[3.2,4.4], ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 1 ## [2] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[2.9,3.2), ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 2 ## [3] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[3.2,4.4], ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 3 ## [4] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[2.9,3.2), ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 4 ## [5] {Sepal.Length=[4.3,5.4), ## Sepal.Width=[3.2,4.4], ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 5 ## [6] {Sepal.Length=[5.4,6.3), ## Sepal.Width=[3.2,4.4], ## Petal.Length=[1,2.63), ## Petal.Width=[0.1,0.867), ## Species=setosa} 6 Next, we mine association rules. rules &lt;- apriori(iris_trans, parameter = list(support = 0.1, confidence = 0.8)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.8 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.1 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 15 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[15 item(s), 150 transaction(s)] done [0.00s]. ## sorting and recoding items ... [15 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 done [0.00s]. ## writing ... [144 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. rules ## set of 144 rules 5.6.1 Interactive Inspect With Sorting, Filtering and Paging inspectDT(rules) 5.6.2 Scatter Plot Plot rules as a scatter plot using an interactive html widget. To avoid overplotting, jitter is added automatically. Set jitter = 0 to disable jitter. Hovering over rules shows rule information. Note: plotly/javascript does not do well with too many points, so plot selects the top 1000 rules with a warning if more rules are supplied. plot(rules, engine = &quot;html&quot;) ## To reduce overplotting, jitter is added! Use jitter = 0 to prevent jitter. 5.6.3 Matrix Visualization Plot rules as a matrix using an interactive html widget. plot(rules, method = &quot;matrix&quot;, engine = &quot;html&quot;) 5.6.4 Visualization as Graph Plot rules as a graph using an interactive html widget. Note: the used javascript library does not do well with too many graph nodes, so plot selects the top 100 rules only (with a warning). plot(rules, method = &quot;graph&quot;, engine = &quot;html&quot;) ## Warning: Too many rules supplied. Only plotting the ## best 100 rules using lift (change control parameter max ## if needed) 5.6.5 Interactive Rule Explorer You can specify a rule set or a dataset. To explore rules that can be mined from iris, use: ruleExplorer(iris) The rule explorer creates an interactive Shiny application that can be used locally or deployed on a server for sharing. A deployed version of the ruleExplorer is available here (using shinyapps.io). References Hahsler, M. (2021a). arulesViz: Visualizing association rules and frequent itemsets. https://github.com/mhahsler/arulesViz Hahsler, M., Buchta, C., Gruen, B., &amp; Hornik, K. (2021). Arules: Mining association rules and frequent itemsets. https://github.com/mhahsler/arules Leisch, F., &amp; Dimitriadou., E. (2021). Mlbench: Machine learning benchmark problems. https://CRAN.R-project.org/package=mlbench Tan, P.-N., Steinbach, M. S., &amp; Kumar, V. (2005). Introduction to data mining (1st Edition). Addison-Wesley. https://www-users.cs.umn.edu/~kumar001/dmbook/firsted.php Wickham, H. (2021c). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse "],["association-analysis-advanced-concepts.html", "Chapter 6 Association Analysis: Advanced Concepts", " Chapter 6 Association Analysis: Advanced Concepts No code is available for this Chapter. Some topics were already covered in the code for the previous chapter. "],["clustering-analysis.html", "Chapter 7 Clustering Analysis 7.1 Data Preparation 7.2 Clustering methods 7.3 Internal Cluster Validation 7.4 External Cluster Validation 7.5 Advanced Data Preparation for Clustering", " Chapter 7 Clustering Analysis Packages used for this chapter: cluster (Maechler et al., 2021), dbscan (Hahsler &amp; Piekenbrock, 2021), e1071 (Meyer et al., 2021), factoextra (Kassambara &amp; Mundt, 2020), fpc (Hennig, 2020), GGally (Schloerke et al., 2021), kernlab (Karatzoglou et al., 2019), mclust (Fraley et al., 2020), mlbench (Leisch &amp; Dimitriadou., 2021), scatterpie (Yu, 2021), seriation (Hahsler, Buchta, &amp; Hornik, 2021), tidyverse (Wickham, 2021c) You can read the free sample chapter from the textbook (Tan et al., 2005): Chapter 7. Cluster Analysis: Basic Concepts and Algorithms 7.1 Data Preparation library(tidyverse) We will use here a small and very clean dataset called Ruspini which is included in the R package cluster. data(ruspini, package = &quot;cluster&quot;) The Ruspini data set, consisting of 75 points in four groups that is popular for illustrating clustering techniques. It is a very simple data set with well separated clusters. The original dataset has the points ordered by group. We can shuffle the data (rows) using sample_frac which samples by default 100%. ruspini &lt;- as_tibble(ruspini) %&gt;% sample_frac() ruspini ## # A tibble: 75 x 2 ## x y ## &lt;int&gt; &lt;int&gt; ## 1 38 143 ## 2 30 52 ## 3 22 74 ## 4 70 4 ## 5 77 12 ## 6 18 61 ## 7 85 115 ## 8 34 141 ## 9 53 144 ## 10 35 153 ## # … with 65 more rows 7.1.1 Data cleaning ggplot(ruspini, aes(x = x, y = y)) + geom_point() summary(ruspini) ## x y ## Min. : 4.0 Min. : 4.0 ## 1st Qu.: 31.5 1st Qu.: 56.5 ## Median : 52.0 Median : 96.0 ## Mean : 54.9 Mean : 92.0 ## 3rd Qu.: 76.5 3rd Qu.:141.5 ## Max. :117.0 Max. :156.0 For most clustering algorithms it is necessary to handle missing values and outliers (e.g., remove the observations). For details see Section “Outlier removal” below. This data set has not missing values or strong outlier and looks like it has some very clear groups. 7.1.2 Scale data Clustering algorithms use distances and the variables with the largest number range will dominate distance calculation. The summary above shows that this is not an issue for the Ruspini dataset with both, x and y, being roughly between 0 and 150. Most data analysts will still scale each column in the data to zero mean and unit standard deviation (z-scores). Note: The standard scale() function scales a whole data matrix so we implement a function for a single vector and apply it to all numeric columns. ## I use this till tidyverse implements a scale function scale_numeric &lt;- function(x) x %&gt;% mutate_if(is.numeric, function(y) as.vector(scale(y))) ruspini_scaled &lt;- ruspini %&gt;% scale_numeric() summary(ruspini_scaled) ## x y ## Min. :-1.668 Min. :-1.807 ## 1st Qu.:-0.766 1st Qu.:-0.729 ## Median :-0.094 Median : 0.082 ## Mean : 0.000 Mean : 0.000 ## 3rd Qu.: 0.709 3rd Qu.: 1.016 ## Max. : 2.037 Max. : 1.314 After scaling, most z-scores will fall in the range \\([-3,3]\\) (z-scores are measured in standard deviations from the mean), where \\(0\\) means average. 7.2 Clustering methods 7.2.1 k-means Clustering k-means implicitly assumes Euclidean distances. We use \\(k = 4\\) clusters and run the algorithm 10 times with random initialized centroids. The best result is returned. km &lt;- kmeans(ruspini_scaled, centers = 4, nstart = 10) km ## K-means clustering with 4 clusters of sizes 17, 23, 15, 20 ## ## Cluster means: ## x y ## 1 1.419 0.469 ## 2 -0.360 1.109 ## 3 0.461 -1.491 ## 4 -1.139 -0.556 ## ## Clustering vector: ## [1] 2 4 4 3 3 4 1 2 2 2 3 2 3 4 1 3 4 4 1 4 2 3 1 2 1 ## [26] 2 3 1 3 2 1 4 3 4 4 1 1 2 1 4 2 1 2 2 4 3 3 2 2 4 ## [51] 1 2 4 2 3 4 2 3 2 4 1 4 4 1 1 2 4 3 1 2 2 3 4 1 2 ## ## Within cluster sum of squares by cluster: ## [1] 3.64 2.66 1.08 2.71 ## (between_SS / total_SS = 93.2 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; ## [4] &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; ## [7] &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; km is an R object implemented as a list. The clustering vector contains the cluster assignment for each data row and can be accessed using km$cluster. I add the cluster assignment as a column to the scaled dataset (I make it a factor since it represents a nominal label). ruspini_clustered &lt;- ruspini_scaled %&gt;% add_column(cluster = factor(km$cluster)) ruspini_clustered ## # A tibble: 75 x 3 ## x y cluster ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.553 1.05 2 ## 2 -0.816 -0.822 4 ## 3 -1.08 -0.370 4 ## 4 0.496 -1.81 3 ## 5 0.725 -1.64 3 ## 6 -1.21 -0.637 4 ## 7 0.987 0.472 1 ## 8 -0.685 1.01 2 ## 9 -0.0616 1.07 2 ## 10 -0.652 1.25 2 ## # … with 65 more rows ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() Add the centroids to the plot. centroids &lt;- as_tibble(km$centers, rownames = &quot;cluster&quot;) centroids ## # A tibble: 4 x 3 ## cluster x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.42 0.469 ## 2 2 -0.360 1.11 ## 3 3 0.461 -1.49 ## 4 4 -1.14 -0.556 ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10) Use the factoextra package for visualization library(factoextra) fviz_cluster(km, data = ruspini_scaled, centroids = TRUE, repel = TRUE, ellipse.type = &quot;norm&quot;) ## Warning: ggrepel: 10 unlabeled data points (too many ## overlaps). Consider increasing max.overlaps 7.2.1.1 Inspect clusters We inspect the clusters created by the 4-cluster k-means solution. The following code can be adapted to be used for other clustering methods. 7.2.1.1.1 Cluster Profiles Inspect the centroids with horizontal bar charts organized by cluster. To group the plots by cluster, we have to change the data format to the “long”-format using a pivot operation. I use colors to match the clusters in the scatter plots. ggplot(pivot_longer(centroids, cols = c(x, y), names_to = &quot;feature&quot;), aes(x = value, y = feature, fill = cluster)) + geom_bar(stat = &quot;identity&quot;) + facet_grid(rows = vars(cluster)) 7.2.1.1.2 Extract a single cluster You need is to filter the rows corresponding to the cluster index. The next example calculates summary statistics and then plots all data points of cluster 1. cluster1 &lt;- ruspini_clustered %&gt;% filter(cluster == 1) cluster1 ## # A tibble: 17 x 3 ## x y cluster ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.987 0.472 1 ## 2 1.74 0.492 1 ## 3 1.38 0.615 1 ## 4 0.758 0.0405 1 ## 5 1.97 0.513 1 ## 6 1.45 0.554 1 ## 7 1.51 0.472 1 ## 8 0.987 0.0816 1 ## 9 0.627 0.0816 1 ## 10 1.74 0.390 1 ## 11 2.04 0.472 1 ## 12 1.45 0.739 1 ## 13 1.84 0.698 1 ## 14 1.81 0.390 1 ## 15 1.02 0.821 1 ## 16 1.41 0.657 1 ## 17 1.41 0.492 1 summary(cluster1) ## x y cluster ## Min. :0.627 Min. :0.041 1:17 ## 1st Qu.:1.020 1st Qu.:0.390 2: 0 ## Median :1.446 Median :0.492 3: 0 ## Mean :1.419 Mean :0.469 4: 0 ## 3rd Qu.:1.741 3rd Qu.:0.615 ## Max. :2.037 Max. :0.821 ggplot(cluster1, aes(x = x, y = y)) + geom_point() + coord_cartesian(xlim = c(-2, 2), ylim = c(-2, 2)) What happens if we try to cluster with 8 centers? fviz_cluster(kmeans(ruspini_scaled, centers = 8), data = ruspini_scaled, centroids = TRUE, geom = &quot;point&quot;, ellipse.type = &quot;norm&quot;) ## Too few points to calculate an ellipse 7.2.2 Hierarchical Clustering Hierarchical clustering starts with a distance matrix. dist() defaults to method=“Euclidean.” Note: Distance matrices become very large quickly (size and time complexity is \\(O(n^2)\\) where \\(n\\) is the number if data points). It is only possible to calculate and store the matrix for small data sets (maybe a few hundred thousand data points) in main memory. If your data is too large then you can use sampling. d &lt;- dist(ruspini_scaled) hclust() implements agglomerative hierarchical clustering. We cluster using complete link. hc &lt;- hclust(d, method = &quot;complete&quot;) Hierarchical clustering does not return cluster assignments but a dendrogram. The standard plot function plots the dendrogram. plot(hc) Use factoextra (ggplot version). We can specify the number of clusters to visualize how the dendrogram will be cut into clusters. fviz_dend(hc, k = 4) ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. ## Please use `guides(&lt;scale&gt; = &quot;none&quot;)` instead. More plotting options for dendrograms, including plotting parts of large dendrograms can be found here. Extract cluster assignments by cutting the dendrogram into four parts and add the cluster id to the data. clusters &lt;- cutree(hc, k = 4) cluster_complete &lt;- ruspini_scaled %&gt;% add_column(cluster = factor(clusters)) cluster_complete ## # A tibble: 75 x 3 ## x y cluster ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 -0.553 1.05 1 ## 2 -0.816 -0.822 2 ## 3 -1.08 -0.370 2 ## 4 0.496 -1.81 3 ## 5 0.725 -1.64 3 ## 6 -1.21 -0.637 2 ## 7 0.987 0.472 4 ## 8 -0.685 1.01 1 ## 9 -0.0616 1.07 1 ## 10 -0.652 1.25 1 ## # … with 65 more rows ggplot(cluster_complete, aes(x, y, color = cluster)) + geom_point() Try 8 clusters (Note: fviz_cluster needs a list with data and the cluster labels for hclust) fviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc, k = 8)), geom = &quot;point&quot;) Clustering with single link hc_single &lt;- hclust(d, method = &quot;single&quot;) fviz_dend(hc_single, k = 4) ## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated. ## Please use `guides(&lt;scale&gt; = &quot;none&quot;)` instead. fviz_cluster(list(data = ruspini_scaled, cluster = cutree(hc_single, k = 4)), geom = &quot;point&quot;) 7.2.3 Density-based clustering with DBSCAN library(dbscan) DBSCAN stands for “Density-Based Spatial Clustering of Applications with Noise.” It groups together points that are closely packed together and treats points in low-density regions as outliers. Parameters: minPts defines how many points in the epsilon neighborhood are needed to make a point a core point. It is often chosen as a smoothing parameter. I use here minPts = 4. To decide on epsilon, the knee in the kNN distance plot is often used. Note that minPts contains the point itself, while the k-nearest neighbor does not. We therefore have to use k = minPts - 1! The knee is around eps = .32. kNNdistplot(ruspini_scaled, k = 3) abline(h = .32, col = &quot;red&quot;) run dbscan db &lt;- dbscan(ruspini_scaled, eps = .32, minPts = 4) db ## DBSCAN clustering for 75 objects. ## Parameters: eps = 0.32, minPts = 4 ## The clustering contains 4 cluster(s) and 5 noise points. ## ## 0 1 2 3 4 ## 5 23 20 15 12 ## ## Available fields: cluster, eps, minPts str(db) ## List of 3 ## $ cluster: int [1:75] 1 2 2 3 3 2 0 1 1 1 ... ## $ eps : num 0.32 ## $ minPts : num 4 ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;dbscan_fast&quot; &quot;dbscan&quot; ggplot(ruspini_scaled %&gt;% add_column(cluster = factor(db$cluster)), aes(x, y, color = cluster)) + geom_point() Note: Cluster 0 represents outliers). fviz_cluster(db, ruspini_scaled, geom = &quot;point&quot;) Play with eps (neighborhood size) and MinPts (minimum of points needed for core cluster) 7.2.4 Partitioning Around Medoids (PAM) PAM tries to solve the \\(k\\)-medoids problem. The problem is similar to \\(k\\)-means, but uses medoids instead of centroids to represent clusters. Like hierarchical clustering, it typically works with precomputed distance matrix. An advantage is that you can use any distance metric not just Euclidean distances. Note: The medoid is the most central data point in the middle of the cluster. library(cluster) ## ## Attaching package: &#39;cluster&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## ruspini d &lt;- dist(ruspini_scaled) str(d) ## &#39;dist&#39; num [1:2775] 1.89 1.51 3.04 2.98 1.81 ... ## - attr(*, &quot;Size&quot;)= int 75 ## - attr(*, &quot;Diag&quot;)= logi FALSE ## - attr(*, &quot;Upper&quot;)= logi FALSE ## - attr(*, &quot;method&quot;)= chr &quot;Euclidean&quot; ## - attr(*, &quot;call&quot;)= language dist(x = ruspini_scaled) p &lt;- pam(d, k = 4) p ## Medoids: ## ID ## [1,] 66 66 ## [2,] 56 56 ## [3,] 33 33 ## [4,] 28 28 ## Clustering vector: ## [1] 1 2 2 3 3 2 4 1 1 1 3 1 3 2 4 3 2 2 4 2 1 3 4 1 4 ## [26] 1 3 4 3 1 4 2 3 2 2 4 4 1 4 2 1 4 1 1 2 3 3 1 1 2 ## [51] 4 1 2 1 3 2 1 3 1 2 4 2 2 4 4 1 2 3 4 1 1 3 2 4 1 ## Objective function: ## build swap ## 0.442 0.319 ## ## Available components: ## [1] &quot;medoids&quot; &quot;id.med&quot; &quot;clustering&quot; &quot;objective&quot; ## [5] &quot;isolation&quot; &quot;clusinfo&quot; &quot;silinfo&quot; &quot;diss&quot; ## [9] &quot;call&quot; ruspini_clustered &lt;- ruspini_scaled %&gt;% add_column(cluster = factor(p$cluster)) medoids &lt;- as_tibble(ruspini_scaled[p$medoids, ], rownames = &quot;cluster&quot;) medoids ## # A tibble: 4 x 3 ## cluster x y ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.357 1.17 ## 2 2 -1.18 -0.555 ## 3 3 0.463 -1.46 ## 4 4 1.45 0.554 ggplot(ruspini_clustered, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = medoids, aes(x = x, y = y, color = cluster), shape = 3, size = 10) ## __Note:__ `fviz_cluster` needs the original data. fviz_cluster(c(p, list(data = ruspini_scaled)), geom = &quot;point&quot;, ellipse.type = &quot;norm&quot;) 7.2.5 Gaussian Mixture Models library(mclust) ## Package &#39;mclust&#39; version 5.4.7 ## Type &#39;citation(&quot;mclust&quot;)&#39; for citing this R package in publications. ## ## Attaching package: &#39;mclust&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map Gaussian mixture models assume that the data set is the result of drawing data from a set of Gaussian distributions where each distribution represents a cluster. Estimation algorithms try to identify the location parameters of the distributions and thus can be used to find clusters. Mclust() uses Bayesian Information Criterion (BIC) to find the number of clusters (model selection). BIC uses the likelihood and a penalty term to guard against overfitting. m &lt;- Mclust(ruspini_scaled) summary(m) ## ---------------------------------------------------- ## Gaussian finite mixture model fitted by EM algorithm ## ---------------------------------------------------- ## ## Mclust EEI (diagonal, equal volume and shape) model ## with 5 components: ## ## log-likelihood n df BIC ICL ## -91.3 75 16 -252 -252 ## ## Clustering table: ## 1 2 3 4 5 ## 23 20 15 3 14 plot(m, what = &quot;classification&quot;) Rerun with a fixed number of 4 clusters m &lt;- Mclust(ruspini_scaled, G=4) summary(m) ## ---------------------------------------------------- ## Gaussian finite mixture model fitted by EM algorithm ## ---------------------------------------------------- ## ## Mclust EEI (diagonal, equal volume and shape) model ## with 4 components: ## ## log-likelihood n df BIC ICL ## -102 75 13 -259 -259 ## ## Clustering table: ## 1 2 3 4 ## 23 20 15 17 plot(m, what = &quot;classification&quot;) 7.2.6 Spectral clustering Spectral clustering works by embedding the data points of the partitioning problem into the subspace of the k largest eigenvectors of a normalized affinity/kernel matrix. Then uses a simple clustering method like k-means. library(&quot;kernlab&quot;) ## ## Attaching package: &#39;kernlab&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## alpha ## The following object is masked from &#39;package:arules&#39;: ## ## size ## The following object is masked from &#39;package:purrr&#39;: ## ## cross ## The following object is masked from &#39;package:ggplot2&#39;: ## ## alpha cluster_spec &lt;- specc(as.matrix(ruspini_scaled), centers = 4) cluster_spec ## Spectral Clustering object of class &quot;specc&quot; ## ## Cluster memberships: ## ## 1 3 3 2 2 3 4 1 1 1 2 1 2 3 4 2 3 3 4 3 1 2 4 1 4 1 2 4 2 1 4 3 2 3 3 4 4 1 4 3 1 4 1 1 3 2 2 1 1 3 4 1 3 1 2 3 1 2 1 3 4 3 3 4 4 1 3 2 4 1 1 2 3 4 1 ## ## Gaussian Radial Basis kernel function. ## Hyperparameter : sigma = 41.7670067458421 ## ## Centers: ## [,1] [,2] ## [1,] -0.360 1.109 ## [2,] 0.461 -1.491 ## [3,] -1.139 -0.556 ## [4,] 1.419 0.469 ## ## Cluster size: ## [1] 23 15 20 17 ## ## Within-cluster sum of squares: ## [1] 53.27 53.27 8.81 18.84 ggplot(ruspini_scaled %&gt;% add_column(cluster = factor(cluster_spec)), aes(x, y, color = cluster)) + geom_point() 7.2.7 Fuzzy C-Means Clustering The fuzzy clustering version of the k-means clustering problem. Each data point has a degree of membership to for each cluster. library(&quot;e1071&quot;) cluster_cmeans &lt;- cmeans(as.matrix(ruspini_scaled), centers = 4) cluster_cmeans ## Fuzzy c-means clustering with 4 clusters ## ## Cluster centers: ## x y ## 1 -1.137 -0.555 ## 2 0.455 -1.476 ## 3 1.505 0.516 ## 4 -0.376 1.114 ## ## Memberships: ## 1 2 3 4 ## [1,] 0.012065 0.004750 7.76e-03 9.75e-01 ## [2,] 0.866509 0.074035 2.11e-02 3.84e-02 ## [3,] 0.971282 0.010239 4.91e-03 1.36e-02 ## [4,] 0.024935 0.947252 1.65e-02 1.14e-02 ## [5,] 0.020593 0.950361 1.82e-02 1.09e-02 ## [6,] 0.992095 0.003402 1.36e-03 3.14e-03 ## [7,] 0.039260 0.053619 8.11e-01 9.62e-02 ## [8,] 0.037605 0.013313 1.97e-02 9.29e-01 ## [9,] 0.024784 0.013940 3.40e-02 9.27e-01 ## [10,] 0.025639 0.010355 1.73e-02 9.47e-01 ## [11,] 0.008241 0.983990 4.42e-03 3.35e-03 ## [12,] 0.001560 0.000705 1.32e-03 9.96e-01 ## [13,] 0.003861 0.992177 2.30e-03 1.66e-03 ## [14,] 0.768380 0.097124 4.14e-02 9.31e-02 ## [15,] 0.005870 0.009963 9.73e-01 1.13e-02 ## [16,] 0.024150 0.952363 1.34e-02 1.01e-02 ## [17,] 0.828839 0.045276 2.77e-02 9.82e-02 ## [18,] 0.904502 0.033979 1.64e-02 4.51e-02 ## [19,] 0.003221 0.004747 9.85e-01 7.44e-03 ## [20,] 0.934346 0.027260 1.15e-02 2.69e-02 ## [21,] 0.003385 0.001497 2.77e-03 9.92e-01 ## [22,] 0.020387 0.949234 1.93e-02 1.11e-02 ## [23,] 0.107506 0.177387 5.41e-01 1.74e-01 ## [24,] 0.011470 0.004817 8.41e-03 9.75e-01 ## [25,] 0.018433 0.031839 9.16e-01 3.39e-02 ## [26,] 0.004627 0.002182 4.27e-03 9.89e-01 ## [27,] 0.003167 0.993633 1.85e-03 1.35e-03 ## [28,] 0.000609 0.000943 9.97e-01 1.32e-03 ## [29,] 0.028738 0.947019 1.34e-02 1.08e-02 ## [30,] 0.071388 0.050971 1.76e-01 7.02e-01 ## [31,] 0.000250 0.000411 9.99e-01 5.07e-04 ## [32,] 0.939767 0.029086 1.01e-02 2.10e-02 ## [33,] 0.000110 0.999766 7.43e-05 5.05e-05 ## [34,] 0.860429 0.059383 2.50e-02 5.52e-02 ## [35,] 0.895316 0.033633 1.80e-02 5.31e-02 ## [36,] 0.065465 0.118857 7.06e-01 1.10e-01 ## [37,] 0.128305 0.183755 4.70e-01 2.18e-01 ## [38,] 0.011252 0.005928 1.35e-02 9.69e-01 ## [39,] 0.007575 0.013540 9.65e-01 1.39e-02 ## [40,] 0.890088 0.054964 1.83e-02 3.66e-02 ## [41,] 0.067223 0.044821 1.33e-01 7.55e-01 ## [42,] 0.022924 0.040523 8.96e-01 4.09e-02 ## [43,] 0.009541 0.004635 9.54e-03 9.76e-01 ## [44,] 0.048384 0.016805 2.45e-02 9.10e-01 ## [45,] 0.914871 0.040505 1.46e-02 3.00e-02 ## [46,] 0.049811 0.912543 2.04e-02 1.73e-02 ## [47,] 0.038484 0.892180 4.59e-02 2.34e-02 ## [48,] 0.004484 0.002237 4.75e-03 9.89e-01 ## [49,] 0.015164 0.007890 1.73e-02 9.60e-01 ## [50,] 0.872757 0.063345 2.13e-02 4.26e-02 ## [51,] 0.006153 0.008725 9.70e-01 1.48e-02 ## [52,] 0.075851 0.025668 3.63e-02 8.62e-01 ## [53,] 0.942647 0.022073 9.90e-03 2.54e-02 ## [54,] 0.041983 0.015519 2.38e-02 9.19e-01 ## [55,] 0.017339 0.959100 1.45e-02 9.02e-03 ## [56,] 0.998933 0.000436 1.84e-04 4.47e-04 ## [57,] 0.020461 0.011470 2.85e-02 9.40e-01 ## [58,] 0.018343 0.953743 1.78e-02 1.02e-02 ## [59,] 0.037153 0.014629 2.37e-02 9.25e-01 ## [60,] 0.962608 0.013809 6.49e-03 1.71e-02 ## [61,] 0.013081 0.020545 9.40e-01 2.68e-02 ## [62,] 0.930263 0.035820 1.14e-02 2.25e-02 ## [63,] 0.954076 0.015519 7.84e-03 2.26e-02 ## [64,] 0.010680 0.019237 9.51e-01 1.93e-02 ## [65,] 0.039416 0.046127 7.88e-01 1.27e-01 ## [66,] 0.000964 0.000451 8.88e-04 9.98e-01 ## [67,] 0.973167 0.012776 4.51e-03 9.55e-03 ## [68,] 0.025463 0.953144 1.19e-02 9.53e-03 ## [69,] 0.003456 0.005041 9.83e-01 8.07e-03 ## [70,] 0.010326 0.004135 6.88e-03 9.79e-01 ## [71,] 0.033362 0.019994 5.51e-02 8.92e-01 ## [72,] 0.003079 0.993497 2.03e-03 1.40e-03 ## [73,] 0.887734 0.043108 2.00e-02 4.92e-02 ## [74,] 0.001160 0.001840 9.95e-01 2.46e-03 ## [75,] 0.092067 0.051905 1.05e-01 7.51e-01 ## ## Closest hard clustering: ## [1] 4 1 1 2 2 1 3 4 4 4 2 4 2 1 3 2 1 1 3 1 4 2 3 4 3 ## [26] 4 2 3 2 4 3 1 2 1 1 3 3 4 3 1 4 3 4 4 1 2 2 4 4 1 ## [51] 3 4 1 4 2 1 4 2 4 1 3 1 1 3 3 4 1 2 3 4 4 2 1 3 4 ## ## Available components: ## [1] &quot;centers&quot; &quot;size&quot; &quot;cluster&quot; ## [4] &quot;membership&quot; &quot;iter&quot; &quot;withinerror&quot; ## [7] &quot;call&quot; Plot membership (shown as small pie charts) library(&quot;scatterpie&quot;) ggplot() + geom_scatterpie(data = cbind(ruspini_scaled, cluster_cmeans$membership), aes(x = x, y = y), cols = colnames(cluster_cmeans$membership), legend_name = &quot;Membership&quot;) + coord_equal() 7.3 Internal Cluster Validation 7.3.1 Compare the Clustering Quality The two most popular quality metrics are the within-cluster sum of squares (WCSS) used by \\(k\\)-means and the average silhouette width. Look at within.cluster.ss and avg.silwidth below. ##library(fpc) Notes: * I do not load fpc since the NAMESPACE overwrites dbscan. * The clustering (second argument below) has to be supplied as a vector with numbers (cluster IDs) and cannot be a factor (use as.integer() to convert the factor to an ID). fpc::cluster.stats(d, km$cluster) ## $n ## [1] 75 ## ## $cluster.number ## [1] 4 ## ## $cluster.size ## [1] 17 23 15 20 ## ## $min.cluster.size ## [1] 15 ## ## $noisen ## [1] 0 ## ## $diameter ## [1] 1.463 1.159 0.836 1.119 ## ## $average.distance ## [1] 0.581 0.429 0.356 0.482 ## ## $median.distance ## [1] 0.502 0.393 0.338 0.449 ## ## $separation ## [1] 0.768 0.768 1.158 1.158 ## ## $average.toother ## [1] 2.29 2.15 2.31 2.16 ## ## $separation.matrix ## [,1] [,2] [,3] [,4] ## [1,] 0.000 0.768 1.31 1.34 ## [2,] 0.768 0.000 1.96 1.22 ## [3,] 1.308 1.958 0.00 1.16 ## [4,] 1.340 1.220 1.16 0.00 ## ## $ave.between.matrix ## [,1] [,2] [,3] [,4] ## [1,] 0.00 1.92 2.22 2.77 ## [2,] 1.92 0.00 2.75 1.89 ## [3,] 2.22 2.75 0.00 1.87 ## [4,] 2.77 1.89 1.87 0.00 ## ## $average.between ## [1] 2.22 ## ## $average.within ## [1] 0.463 ## ## $n.between ## [1] 2091 ## ## $n.within ## [1] 684 ## ## $max.diameter ## [1] 1.46 ## ## $min.separation ## [1] 0.768 ## ## $within.cluster.ss ## [1] 10.1 ## ## $clus.avg.silwidths ## 1 2 3 4 ## 0.681 0.745 0.807 0.721 ## ## $avg.silwidth ## [1] 0.737 ## ## $g2 ## NULL ## ## $g3 ## NULL ## ## $pearsongamma ## [1] 0.842 ## ## $dunn ## [1] 0.525 ## ## $dunn2 ## [1] 3.23 ## ## $entropy ## [1] 1.37 ## ## $wb.ratio ## [1] 0.209 ## ## $ch ## [1] 324 ## ## $cwidegap ## [1] 0.415 0.315 0.235 0.261 ## ## $widestgap ## [1] 0.415 ## ## $sindex ## [1] 0.858 ## ## $corrected.rand ## NULL ## ## $vi ## NULL Read ? cluster.stats for an explanation of all the available indices. sapply( list( km = km$cluster, hc_compl = cutree(hc, k = 4), hc_single = cutree(hc_single, k = 4) ), FUN = function(x) fpc::cluster.stats(d, x))[c(&quot;within.cluster.ss&quot;, &quot;avg.silwidth&quot;), ] ## km hc_compl hc_single ## within.cluster.ss 10.1 10.1 10.1 ## avg.silwidth 0.737 0.737 0.737 7.3.2 Silhouette plot library(cluster) plot(silhouette(km$cluster, d)) Note: The silhouette plot does not show correctly in R Studio if you have too many objects (bars are missing). I will work when you open a new plotting device with windows(), x11() or quartz(). ggplot visualization using factoextra fviz_silhouette(silhouette(km$cluster, d)) ## cluster size ave.sil.width ## 1 1 17 0.68 ## 2 2 23 0.75 ## 3 3 15 0.81 ## 4 4 20 0.72 7.3.3 Find Optimal Number of Clusters for k-means ggplot(ruspini_scaled, aes(x, y)) + geom_point() ## We will use different methods and try 1-10 clusters. set.seed(1234) ks &lt;- 2:10 7.3.3.1 Elbow Method: Within-Cluster Sum of Squares Calculate the within-cluster sum of squares for different numbers of clusters and look for the knee or elbow in the plot. (nstart = 5 just repeats k-means 5 times and returns the best solution) WCSS &lt;- sapply(ks, FUN = function(k) { kmeans(ruspini_scaled, centers = k, nstart = 5)$tot.withinss }) ggplot(as_tibble(ks, WCSS), aes(ks, WCSS)) + geom_line() + geom_vline(xintercept = 4, color = &quot;red&quot;, linetype = 2) 7.3.3.2 Average Silhouette Width Plot the average silhouette width for different number of clusters and look for the maximum in the plot. ASW &lt;- sapply(ks, FUN=function(k) { fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart = 5)$cluster)$avg.silwidth }) best_k &lt;- ks[which.max(ASW)] best_k ## [1] 4 ggplot(as_tibble(ks, ASW), aes(ks, ASW)) + geom_line() + geom_vline(xintercept = best_k, color = &quot;red&quot;, linetype = 2) 7.3.3.3 Dunn Index Use Dunn index (another internal measure given by min. separation/ max. diameter) DI &lt;- sapply(ks, FUN=function(k) { fpc::cluster.stats(d, kmeans(ruspini_scaled, centers=k, nstart=5)$cluster)$dunn }) best_k &lt;- ks[which.max(DI)] ggplot(as_tibble(ks, DI), aes(ks, DI)) + geom_line() + geom_vline(xintercept = best_k, color = &quot;red&quot;, linetype = 2) 7.3.3.4 Gap Statistic Compares the change in within-cluster dispersion with that expected from a null model (see ? clusGap). The default method is to choose the smallest k such that its value Gap(k) is not more than 1 standard error away from the first local maximum. library(cluster) k &lt;- clusGap(ruspini_scaled, FUN = kmeans, nstart = 10, K.max = 10) k ## Clustering Gap statistic [&quot;clusGap&quot;] from call: ## clusGap(x = ruspini_scaled, FUNcluster = kmeans, K.max = 10, nstart = 10) ## B=100 simulated reference sets, k = 1..10; spaceH0=&quot;scaledPCA&quot; ## --&gt; Number of clusters (method &#39;firstSEmax&#39;, SE.factor=1): 4 ## logW E.logW gap SE.sim ## [1,] 3.50 3.47 -0.0308 0.0357 ## [2,] 3.07 3.15 0.0762 0.0374 ## [3,] 2.68 2.90 0.2247 0.0380 ## [4,] 2.11 2.70 0.5971 0.0363 ## [5,] 1.99 2.57 0.5827 0.0347 ## [6,] 1.86 2.45 0.5871 0.0365 ## [7,] 1.73 2.35 0.6156 0.0395 ## [8,] 1.66 2.26 0.5987 0.0413 ## [9,] 1.61 2.17 0.5630 0.0409 ## [10,] 1.50 2.09 0.5910 0.0393 plot(k) Note: these methods can also be used for hierarchical clustering. There have been many other methods and indices proposed to determine the number of clusters. See, e.g., package NbClust. 7.3.4 Visualizing the Distance Matrix ggplot(ruspini_scaled, aes(x, y, color = factor(km$cluster))) + geom_point() d &lt;- dist(ruspini_scaled) Inspect the distance matrix between the first 5 objects. as.matrix(d)[1:5, 1:5] ## 1 2 3 4 5 ## 1 0.00 1.887 1.511 3.041 2.978 ## 2 1.89 0.000 0.522 1.640 1.746 ## 3 1.51 0.522 0.000 2.131 2.207 ## 4 3.04 1.640 2.131 0.000 0.282 ## 5 2.98 1.746 2.207 0.282 0.000 A false-color image visualizes each value in the matrix as a pixel with the color representing the value. library(seriation) pimage(d, col = bluered(100)) Rows and columns are the objects as they are ordered in the data set. The diagonal represents the distance between an object and itself and has by definition a distance of 0 (dark line). Visualizing the unordered distance matrix does not show much structure, but we can reorder the matrix (rows and columns) using the k-means cluster labels from cluster 1 to 4. A clear block structure representing the clusters becomes visible. pimage(d, order=order(km$cluster), col = bluered(100)) Plot function dissplot in package seriation rearranges the matrix and adds lines and cluster labels. In the lower half of the plot, it shows average dissimilarities between clusters. The function organizes the objects by cluster and then reorders clusters and objects within clusters so that more similar objects are closer together. dissplot(d, labels = km$cluster, options=list(main=&quot;k-means with k=4&quot;)) The reordering by dissplot makes the misspecification of k visible as blocks. dissplot(d, labels = kmeans(ruspini_scaled, centers = 3)$cluster, col = bluered(100)) dissplot(d, labels = kmeans(ruspini_scaled, centers = 9)$cluster, col = bluered(100)) Using factoextra fviz_dist(d) 7.4 External Cluster Validation External cluster validation uses ground truth information. That is, the user has an idea how the data should be grouped. This could be a known class label not provided to the clustering algorithm. We use an artificial data set with known groups. library(mlbench) set.seed(1234) shapes &lt;- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05) plot(shapes) Prepare data truth &lt;- as.integer(shapes$class) shapes &lt;- scale(shapes$x) colnames(shapes) &lt;- c(&quot;x&quot;, &quot;y&quot;) shapes &lt;- as_tibble(shapes) ggplot(shapes, aes(x, y)) + geom_point() Find optimal number of Clusters for k-means ks &lt;- 2:20 Use within sum of squares (look for the knee) WCSS &lt;- sapply(ks, FUN = function(k) { kmeans(shapes, centers = k, nstart = 10)$tot.withinss }) ggplot(as_tibble(ks, WCSS), aes(ks, WCSS)) + geom_line() Looks like it could be 7 clusters km &lt;- kmeans(shapes, centers = 7, nstart = 10) ggplot(shapes %&gt;% add_column(cluster = factor(km$cluster)), aes(x, y, color = cluster)) + geom_point() Hierarchical clustering: We use single-link because of the mouth is non-convex and chaining may help. d &lt;- dist(shapes) hc &lt;- hclust(d, method = &quot;single&quot;) Find optimal number of clusters ASW &lt;- sapply(ks, FUN = function(k) { fpc::cluster.stats(d, cutree(hc, k))$avg.silwidth }) ggplot(as_tibble(ks, ASW), aes(ks, ASW)) + geom_line() The maximum is clearly at 4 clusters. hc_4 &lt;- cutree(hc, 4) ggplot(shapes %&gt;% add_column(cluster = factor(hc_4)), aes(x, y, color = cluster)) + geom_point() Compare with ground truth with the corrected (=adjusted) Rand index (ARI), the variation of information (VI) index, entropy and purity. cluster_stats computes ARI and VI as comparative measures. I define functions for entropy and purity here: entropy &lt;- function(cluster, truth) { k &lt;- max(cluster, truth) cluster &lt;- factor(cluster, levels = 1:k) truth &lt;- factor(truth, levels = 1:k) w &lt;- table(cluster)/length(cluster) cnts &lt;- sapply(split(truth, cluster), table) p &lt;- sweep(cnts, 1, rowSums(cnts), &quot;/&quot;) p[is.nan(p)] &lt;- 0 e &lt;- -p * log(p, 2) sum(w * rowSums(e, na.rm = TRUE)) } purity &lt;- function(cluster, truth) { k &lt;- max(cluster, truth) cluster &lt;- factor(cluster, levels = 1:k) truth &lt;- factor(truth, levels = 1:k) w &lt;- table(cluster)/length(cluster) cnts &lt;- sapply(split(truth, cluster), table) p &lt;- sweep(cnts, 1, rowSums(cnts), &quot;/&quot;) p[is.nan(p)] &lt;- 0 sum(w * apply(p, 1, max)) } calculate measures (for comparison we also use random “clusterings” with 4 and 6 clusters) random_4 &lt;- sample(1:4, nrow(shapes), replace = TRUE) random_6 &lt;- sample(1:6, nrow(shapes), replace = TRUE) r &lt;- rbind( kmeans_7 = c( unlist(fpc::cluster.stats(d, km$cluster, truth, compareonly = TRUE)), entropy = entropy(km$cluster, truth), purity = purity(km$cluster, truth) ), hc_4 = c( unlist(fpc::cluster.stats(d, hc_4, truth, compareonly = TRUE)), entropy = entropy(hc_4, truth), purity = purity(hc_4, truth) ), random_4 = c( unlist(fpc::cluster.stats(d, random_4, truth, compareonly = TRUE)), entropy = entropy(random_4, truth), purity = purity(random_4, truth) ), random_6 = c( unlist(fpc::cluster.stats(d, random_6, truth, compareonly = TRUE)), entropy = entropy(random_6, truth), purity = purity(random_6, truth) ) ) r ## corrected.rand vi entropy purity ## kmeans_7 0.63823 0.571 0.229 0.464 ## hc_4 1.00000 0.000 0.000 1.000 ## random_4 -0.00324 2.683 1.988 0.288 ## random_6 -0.00213 3.076 1.728 0.144 Notes: Hierarchical clustering found the perfect clustering. Entropy and purity are heavily impacted by the number of clusters (more clusters improve the metric). The corrected rand index shows clearly that the random clusterings have no relationship with the ground truth (very close to 0). This is a very helpful property. Read ? cluster.stats for an explanation of all the available indices. 7.5 Advanced Data Preparation for Clustering 7.5.1 Outlier Removal Most clustering algorithms perform complete assignment (i.e., all data points need to be assigned to a cluster). Outliers will affect the clustering. It is useful to identify outliers and remove strong outliers prior to clustering. A density based method to identify outlier is LOF (Local Outlier Factor). It is related to dbscan and compares the density around a point with the densities around its neighbors (you have to specify the neighborhood size \\(k\\)). The LOF value for a regular data point is 1. The larger the LOF value gets, the more likely the point is an outlier. library(dbscan) Add a clear outlier to the scaled Ruspini dataset that is 10 standard deviations above the average for the x axis. ruspini_scaled_outlier &lt;- ruspini_scaled %&gt;% add_case(x=10,y=0) 7.5.1.1 Visual inspection of the data Outliers can be identified using summary statistics, histograms, scatterplots (pairs plots), and boxplots, etc. We use here a pairs plot (the diagonal contains smoothed histograms). The outlier is visible as the single separate point in the scatter plot and as the long tail of the smoothed histogram for x (we would expect most observations to fall in the range [-3,3] in normalized data). library(&quot;GGally&quot;) ggpairs(ruspini_scaled_outlier) The outlier is a problem for k-means km &lt;- kmeans(ruspini_scaled_outlier, centers = 4, nstart = 10) ruspini_scaled_outlier_km &lt;- ruspini_scaled_outlier%&gt;% add_column(cluster = factor(km$cluster)) centroids &lt;- as_tibble(km$centers, rownames = &quot;cluster&quot;) ggplot(ruspini_scaled_outlier_km, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10) This problem can be fixed by increasing the number of clusters and removing small clusters in a post-processing step or by identifying and removing outliers before clustering. 7.5.1.2 Local Outlier Factor (LOF) The Local Outlier Factor is related to concepts of DBSCAN can help to identify potential outliers. Calculate the LOF (I choose a neighborhood size of 10 for density estimation), lof &lt;- lof(ruspini_scaled_outlier, k = 10) ## Warning in lof(ruspini_scaled_outlier, k = 10): lof: k ## is now deprecated. use minPts = 11 instead . lof ## [1] 0.978 1.044 0.926 1.024 1.022 0.966 1.154 ## [8] 1.071 1.124 1.058 1.008 0.933 1.000 1.074 ## [15] 1.008 0.987 1.184 0.989 0.984 1.080 0.911 ## [22] 1.019 1.524 0.979 1.045 0.958 1.022 0.934 ## [29] 0.979 1.470 0.964 0.988 0.973 1.236 1.082 ## [36] 1.326 1.566 1.018 0.998 1.029 1.378 1.107 ## [43] 0.952 1.083 1.091 1.029 1.181 1.009 1.031 ## [50] 1.030 1.002 1.201 1.001 1.071 0.968 0.954 ## [57] 1.046 0.970 1.066 1.045 0.989 0.966 1.028 ## [64] 0.991 1.152 0.942 0.977 1.000 0.984 0.998 ## [71] 1.174 0.996 1.116 0.934 1.588 17.027 ggplot(ruspini_scaled_outlier %&gt;% add_column(lof = lof), aes(x, y, color = lof)) + geom_point() + scale_color_gradient(low = &quot;gray&quot;, high = &quot;red&quot;) Plot the points sorted by increasing LOF and look for a knee. ggplot(tibble(index = seq_len(length(lof)), lof = sort(lof)), aes(index, lof)) + geom_line() + geom_hline(yintercept = 1, color = &quot;red&quot;, linetype = 2) Choose a threshold above 1. ggplot(ruspini_scaled_outlier %&gt;% add_column(outlier = lof &gt;= 2), aes(x, y, color = outlier)) + geom_point() ## Analyze the found outliers (they might be interesting data points) and then cluster the data without them. ruspini_scaled_clean &lt;- ruspini_scaled_outlier %&gt;% filter(lof &lt; 2) km &lt;- kmeans(ruspini_scaled_clean, centers = 4, nstart = 10) ruspini_scaled_clean_km &lt;- ruspini_scaled_clean%&gt;% add_column(cluster = factor(km$cluster)) centroids &lt;- as_tibble(km$centers, rownames = &quot;cluster&quot;) ggplot(ruspini_scaled_clean_km, aes(x = x, y = y, color = cluster)) + geom_point() + geom_point(data = centroids, aes(x = x, y = y, color = cluster), shape = 3, size = 10) There are many other outlier removal strategies available. See, e.g., package outliers. 7.5.2 Clustering Tendency Most clustering algorithms will always produce a clustering, even if the data does not contain a cluster structure. It is typically good to check cluster tendency before attempting to cluster the data. We use again the smiley data. library(mlbench) shapes &lt;- mlbench.smiley(n = 500, sd1 = 0.1, sd2 = 0.05)$x colnames(shapes) &lt;- c(&quot;x&quot;, &quot;y&quot;) shapes &lt;- as_tibble(shapes) 7.5.2.1 Scatter plots The first step is visual inspection using scatter plots. ggplot(shapes, aes(x = x, y = y)) + geom_point() Cluster tendency is typically indicated by several separated point clouds. Often an appropriate number of clusters can also be visually obtained by counting the number of point clouds. We see four clusters, but the mouth is not convex/spherical and thus will pose a problems to algorithms like k-means. If the data has more than two features then you can use a pairs plot (scatterplot matrix) or look at a scatterplot of the first two principal components using PCA. #### Visual Analysis for Cluster Tendency Assessment (VAT) VAT reorders the objects to show potential clustering tendency as a block structure (dark blocks along the main diagonal). We scale the data before using Euclidean distance. library(seriation) d_shapes &lt;- dist(scale(shapes)) VAT(d_shapes, col = bluered(100)) iVAT uses the largest distances for all possible paths between two objects instead of the direct distances to make the block structure better visible. iVAT(d_shapes, col = bluered(100)) 7.5.2.2 Hopkins statistic factoextra can also create a VAT plot and calculate the Hopkins statistic to assess clustering tendency. For the Hopkins statistic, a sample of size \\(n\\) is drawn from the data and then compares the nearest neighbor distribution with a simulated dataset drawn from a random uniform distribution (see detailed explanation). A values &gt;.5 indicates usually a clustering tendency. get_clust_tendency(shapes, n = 10) ## $hopkins_stat ## [1] 0.907 ## ## $plot Both plots show a strong cluster structure with 4 clusters. 7.5.2.3 Data Without Clustering Tendency data_random &lt;- tibble(x = runif(500), y = runif(500)) ggplot(data_random, aes(x, y)) + geom_point() No point clouds are visible, just noise. d_random &lt;- dist(data_random) VAT(d_random, col = bluered(100)) iVAT(d_random, col = bluered(100)) get_clust_tendency(data_random, n = 10, graph = FALSE) ## $hopkins_stat ## [1] 0.464 ## ## $plot ## NULL There is very little clustering structure visible indicating low clustering tendency and clustering should not be performed on this data. However, k-means can be used to partition the data into \\(k\\) regions of roughly equivalent size. This can be used as a data-driven discretization of the space. 7.5.2.4 k-means on Data Without Clustering Tendency What happens if we perform k-means on data that has no inherent clustering structure? km &lt;- kmeans(data_random, centers = 4) random_clustered&lt;- data_random %&gt;% add_column(cluster = factor(km$cluster)) ggplot(random_clustered, aes(x = x, y = y, color = cluster)) + geom_point() k-means discretizes the space into similarly sized regions. References Fraley, C., Raftery, A. E., &amp; Scrucca, L. (2020). Mclust: Gaussian mixture modelling for model-based clustering, classification, and density estimation. https://mclust-org.github.io/mclust/ Hahsler, M., Buchta, C., &amp; Hornik, K. (2021). Seriation: Infrastructure for ordering objects using seriation. https://github.com/mhahsler/seriation Hahsler, M., &amp; Piekenbrock, M. (2021). Dbscan: Density based clustering of applications with noise (DBSCAN) and related algorithms. https://github.com/mhahsler/dbscan Hennig, C. (2020). Fpc: Flexible procedures for clustering. https://www.unibo.it/sitoweb/christian.hennig/en/ Karatzoglou, A., Smola, A., &amp; Hornik, K. (2019). Kernlab: Kernel-based machine learning lab. https://CRAN.R-project.org/package=kernlab Kassambara, A., &amp; Mundt, F. (2020). Factoextra: Extract and visualize the results of multivariate data analyses. http://www.sthda.com/english/rpkgs/factoextra Leisch, F., &amp; Dimitriadou., E. (2021). Mlbench: Machine learning benchmark problems. https://CRAN.R-project.org/package=mlbench Maechler, M., Rousseeuw, P., Struyf, A., &amp; Hubert, M. (2021). Cluster: \"Finding groups in data\": Cluster analysis extended rousseeuw et al. https://svn.r-project.org/R-packages/trunk/cluster/ Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., &amp; Leisch, F. (2021). e1071: Misc functions of the department of statistics, probability theory group (formerly: E1071), TU wien. https://CRAN.R-project.org/package=e1071 Schloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp; Crowley, J. (2021). GGally: Extension to ggplot2. https://CRAN.R-project.org/package=GGally Tan, P.-N., Steinbach, M. S., &amp; Kumar, V. (2005). Introduction to data mining (1st Edition). Addison-Wesley. https://www-users.cs.umn.edu/~kumar001/dmbook/firsted.php Wickham, H. (2021c). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse Yu, G. (2021). Scatterpie: Scatter pie plot. https://CRAN.R-project.org/package=scatterpie "],["references.html", "References", " References Allaire, J., &amp; Chollet, F. (2021). Keras: R interface to keras. https://keras.rstudio.com Bates, D., &amp; Maechler, M. (2021). Matrix: Sparse and dense matrix classes and methods. http://Matrix.R-forge.R-project.org/ Breiman, L., Cutler, A., Liaw, A., &amp; Wiener, M. (2018). randomForest: Breiman and cutler’s random forests for classification and regression. https://www.stat.berkeley.edu/~breiman/RandomForests/ Fraley, C., Raftery, A. E., &amp; Scrucca, L. (2020). Mclust: Gaussian mixture modelling for model-based clustering, classification, and density estimation. https://mclust-org.github.io/mclust/ Hahsler, M. (2017). ArulesViz: Interactive visualization of association rules with R. R Journal, 9(2), 163–175. https://doi.org/10.32614/RJ-2017-047 Hahsler, M. (2021a). arulesViz: Visualizing association rules and frequent itemsets. https://github.com/mhahsler/arulesViz Hahsler, M. (2021b). R companion for the textbook introduction to data mining. Online Book. https://mhahsler.github.io/Introduction_to_Data_Mining_R_Examples/book Hahsler, M., Buchta, C., Gruen, B., &amp; Hornik, K. (2021). Arules: Mining association rules and frequent itemsets. https://github.com/mhahsler/arules Hahsler, M., Buchta, C., &amp; Hornik, K. (2021). Seriation: Infrastructure for ordering objects using seriation. https://github.com/mhahsler/seriation Hahsler, M., Chelluboina, S., Hornik, K., &amp; Buchta, C. (2011). The arules r-package ecosystem: Analyzing interesting patterns from large transaction datasets. Journal of Machine Learning Research, 12, 1977–1981. https://jmlr.csail.mit.edu/papers/v12/hahsler11a.html Hahsler, M., Gruen, B., &amp; Hornik, K. (2005). Arules – A computational environment for mining association rules and frequent item sets. Journal of Statistical Software, 14(15), 1–25. https://doi.org/10.18637/jss.v014.i15 Hahsler, M., Hornik, K., &amp; Buchta, C. (2008). Getting things in order: An introduction to the r package seriation. Journal of Statistical Software, 25(3), 1–34. https://doi.org/10.18637/jss.v025.i03 Hahsler, M., &amp; Piekenbrock, M. (2021). Dbscan: Density based clustering of applications with noise (DBSCAN) and related algorithms. https://github.com/mhahsler/dbscan Hahsler, M., Piekenbrock, M., &amp; Doran, D. (2019). dbscan: Fast density-based clustering with R. Journal of Statistical Software, 91(1), 1–30. https://doi.org/10.18637/jss.v091.i01 Hennig, C. (2020). Fpc: Flexible procedures for clustering. https://www.unibo.it/sitoweb/christian.hennig/en/ Henry, L., &amp; Wickham, H. (2020). Purrr: Functional programming tools. https://CRAN.R-project.org/package=purrr Hornik, K. (2020). RWeka: R/weka interface. https://CRAN.R-project.org/package=RWeka Hornik, K., Buchta, C., &amp; Zeileis, A. (2009). Open-source machine learning: R meets Weka. Computational Statistics, 24(2), 225–232. https://doi.org/10.1007/s00180-008-0119-7 Karatzoglou, A., Smola, A., &amp; Hornik, K. (2019). Kernlab: Kernel-based machine learning lab. https://CRAN.R-project.org/package=kernlab Karatzoglou, A., Smola, A., Hornik, K., &amp; Zeileis, A. (2004). Kernlab – an S4 package for kernel methods in R. Journal of Statistical Software, 11(9), 1–20. http://www.jstatsoft.org/v11/i09/ Kassambara, A. (2019). Ggcorrplot: Visualization of a correlation matrix using ggplot2. http://www.sthda.com/english/wiki/ggcorrplot Kassambara, A., &amp; Mundt, F. (2020). Factoextra: Extract and visualize the results of multivariate data analyses. http://www.sthda.com/english/rpkgs/factoextra Kuhn, M. (2021). Caret: Classification and regression training. https://github.com/topepo/caret/ Kuhn, M., &amp; Quinlan, R. (2021). C50: C5.0 decision trees and rule-based models. https://topepo.github.io/C5.0/ Leisch, F., &amp; Dimitriadou., E. (2021). Mlbench: Machine learning benchmark problems. https://CRAN.R-project.org/package=mlbench Liaw, A., &amp; Wiener, M. (2002). Classification and regression by randomForest. R News, 2(3), 18–22. https://CRAN.R-project.org/doc/Rnews/ Maechler, M., Rousseeuw, P., Struyf, A., &amp; Hubert, M. (2021). Cluster: \"Finding groups in data\": Cluster analysis extended rousseeuw et al. https://svn.r-project.org/R-packages/trunk/cluster/ Meyer, D., &amp; Buchta, C. (2021). Proxy: Distance and similarity measures. https://CRAN.R-project.org/package=proxy Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., &amp; Leisch, F. (2021). e1071: Misc functions of the department of statistics, probability theory group (formerly: E1071), TU wien. https://CRAN.R-project.org/package=e1071 Milborrow, S. (2020). Rpart.plot: Plot rpart models: An enhanced version of plot.rpart. http://www.milbo.org/rpart-plot/index.html Müller, K., &amp; Wickham, H. (2021). Tibble: Simple data frames. https://CRAN.R-project.org/package=tibble Newman, D. J., Hettich, S., Blake, C. L., &amp; Merz, C. J. (1998). UCI repository of machine learning databases. University of California, Irvine, Dept. of Information; Computer Sciences. http://www.ics.uci.edu/~mlearn/MLRepository.html R Core Team. (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing. https://www.R-project.org/ Ripley, B. (2021a). MASS: Support functions and datasets for venables and ripley’s MASS. http://www.stats.ox.ac.uk/pub/MASS4/ Ripley, B. (2021b). Nnet: Feed-forward neural networks and multinomial log-linear models. http://www.stats.ox.ac.uk/pub/MASS4/ Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., &amp; Müller, M. (2011). pROC: An open-source package for r and s+ to analyze and compare ROC curves. BMC Bioinformatics, 12, 77. Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F., Sanchez, J.-C., &amp; Müller, M. (2021). pROC: Display and analyze ROC curves. http://expasy.org/tools/pROC/ Romanski, P., Kotthoff, L., &amp; Schratz, P. (2021). FSelector: Selecting attributes. https://github.com/larskotthoff/fselector Sarkar, D. (2008). Lattice: Multivariate data visualization with r. Springer. http://lmdvr.r-forge.r-project.org Sarkar, D. (2021). Lattice: Trellis graphics for r. http://lattice.r-forge.r-project.org/ Schloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp; Crowley, J. (2021). GGally: Extension to ggplot2. https://CRAN.R-project.org/package=GGally Scrucca, L., Fop, M., Murphy, T. B., &amp; Raftery, A. E. (2016). mclust 5: Clustering, classification and density estimation using Gaussian finite mixture models. The R Journal, 8(1), 289–317. https://doi.org/10.32614/RJ-2016-021 Sievert, C. (2020). Interactive web-based data visualization with r, plotly, and shiny. Chapman; Hall/CRC. https://plotly-r.com Sievert, C., Parmer, C., Hocking, T., Chamberlain, S., Ram, K., Corvellec, M., &amp; Despouy, P. (2021). Plotly: Create interactive web graphics via plotly.js. https://CRAN.R-project.org/package=plotly Tan, P.-N., Steinbach, M. S., Karpatne, A., &amp; Kumar, V. (2017). Introduction to data mining (2nd Edition). Pearson. https://www-users.cs.umn.edu/~kumar001/dmbook Tan, P.-N., Steinbach, M. S., &amp; Kumar, V. (2005). Introduction to data mining (1st Edition). Addison-Wesley. https://www-users.cs.umn.edu/~kumar001/dmbook/firsted.php Therneau, T., &amp; Atkinson, B. (2019). Rpart: Recursive partitioning and regression trees. https://CRAN.R-project.org/package=rpart Tillé, Y., &amp; Matei, A. (2021). Sampling: Survey sampling. https://CRAN.R-project.org/package=sampling Venables, W. N., &amp; Ripley, B. D. (2002a). Modern applied statistics with s (Fourth). Springer. https://www.stats.ox.ac.uk/pub/MASS4/ Venables, W. N., &amp; Ripley, B. D. (2002b). Modern applied statistics with s (Fourth). Springer. https://www.stats.ox.ac.uk/pub/MASS4/ Venables, W. N., Smith, D. M., &amp; the R Core Team. (2021). An introduction to R. Wickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org Wickham, H. (2019). Stringr: Simple, consistent wrappers for common string operations. https://CRAN.R-project.org/package=stringr Wickham, H. (2021a). Forcats: Tools for working with categorical variables (factors). https://CRAN.R-project.org/package=forcats Wickham, H. (2021b). Tidyr: Tidy messy data. https://CRAN.R-project.org/package=tidyr Wickham, H. (2021c). Tidyverse: Easily install and load the tidyverse. https://CRAN.R-project.org/package=tidyverse Wickham, H., Averick, M., Bryan, J., Chang, W., McGowan, L. D., François, R., Grolemund, G., Hayes, A., Henry, L., Hester, J., Kuhn, M., Pedersen, T. L., Miller, E., Bache, S. M., Müller, K., Ooms, J., Robinson, D., Seidel, D. P., Spinu, V., … Yutani, H. (2019). Welcome to the tidyverse. Journal of Open Source Software, 4(43), 1686. https://doi.org/10.21105/joss.01686 Wickham, H., Chang, W., Henry, L., Pedersen, T. L., Takahashi, K., Wilke, C., Woo, K., Yutani, H., &amp; Dunnington, D. (2021). ggplot2: Create elegant data visualisations using the grammar of graphics. https://CRAN.R-project.org/package=ggplot2 Wickham, H., François, R., Henry, L., &amp; Müller, K. (2021). Dplyr: A grammar of data manipulation. https://CRAN.R-project.org/package=dplyr Wickham, H., &amp; Grolemund, G. (2017). R for data science: Import, tidy, transform, visualize, and model data (1st ed.). O’Reilly Media, Inc. https://r4ds.had.co.nz/ Wickham, H., &amp; Hester, J. (2020). Readr: Read rectangular text data. https://CRAN.R-project.org/package=readr Wickham, H., &amp; Seidel, D. (2020). Scales: Scale functions for visualization. https://CRAN.R-project.org/package=scales Wilkinson, L. (2005). The grammar of graphics (statistics and computing). Springer-Verlag. https://doi.org/10.1007/0-387-28695-0 Witten, I. H., &amp; Frank, E. (2005). Data mining: Practical machine learning tools and techniques (2nd ed.). Morgan Kaufmann. Yu, G. (2021). Scatterpie: Scatter pie plot. https://CRAN.R-project.org/package=scatterpie "]]
