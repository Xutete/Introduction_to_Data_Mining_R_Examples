<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Clustering Analysis | R Companion for the Textbook Introduction to Data Mining</title>
  <meta name="description" content="This book contains documented R examples to accompany several chapters of the popular data mining text book Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach and Vipin Kumar (1st or 2nd edition)." />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Clustering Analysis | R Companion for the Textbook Introduction to Data Mining" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book contains documented R examples to accompany several chapters of the popular data mining text book Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach and Vipin Kumar (1st or 2nd edition)." />
  <meta name="github-repo" content="mhahsler/Introduction_to_Data_Mining_R_Examples" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Clustering Analysis | R Companion for the Textbook Introduction to Data Mining" />
  
  <meta name="twitter:description" content="This book contains documented R examples to accompany several chapters of the popular data mining text book Introduction to Data Mining by Pang-Ning Tan, Michael Steinbach and Vipin Kumar (1st or 2nd edition)." />
  

<meta name="author" content="Michael Hahsler" />


<meta name="date" content="2021-07-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="association-analysis-advanced-concepts.html"/>
<link rel="next" href="references.html"/>
<script src="libs/header-attrs/header-attrs.js"></script>
<script src="libs/jquery/jquery.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>
<script src="libs/htmlwidgets/htmlwidgets.js"></script>
<script src="libs/plotly-binding/plotly.js"></script>
<script src="libs/typedarray/typedarray.min.js"></script>
<link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main/plotly-latest.min.js"></script>
<link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding/datatables.js"></script>
<link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
<link href="libs/nouislider/jquery.nouislider.min.css" rel="stylesheet" />
<script src="libs/nouislider/jquery.nouislider.min.js"></script>
<link href="libs/selectize/selectize.bootstrap3.css" rel="stylesheet" />
<script src="libs/selectize/selectize.min.js"></script>
<link href="libs/vis/vis.css" rel="stylesheet" />
<script src="libs/vis/vis.min.js"></script>
<script src="libs/visNetwork-binding/visNetwork.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R Companion for Data Mining</a></li>
<li><a href="http://michael.hahsler.net">by Michael Hahsler</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>2</b> Data</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data.html"><a href="data.html#the-iris-dataset"><i class="fa fa-check"></i><b>2.1</b> The Iris Dataset</a></li>
<li class="chapter" data-level="2.2" data-path="data.html"><a href="data.html#data-quality"><i class="fa fa-check"></i><b>2.2</b> Data Quality</a></li>
<li class="chapter" data-level="2.3" data-path="data.html"><a href="data.html#aggregation"><i class="fa fa-check"></i><b>2.3</b> Aggregation</a></li>
<li class="chapter" data-level="2.4" data-path="data.html"><a href="data.html#sampling"><i class="fa fa-check"></i><b>2.4</b> Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="data.html"><a href="data.html#random-sampling"><i class="fa fa-check"></i><b>2.4.1</b> Random Sampling</a></li>
<li class="chapter" data-level="2.4.2" data-path="data.html"><a href="data.html#stratified-sampling"><i class="fa fa-check"></i><b>2.4.2</b> Stratified Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="data.html"><a href="data.html#features"><i class="fa fa-check"></i><b>2.5</b> Features</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="data.html"><a href="data.html#dimensionality-reduction"><i class="fa fa-check"></i><b>2.5.1</b> Dimensionality Reduction</a></li>
<li class="chapter" data-level="2.5.2" data-path="data.html"><a href="data.html#feature-selection"><i class="fa fa-check"></i><b>2.5.2</b> Feature Selection</a></li>
<li class="chapter" data-level="2.5.3" data-path="data.html"><a href="data.html#discretize-features"><i class="fa fa-check"></i><b>2.5.3</b> Discretize Features</a></li>
<li class="chapter" data-level="2.5.4" data-path="data.html"><a href="data.html#standardize-data-z-scores"><i class="fa fa-check"></i><b>2.5.4</b> Standardize Data (Z-Scores)</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="data.html"><a href="data.html#proximities-similarities-and-distances"><i class="fa fa-check"></i><b>2.6</b> Proximities: Similarities and Distances</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="data.html"><a href="data.html#minkowsky-distances"><i class="fa fa-check"></i><b>2.6.1</b> Minkowsky Distances</a></li>
<li class="chapter" data-level="2.6.2" data-path="data.html"><a href="data.html#distances-for-binary-data"><i class="fa fa-check"></i><b>2.6.2</b> Distances for Binary Data</a></li>
<li class="chapter" data-level="2.6.3" data-path="data.html"><a href="data.html#distances-for-mixed-data"><i class="fa fa-check"></i><b>2.6.3</b> Distances for Mixed Data</a></li>
<li class="chapter" data-level="2.6.4" data-path="data.html"><a href="data.html#additional-proximity-measures-available-in-package-proxy"><i class="fa fa-check"></i><b>2.6.4</b> Additional proximity Measures Available in Package proxy</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="data.html"><a href="data.html#relationships-between-features"><i class="fa fa-check"></i><b>2.7</b> Relationships Between Features</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="data.html"><a href="data.html#correlation"><i class="fa fa-check"></i><b>2.7.1</b> Correlation</a></li>
<li class="chapter" data-level="2.7.2" data-path="data.html"><a href="data.html#rank-correlation"><i class="fa fa-check"></i><b>2.7.2</b> Rank Correlation</a></li>
<li class="chapter" data-level="2.7.3" data-path="data.html"><a href="data.html#relationship-between-nominal-and-ordinal-features"><i class="fa fa-check"></i><b>2.7.3</b> Relationship Between Nominal and Ordinal Features</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="data.html"><a href="data.html#density-estimation"><i class="fa fa-check"></i><b>2.8</b> Density Estimation</a></li>
<li class="chapter" data-level="2.9" data-path="data.html"><a href="data.html#exploring-data"><i class="fa fa-check"></i><b>2.9</b> Exploring Data</a>
<ul>
<li class="chapter" data-level="2.9.1" data-path="data.html"><a href="data.html#basic-statistics"><i class="fa fa-check"></i><b>2.9.1</b> Basic statistics</a></li>
<li class="chapter" data-level="2.9.2" data-path="data.html"><a href="data.html#tabulate-data"><i class="fa fa-check"></i><b>2.9.2</b> Tabulate data</a></li>
<li class="chapter" data-level="2.9.3" data-path="data.html"><a href="data.html#percentiles-quantiles"><i class="fa fa-check"></i><b>2.9.3</b> Percentiles (Quantiles)</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="data.html"><a href="data.html#visualization"><i class="fa fa-check"></i><b>2.10</b> Visualization</a>
<ul>
<li class="chapter" data-level="2.10.1" data-path="data.html"><a href="data.html#histogram"><i class="fa fa-check"></i><b>2.10.1</b> Histogram</a></li>
<li class="chapter" data-level="2.10.2" data-path="data.html"><a href="data.html#boxplot"><i class="fa fa-check"></i><b>2.10.2</b> Boxplot</a></li>
<li class="chapter" data-level="2.10.3" data-path="data.html"><a href="data.html#scatter-plot"><i class="fa fa-check"></i><b>2.10.3</b> Scatter plot</a></li>
<li class="chapter" data-level="2.10.4" data-path="data.html"><a href="data.html#scatter-plot-matrix"><i class="fa fa-check"></i><b>2.10.4</b> Scatter Plot Matrix</a></li>
<li class="chapter" data-level="2.10.5" data-path="data.html"><a href="data.html#data-matrix-visualization"><i class="fa fa-check"></i><b>2.10.5</b> Data Matrix Visualization</a></li>
<li class="chapter" data-level="2.10.6" data-path="data.html"><a href="data.html#correlation-matrix"><i class="fa fa-check"></i><b>2.10.6</b> Correlation Matrix</a></li>
<li class="chapter" data-level="2.10.7" data-path="data.html"><a href="data.html#parallel-coordinates-plot"><i class="fa fa-check"></i><b>2.10.7</b> Parallel Coordinates Plot</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html"><i class="fa fa-check"></i><b>3</b> Classification: Basic Concepts and Techniques</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#the-zoo-dataset"><i class="fa fa-check"></i><b>3.1</b> The Zoo Dataset</a></li>
<li class="chapter" data-level="3.2" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#decision-trees"><i class="fa fa-check"></i><b>3.2</b> Decision Trees</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#create-tree-with-default-settings-uses-pre-pruning"><i class="fa fa-check"></i><b>3.2.1</b> Create Tree With Default Settings (uses pre-pruning)</a></li>
<li class="chapter" data-level="3.2.2" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#create-a-full-tree"><i class="fa fa-check"></i><b>3.2.2</b> Create a Full Tree</a></li>
<li class="chapter" data-level="3.2.3" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#make-predictions-for-new-data"><i class="fa fa-check"></i><b>3.2.3</b> Make Predictions for New Data</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#model-evaluation-with-caret"><i class="fa fa-check"></i><b>3.3</b> Model Evaluation with Caret</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#hold-out-test-data"><i class="fa fa-check"></i><b>3.3.1</b> Hold out Test Data</a></li>
<li class="chapter" data-level="3.3.2" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#learn-a-model-and-tune-hyperparameters-on-the-training-data"><i class="fa fa-check"></i><b>3.3.2</b> Learn a Model and Tune Hyperparameters on the Training Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#testing-confusion-matrix-and-confidence-interval-for-accuracy"><i class="fa fa-check"></i><b>3.4</b> Testing: Confusion Matrix and Confidence Interval for Accuracy</a></li>
<li class="chapter" data-level="3.5" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#model-comparison"><i class="fa fa-check"></i><b>3.5</b> Model Comparison</a></li>
<li class="chapter" data-level="3.6" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#feature-selection-and-feature-preparation"><i class="fa fa-check"></i><b>3.6</b> Feature Selection and Feature Preparation</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#univariate-feature-importance-score"><i class="fa fa-check"></i><b>3.6.1</b> Univariate Feature Importance Score</a></li>
<li class="chapter" data-level="3.6.2" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#feature-subset-selection"><i class="fa fa-check"></i><b>3.6.2</b> Feature Subset Selection</a></li>
<li class="chapter" data-level="3.6.3" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#using-dummy-variables-for-factors"><i class="fa fa-check"></i><b>3.6.3</b> Using Dummy Variables for Factors</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#class-imbalance"><i class="fa fa-check"></i><b>3.7</b> Class Imbalance</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#option-1-use-the-data-as-is-and-hope-for-the-best"><i class="fa fa-check"></i><b>3.7.1</b> Option 1: Use the Data As Is and Hope For The Best</a></li>
<li class="chapter" data-level="3.7.2" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#option-2-balance-data-with-resampling"><i class="fa fa-check"></i><b>3.7.2</b> Option 2: Balance Data With Resampling</a></li>
<li class="chapter" data-level="3.7.3" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#option-3-build-a-larger-tree-and-use-predicted-probabilities"><i class="fa fa-check"></i><b>3.7.3</b> Option 3: Build A Larger Tree and use Predicted Probabilities</a></li>
<li class="chapter" data-level="3.7.4" data-path="classification-basic-concepts-and-techniques.html"><a href="classification-basic-concepts-and-techniques.html#option-4-use-a-cost-sensitive-classifier"><i class="fa fa-check"></i><b>3.7.4</b> Option 4: Use a Cost-Sensitive Classifier</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html"><i class="fa fa-check"></i><b>4</b> Classification: Alternative Techniques</a>
<ul>
<li class="chapter" data-level="4.1" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#training-and-test-data"><i class="fa fa-check"></i><b>4.1</b> Training and Test Data</a></li>
<li class="chapter" data-level="4.2" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#fitting-different-classification-models-to-the-training-data"><i class="fa fa-check"></i><b>4.2</b> Fitting Different Classification Models to the Training Data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#conditional-inference-tree-decision-tree"><i class="fa fa-check"></i><b>4.2.1</b> Conditional Inference Tree (Decision Tree)</a></li>
<li class="chapter" data-level="4.2.2" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#c-4.5-decision-tree"><i class="fa fa-check"></i><b>4.2.2</b> C 4.5 Decision Tree</a></li>
<li class="chapter" data-level="4.2.3" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>4.2.3</b> K-Nearest Neighbors</a></li>
<li class="chapter" data-level="4.2.4" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#part-rule-based-classifier"><i class="fa fa-check"></i><b>4.2.4</b> PART (Rule-based classifier)</a></li>
<li class="chapter" data-level="4.2.5" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#linear-support-vector-machines"><i class="fa fa-check"></i><b>4.2.5</b> Linear Support Vector Machines</a></li>
<li class="chapter" data-level="4.2.6" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#random-forest"><i class="fa fa-check"></i><b>4.2.6</b> Random Forest</a></li>
<li class="chapter" data-level="4.2.7" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#gradient-boosted-decision-trees-xgboost"><i class="fa fa-check"></i><b>4.2.7</b> Gradient Boosted Decision Trees (xgboost)</a></li>
<li class="chapter" data-level="4.2.8" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#artificial-neural-network"><i class="fa fa-check"></i><b>4.2.8</b> Artificial Neural Network</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#comparing-models"><i class="fa fa-check"></i><b>4.3</b> Comparing Models</a></li>
<li class="chapter" data-level="4.4" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#applying-the-chosen-model-to-the-test-data"><i class="fa fa-check"></i><b>4.4</b> Applying the Chosen Model to the Test Data</a></li>
<li class="chapter" data-level="4.5" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#decision-boundaries"><i class="fa fa-check"></i><b>4.5</b> Decision Boundaries</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#iris-dataset"><i class="fa fa-check"></i><b>4.5.1</b> Iris Dataset</a></li>
<li class="chapter" data-level="4.5.2" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#circle-dataset"><i class="fa fa-check"></i><b>4.5.2</b> Circle Dataset</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="classification-alternative-techniques.html"><a href="classification-alternative-techniques.html#more-information"><i class="fa fa-check"></i><b>4.6</b> More Information</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html"><i class="fa fa-check"></i><b>5</b> Association Analysis: Basic Concepts and Algorithms</a>
<ul>
<li class="chapter" data-level="5.1" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#the-arules-package"><i class="fa fa-check"></i><b>5.1</b> The arules Package</a></li>
<li class="chapter" data-level="5.2" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#transactions"><i class="fa fa-check"></i><b>5.2</b> Transactions</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#create-transactions"><i class="fa fa-check"></i><b>5.2.1</b> Create Transactions</a></li>
<li class="chapter" data-level="5.2.2" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#inspect-transactions"><i class="fa fa-check"></i><b>5.2.2</b> Inspect Transactions</a></li>
<li class="chapter" data-level="5.2.3" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#vertical-layout-transaction-id-lists"><i class="fa fa-check"></i><b>5.2.3</b> Vertical Layout (Transaction ID Lists)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#frequent-itemsets"><i class="fa fa-check"></i><b>5.3</b> Frequent Itemsets</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#mine-frequent-itemsets"><i class="fa fa-check"></i><b>5.3.1</b> Mine Frequent Itemsets</a></li>
<li class="chapter" data-level="5.3.2" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#concise-representation-of-itemsets"><i class="fa fa-check"></i><b>5.3.2</b> Concise Representation of Itemsets</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#association-rules"><i class="fa fa-check"></i><b>5.4</b> Association Rules</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#mine-association-rules"><i class="fa fa-check"></i><b>5.4.1</b> Mine Association Rules</a></li>
<li class="chapter" data-level="5.4.2" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#calculate-additional-interest-measures"><i class="fa fa-check"></i><b>5.4.2</b> Calculate Additional Interest Measures</a></li>
<li class="chapter" data-level="5.4.3" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#mine-using-templates"><i class="fa fa-check"></i><b>5.4.3</b> Mine Using Templates</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#association-rule-visualization"><i class="fa fa-check"></i><b>5.5</b> Association Rule Visualization</a></li>
<li class="chapter" data-level="5.6" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#interactive-visualizations"><i class="fa fa-check"></i><b>5.6</b> Interactive Visualizations</a>
<ul>
<li class="chapter" data-level="5.6.1" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#interactive-inspect-with-sorting-filtering-and-paging"><i class="fa fa-check"></i><b>5.6.1</b> Interactive Inspect With Sorting, Filtering and Paging</a></li>
<li class="chapter" data-level="5.6.2" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#scatter-plot-1"><i class="fa fa-check"></i><b>5.6.2</b> Scatter Plot</a></li>
<li class="chapter" data-level="5.6.3" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#matrix-visualization"><i class="fa fa-check"></i><b>5.6.3</b> Matrix Visualization</a></li>
<li class="chapter" data-level="5.6.4" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#visualization-as-graph"><i class="fa fa-check"></i><b>5.6.4</b> Visualization as Graph</a></li>
<li class="chapter" data-level="5.6.5" data-path="association-analysis-basic-concepts-and-algorithms.html"><a href="association-analysis-basic-concepts-and-algorithms.html#interactive-rule-explorer"><i class="fa fa-check"></i><b>5.6.5</b> Interactive Rule Explorer</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="association-analysis-advanced-concepts.html"><a href="association-analysis-advanced-concepts.html"><i class="fa fa-check"></i><b>6</b> Association Analysis: Advanced Concepts</a></li>
<li class="chapter" data-level="7" data-path="clustering-analysis.html"><a href="clustering-analysis.html"><i class="fa fa-check"></i><b>7</b> Clustering Analysis</a>
<ul>
<li class="chapter" data-level="7.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#data-preparation"><i class="fa fa-check"></i><b>7.1</b> Data Preparation</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#data-cleaning"><i class="fa fa-check"></i><b>7.1.1</b> Data cleaning</a></li>
<li class="chapter" data-level="7.1.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#scale-data"><i class="fa fa-check"></i><b>7.1.2</b> Scale data</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#clustering-methods"><i class="fa fa-check"></i><b>7.2</b> Clustering methods</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#k-means-clustering"><i class="fa fa-check"></i><b>7.2.1</b> k-means Clustering</a></li>
<li class="chapter" data-level="7.2.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#hierarchical-clustering"><i class="fa fa-check"></i><b>7.2.2</b> Hierarchical Clustering</a></li>
<li class="chapter" data-level="7.2.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#density-based-clustering-with-dbscan"><i class="fa fa-check"></i><b>7.2.3</b> Density-based clustering with DBSCAN</a></li>
<li class="chapter" data-level="7.2.4" data-path="clustering-analysis.html"><a href="clustering-analysis.html#partitioning-around-medoids-pam"><i class="fa fa-check"></i><b>7.2.4</b> Partitioning Around Medoids (PAM)</a></li>
<li class="chapter" data-level="7.2.5" data-path="clustering-analysis.html"><a href="clustering-analysis.html#gaussian-mixture-models"><i class="fa fa-check"></i><b>7.2.5</b> Gaussian Mixture Models</a></li>
<li class="chapter" data-level="7.2.6" data-path="clustering-analysis.html"><a href="clustering-analysis.html#spectral-clustering"><i class="fa fa-check"></i><b>7.2.6</b> Spectral clustering</a></li>
<li class="chapter" data-level="7.2.7" data-path="clustering-analysis.html"><a href="clustering-analysis.html#fuzzy-c-means-clustering"><i class="fa fa-check"></i><b>7.2.7</b> Fuzzy C-Means Clustering</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#internal-cluster-validation"><i class="fa fa-check"></i><b>7.3</b> Internal Cluster Validation</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#compare-the-clustering-quality"><i class="fa fa-check"></i><b>7.3.1</b> Compare the Clustering Quality</a></li>
<li class="chapter" data-level="7.3.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#silhouette-plot"><i class="fa fa-check"></i><b>7.3.2</b> Silhouette plot</a></li>
<li class="chapter" data-level="7.3.3" data-path="clustering-analysis.html"><a href="clustering-analysis.html#find-optimal-number-of-clusters-for-k-means"><i class="fa fa-check"></i><b>7.3.3</b> Find Optimal Number of Clusters for k-means</a></li>
<li class="chapter" data-level="7.3.4" data-path="clustering-analysis.html"><a href="clustering-analysis.html#visualizing-the-distance-matrix"><i class="fa fa-check"></i><b>7.3.4</b> Visualizing the Distance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="clustering-analysis.html"><a href="clustering-analysis.html#external-cluster-validation"><i class="fa fa-check"></i><b>7.4</b> External Cluster Validation</a></li>
<li class="chapter" data-level="7.5" data-path="clustering-analysis.html"><a href="clustering-analysis.html#advanced-data-preparation-for-clustering"><i class="fa fa-check"></i><b>7.5</b> Advanced Data Preparation for Clustering</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="clustering-analysis.html"><a href="clustering-analysis.html#outlier-removal"><i class="fa fa-check"></i><b>7.5.1</b> Outlier Removal</a></li>
<li class="chapter" data-level="7.5.2" data-path="clustering-analysis.html"><a href="clustering-analysis.html#clustering-tendency"><i class="fa fa-check"></i><b>7.5.2</b> Clustering Tendency</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R Companion for the Textbook Introduction to Data Mining</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clustering-analysis" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> Clustering Analysis</h1>
<p><strong>Packages used for this chapter:</strong> <em>cluster</em> <span class="citation">(<a href="#ref-R-cluster" role="doc-biblioref">Maechler et al., 2021</a>)</span>, <em>dbscan</em> <span class="citation">(<a href="#ref-R-dbscan" role="doc-biblioref">Hahsler &amp; Piekenbrock, 2021</a>)</span>, <em>e1071</em> <span class="citation">(<a href="#ref-R-e1071" role="doc-biblioref">Meyer et al., 2021</a>)</span>, <em>factoextra</em> <span class="citation">(<a href="#ref-R-factoextra" role="doc-biblioref">Kassambara &amp; Mundt, 2020</a>)</span>, <em>fpc</em> <span class="citation">(<a href="#ref-R-fpc" role="doc-biblioref">Hennig, 2020</a>)</span>, <em>GGally</em> <span class="citation">(<a href="#ref-R-GGally" role="doc-biblioref">Schloerke et al., 2021</a>)</span>, <em>kernlab</em> <span class="citation">(<a href="#ref-R-kernlab" role="doc-biblioref">Karatzoglou et al., 2019</a>)</span>, <em>mclust</em> <span class="citation">(<a href="#ref-R-mclust" role="doc-biblioref">Fraley et al., 2020</a>)</span>, <em>mlbench</em> <span class="citation">(<a href="#ref-R-mlbench" role="doc-biblioref">Leisch &amp; Dimitriadou., 2021</a>)</span>, <em>scatterpie</em> <span class="citation">(<a href="#ref-R-scatterpie" role="doc-biblioref">Yu, 2021</a>)</span>, <em>seriation</em> <span class="citation">(<a href="#ref-R-seriation" role="doc-biblioref">Hahsler, Buchta, &amp; Hornik, 2021</a>)</span>, <em>tidyverse</em> <span class="citation">(<a href="#ref-R-tidyverse" role="doc-biblioref">Wickham, 2021c</a>)</span></p>
<p>You can read the free sample chapter from the textbook <span class="citation">(<a href="#ref-Tan2005" role="doc-biblioref">Tan et al., 2005</a>)</span>:
<a href="https://www-users.cs.umn.edu/~kumar001/dmbook/ch7_clustering.pdf">Chapter 7. Cluster Analysis: Basic Concepts and Algorithms</a></p>
<div id="data-preparation" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Data Preparation</h2>
<div class="sourceCode" id="cb650"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb650-1"><a href="clustering-analysis.html#cb650-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<p>We will use here a small and very clean dataset called Ruspini which is included in the R package <strong>cluster</strong>.</p>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb651-1"><a href="clustering-analysis.html#cb651-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(ruspini, <span class="at">package =</span> <span class="st">&quot;cluster&quot;</span>)</span></code></pre></div>
<p>The Ruspini data set, consisting of 75 points in four groups that is popular for illustrating clustering techniques. It is a very simple data set with well separated clusters.
The original dataset has the points ordered by group. We can shuffle the data (rows) using <code>sample_frac</code> which samples by default 100%.</p>
<div class="sourceCode" id="cb652"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb652-1"><a href="clustering-analysis.html#cb652-1" aria-hidden="true" tabindex="-1"></a>ruspini <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(ruspini) <span class="sc">%&gt;%</span> <span class="fu">sample_frac</span>()</span>
<span id="cb652-2"><a href="clustering-analysis.html#cb652-2" aria-hidden="true" tabindex="-1"></a>ruspini</span></code></pre></div>
<pre><code>## # A tibble: 75 x 2
##        x     y
##    &lt;int&gt; &lt;int&gt;
##  1    38   143
##  2    30    52
##  3    22    74
##  4    70     4
##  5    77    12
##  6    18    61
##  7    85   115
##  8    34   141
##  9    53   144
## 10    35   153
## # … with 65 more rows</code></pre>
<div id="data-cleaning" class="section level3" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Data cleaning</h3>
<div class="sourceCode" id="cb654"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb654-1"><a href="clustering-analysis.html#cb654-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-235-1.png" width="672" /></p>
<div class="sourceCode" id="cb655"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb655-1"><a href="clustering-analysis.html#cb655-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ruspini)</span></code></pre></div>
<pre><code>##        x               y        
##  Min.   :  4.0   Min.   :  4.0  
##  1st Qu.: 31.5   1st Qu.: 56.5  
##  Median : 52.0   Median : 96.0  
##  Mean   : 54.9   Mean   : 92.0  
##  3rd Qu.: 76.5   3rd Qu.:141.5  
##  Max.   :117.0   Max.   :156.0</code></pre>
<p>For most clustering algorithms it is necessary to handle missing values and outliers (e.g., remove the observations). For details see Section “Outlier removal” below.
This data set has not missing values or strong outlier and looks like it has some very clear groups.</p>
</div>
<div id="scale-data" class="section level3" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Scale data</h3>
<p>Clustering algorithms use distances and the variables with the largest number range will dominate distance calculation. The summary above shows that this is not an issue for the Ruspini dataset with
both, x and y, being roughly between 0 and 150. Most data analysts will still
scale each column in the data to zero mean and unit standard deviation (z-scores).
<em>Note:</em> The standard <code>scale()</code> function scales a whole data matrix so we implement a function for a single vector and apply it to all numeric columns.</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb657-1"><a href="clustering-analysis.html#cb657-1" aria-hidden="true" tabindex="-1"></a><span class="do">## I use this till tidyverse implements a scale function</span></span>
<span id="cb657-2"><a href="clustering-analysis.html#cb657-2" aria-hidden="true" tabindex="-1"></a>scale_numeric <span class="ot">&lt;-</span> <span class="cf">function</span>(x) x <span class="sc">%&gt;%</span> <span class="fu">mutate_if</span>(is.numeric, <span class="cf">function</span>(y) <span class="fu">as.vector</span>(<span class="fu">scale</span>(y)))</span>
<span id="cb657-3"><a href="clustering-analysis.html#cb657-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb657-4"><a href="clustering-analysis.html#cb657-4" aria-hidden="true" tabindex="-1"></a>ruspini_scaled <span class="ot">&lt;-</span> ruspini <span class="sc">%&gt;%</span> <span class="fu">scale_numeric</span>()</span>
<span id="cb657-5"><a href="clustering-analysis.html#cb657-5" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ruspini_scaled)</span></code></pre></div>
<pre><code>##        x                y         
##  Min.   :-1.668   Min.   :-1.807  
##  1st Qu.:-0.766   1st Qu.:-0.729  
##  Median :-0.094   Median : 0.082  
##  Mean   : 0.000   Mean   : 0.000  
##  3rd Qu.: 0.709   3rd Qu.: 1.016  
##  Max.   : 2.037   Max.   : 1.314</code></pre>
<p>After scaling, most z-scores will fall in the range <span class="math inline">\([-3,3]\)</span> (z-scores are measured in standard deviations from the mean), where
<span class="math inline">\(0\)</span> means average.</p>
</div>
</div>
<div id="clustering-methods" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Clustering methods</h2>
<div id="k-means-clustering" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> k-means Clustering</h3>
<p><a href="https://en.wikipedia.org/wiki/K-means_clustering">k-means</a> implicitly assumes Euclidean distances. We use <span class="math inline">\(k = 4\)</span> clusters and run the algorithm 10 times with random initialized centroids. The best result is returned.</p>
<div class="sourceCode" id="cb659"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb659-1"><a href="clustering-analysis.html#cb659-1" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(ruspini_scaled, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb659-2"><a href="clustering-analysis.html#cb659-2" aria-hidden="true" tabindex="-1"></a>km</span></code></pre></div>
<pre><code>## K-means clustering with 4 clusters of sizes 17, 23, 15, 20
## 
## Cluster means:
##        x      y
## 1  1.419  0.469
## 2 -0.360  1.109
## 3  0.461 -1.491
## 4 -1.139 -0.556
## 
## Clustering vector:
##  [1] 2 4 4 3 3 4 1 2 2 2 3 2 3 4 1 3 4 4 1 4 2 3 1 2 1
## [26] 2 3 1 3 2 1 4 3 4 4 1 1 2 1 4 2 1 2 2 4 3 3 2 2 4
## [51] 1 2 4 2 3 4 2 3 2 4 1 4 4 1 1 2 4 3 1 2 2 3 4 1 2
## 
## Within cluster sum of squares by cluster:
## [1] 3.64 2.66 1.08 2.71
##  (between_SS / total_SS =  93.2 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;       
## [4] &quot;withinss&quot;     &quot;tot.withinss&quot; &quot;betweenss&quot;   
## [7] &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<p><code>km</code> is an R object implemented as a list. The clustering vector contains the
cluster assignment for each data row and can be accessed using <code>km$cluster</code>. I add the
cluster assignment as a column to the scaled dataset (I make it a factor since it represents a
nominal label).</p>
<div class="sourceCode" id="cb661"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb661-1"><a href="clustering-analysis.html#cb661-1" aria-hidden="true" tabindex="-1"></a>ruspini_clustered <span class="ot">&lt;-</span> ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster))</span>
<span id="cb661-2"><a href="clustering-analysis.html#cb661-2" aria-hidden="true" tabindex="-1"></a>ruspini_clustered</span></code></pre></div>
<pre><code>## # A tibble: 75 x 3
##          x      y cluster
##      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  
##  1 -0.553   1.05  2      
##  2 -0.816  -0.822 4      
##  3 -1.08   -0.370 4      
##  4  0.496  -1.81  3      
##  5  0.725  -1.64  3      
##  6 -1.21   -0.637 4      
##  7  0.987   0.472 1      
##  8 -0.685   1.01  2      
##  9 -0.0616  1.07  2      
## 10 -0.652   1.25  2      
## # … with 65 more rows</code></pre>
<div class="sourceCode" id="cb663"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb663-1"><a href="clustering-analysis.html#cb663-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_clustered, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-238-1.png" width="672" /></p>
<p>Add the centroids to the plot.</p>
<div class="sourceCode" id="cb664"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb664-1"><a href="clustering-analysis.html#cb664-1" aria-hidden="true" tabindex="-1"></a>centroids <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(km<span class="sc">$</span>centers, <span class="at">rownames =</span> <span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb664-2"><a href="clustering-analysis.html#cb664-2" aria-hidden="true" tabindex="-1"></a>centroids</span></code></pre></div>
<pre><code>## # A tibble: 4 x 3
##   cluster      x      y
##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 1        1.42   0.469
## 2 2       -0.360  1.11 
## 3 3        0.461 -1.49 
## 4 4       -1.14  -0.556</code></pre>
<div class="sourceCode" id="cb666"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb666-1"><a href="clustering-analysis.html#cb666-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_clustered, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb666-2"><a href="clustering-analysis.html#cb666-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> centroids, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster), <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">size =</span> <span class="dv">10</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-239-1.png" width="672" /></p>
<p>Use the <code>factoextra</code> package for visualization</p>
<div class="sourceCode" id="cb667"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb667-1"><a href="clustering-analysis.html#cb667-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span>
<span id="cb667-2"><a href="clustering-analysis.html#cb667-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(km, <span class="at">data =</span> ruspini_scaled, <span class="at">centroids =</span> <span class="cn">TRUE</span>, <span class="at">repel =</span> <span class="cn">TRUE</span>, <span class="at">ellipse.type =</span> <span class="st">&quot;norm&quot;</span>)</span></code></pre></div>
<pre><code>## Warning: ggrepel: 10 unlabeled data points (too many
## overlaps). Consider increasing max.overlaps</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-240-1.png" width="672" /></p>
<div id="inspect-clusters" class="section level4" number="7.2.1.1">
<h4><span class="header-section-number">7.2.1.1</span> Inspect clusters</h4>
<p>We inspect the clusters created by the 4-cluster k-means solution. The following code can be adapted to be used for other clustering methods.</p>
<div id="cluster-profiles" class="section level5" number="7.2.1.1.1">
<h5><span class="header-section-number">7.2.1.1.1</span> Cluster Profiles</h5>
<p>Inspect the centroids with horizontal bar charts organized by cluster. To group the plots by cluster, we have to change the data format to the “long”-format using a pivot operation. I use colors to match the clusters in the scatter plots.</p>
<div class="sourceCode" id="cb669"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb669-1"><a href="clustering-analysis.html#cb669-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">pivot_longer</span>(centroids, <span class="at">cols =</span> <span class="fu">c</span>(x, y), <span class="at">names_to =</span> <span class="st">&quot;feature&quot;</span>),</span>
<span id="cb669-2"><a href="clustering-analysis.html#cb669-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(<span class="at">x =</span> value, <span class="at">y =</span> feature, <span class="at">fill =</span> cluster)) <span class="sc">+</span></span>
<span id="cb669-3"><a href="clustering-analysis.html#cb669-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">&quot;identity&quot;</span>) <span class="sc">+</span></span>
<span id="cb669-4"><a href="clustering-analysis.html#cb669-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(<span class="at">rows =</span> <span class="fu">vars</span>(cluster))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-241-1.png" width="672" /></p>
</div>
<div id="extract-a-single-cluster" class="section level5" number="7.2.1.1.2">
<h5><span class="header-section-number">7.2.1.1.2</span> Extract a single cluster</h5>
<p>You need is to filter the rows corresponding to the cluster index. The next
example calculates summary statistics and then plots all data points of cluster 1.</p>
<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb670-1"><a href="clustering-analysis.html#cb670-1" aria-hidden="true" tabindex="-1"></a>cluster1 <span class="ot">&lt;-</span> ruspini_clustered <span class="sc">%&gt;%</span> <span class="fu">filter</span>(cluster <span class="sc">==</span> <span class="dv">1</span>)</span>
<span id="cb670-2"><a href="clustering-analysis.html#cb670-2" aria-hidden="true" tabindex="-1"></a>cluster1</span></code></pre></div>
<pre><code>## # A tibble: 17 x 3
##        x      y cluster
##    &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  
##  1 0.987 0.472  1      
##  2 1.74  0.492  1      
##  3 1.38  0.615  1      
##  4 0.758 0.0405 1      
##  5 1.97  0.513  1      
##  6 1.45  0.554  1      
##  7 1.51  0.472  1      
##  8 0.987 0.0816 1      
##  9 0.627 0.0816 1      
## 10 1.74  0.390  1      
## 11 2.04  0.472  1      
## 12 1.45  0.739  1      
## 13 1.84  0.698  1      
## 14 1.81  0.390  1      
## 15 1.02  0.821  1      
## 16 1.41  0.657  1      
## 17 1.41  0.492  1</code></pre>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb672-1"><a href="clustering-analysis.html#cb672-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(cluster1)</span></code></pre></div>
<pre><code>##        x               y         cluster
##  Min.   :0.627   Min.   :0.041   1:17   
##  1st Qu.:1.020   1st Qu.:0.390   2: 0   
##  Median :1.446   Median :0.492   3: 0   
##  Mean   :1.419   Mean   :0.469   4: 0   
##  3rd Qu.:1.741   3rd Qu.:0.615          
##  Max.   :2.037   Max.   :0.821</code></pre>
<div class="sourceCode" id="cb674"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb674-1"><a href="clustering-analysis.html#cb674-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(cluster1, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb674-2"><a href="clustering-analysis.html#cb674-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">coord_cartesian</span>(<span class="at">xlim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>), <span class="at">ylim =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">2</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-242-1.png" width="672" /></p>
<p>What happens if we try to cluster with 8 centers?</p>
<div class="sourceCode" id="cb675"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb675-1"><a href="clustering-analysis.html#cb675-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">kmeans</span>(ruspini_scaled, <span class="at">centers =</span> <span class="dv">8</span>), <span class="at">data =</span> ruspini_scaled,</span>
<span id="cb675-2"><a href="clustering-analysis.html#cb675-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">centroids =</span> <span class="cn">TRUE</span>,  <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="at">ellipse.type =</span> <span class="st">&quot;norm&quot;</span>)</span></code></pre></div>
<pre><code>## Too few points to calculate an ellipse</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-243-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="hierarchical-clustering" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Hierarchical Clustering</h3>
<p>Hierarchical clustering starts with a distance matrix. <code>dist()</code> defaults to method=“Euclidean.” <strong>Note:</strong> Distance matrices become very large quickly (size and time complexity is <span class="math inline">\(O(n^2)\)</span> where <span class="math inline">\(n\)</span> is the number if data points). It is only possible to calculate and store the matrix for small data sets (maybe a few hundred thousand data points) in main memory. If your data is too large then you can use sampling.</p>
<div class="sourceCode" id="cb677"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb677-1"><a href="clustering-analysis.html#cb677-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">dist</span>(ruspini_scaled)</span></code></pre></div>
<p><code>hclust()</code> implements <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering">agglomerative hierarchical clustering</a>. We cluster using complete link.</p>
<div class="sourceCode" id="cb678"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb678-1"><a href="clustering-analysis.html#cb678-1" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> <span class="st">&quot;complete&quot;</span>)</span></code></pre></div>
<p>Hierarchical clustering does not return cluster assignments but a dendrogram. The standard plot
function plots the dendrogram.</p>
<div class="sourceCode" id="cb679"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb679-1"><a href="clustering-analysis.html#cb679-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(hc)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-246-1.png" width="672" /></p>
<p>Use <code>factoextra</code> (ggplot version). We can specify the number of clusters to visualize how the dendrogram will be cut into clusters.</p>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb680-1"><a href="clustering-analysis.html#cb680-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_dend</span>(hc, <span class="at">k =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated.
## Please use `guides(&lt;scale&gt; = &quot;none&quot;)` instead.</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-247-1.png" width="672" /></p>
<p>More plotting options for dendrograms, including plotting
parts of large dendrograms can be found <a href="https://rpubs.com/gaston/dendrograms">here.</a></p>
<p>Extract cluster assignments by cutting the dendrogram into four parts and add the cluster id to the data.</p>
<div class="sourceCode" id="cb682"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb682-1"><a href="clustering-analysis.html#cb682-1" aria-hidden="true" tabindex="-1"></a>clusters <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc, <span class="at">k =</span> <span class="dv">4</span>)</span>
<span id="cb682-2"><a href="clustering-analysis.html#cb682-2" aria-hidden="true" tabindex="-1"></a>cluster_complete <span class="ot">&lt;-</span> ruspini_scaled <span class="sc">%&gt;%</span></span>
<span id="cb682-3"><a href="clustering-analysis.html#cb682-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(clusters))</span>
<span id="cb682-4"><a href="clustering-analysis.html#cb682-4" aria-hidden="true" tabindex="-1"></a>cluster_complete</span></code></pre></div>
<pre><code>## # A tibble: 75 x 3
##          x      y cluster
##      &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;  
##  1 -0.553   1.05  1      
##  2 -0.816  -0.822 2      
##  3 -1.08   -0.370 2      
##  4  0.496  -1.81  3      
##  5  0.725  -1.64  3      
##  6 -1.21   -0.637 2      
##  7  0.987   0.472 4      
##  8 -0.685   1.01  1      
##  9 -0.0616  1.07  1      
## 10 -0.652   1.25  1      
## # … with 65 more rows</code></pre>
<div class="sourceCode" id="cb684"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb684-1"><a href="clustering-analysis.html#cb684-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(cluster_complete, <span class="fu">aes</span>(x, y, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb684-2"><a href="clustering-analysis.html#cb684-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-248-1.png" width="672" /></p>
<p>Try 8 clusters (Note: <code>fviz_cluster</code> needs a list with data and the cluster labels for hclust)</p>
<div class="sourceCode" id="cb685"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb685-1"><a href="clustering-analysis.html#cb685-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">list</span>(<span class="at">data =</span> ruspini_scaled, <span class="at">cluster =</span> <span class="fu">cutree</span>(hc, <span class="at">k =</span> <span class="dv">8</span>)), <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-249-1.png" width="672" /></p>
<p>Clustering with single link</p>
<div class="sourceCode" id="cb686"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb686-1"><a href="clustering-analysis.html#cb686-1" aria-hidden="true" tabindex="-1"></a>hc_single <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> <span class="st">&quot;single&quot;</span>)</span>
<span id="cb686-2"><a href="clustering-analysis.html#cb686-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_dend</span>(hc_single, <span class="at">k =</span> <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## Warning: `guides(&lt;scale&gt; = FALSE)` is deprecated.
## Please use `guides(&lt;scale&gt; = &quot;none&quot;)` instead.</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-250-1.png" width="672" /></p>
<div class="sourceCode" id="cb688"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb688-1"><a href="clustering-analysis.html#cb688-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">list</span>(<span class="at">data =</span> ruspini_scaled, <span class="at">cluster =</span> <span class="fu">cutree</span>(hc_single, <span class="at">k =</span> <span class="dv">4</span>)), <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-250-2.png" width="672" /></p>
</div>
<div id="density-based-clustering-with-dbscan" class="section level3" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Density-based clustering with DBSCAN</h3>
<div class="sourceCode" id="cb689"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb689-1"><a href="clustering-analysis.html#cb689-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dbscan)</span></code></pre></div>
<p><a href="https://en.wikipedia.org/wiki/DBSCAN">DBSCAN</a> stands for “Density-Based Spatial Clustering of Applications with Noise.” It groups together points that are closely packed together and treats points in low-density regions as outliers.</p>
<p><strong>Parameters:</strong> minPts defines how many points in the epsilon neighborhood are needed to make a point
a core point. It is often chosen as a smoothing parameter. I use here minPts = 4.</p>
<p>To decide on epsilon, the knee in the kNN distance plot is often used. Note that minPts contains the point itself, while the k-nearest neighbor does not. We therefore have to use k = minPts - 1!
The knee is around eps = .32.</p>
<div class="sourceCode" id="cb690"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb690-1"><a href="clustering-analysis.html#cb690-1" aria-hidden="true" tabindex="-1"></a><span class="fu">kNNdistplot</span>(ruspini_scaled, <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb690-2"><a href="clustering-analysis.html#cb690-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h =</span> .<span class="dv">32</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-252-1.png" width="672" /></p>
<p>run dbscan</p>
<div class="sourceCode" id="cb691"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb691-1"><a href="clustering-analysis.html#cb691-1" aria-hidden="true" tabindex="-1"></a>db <span class="ot">&lt;-</span> <span class="fu">dbscan</span>(ruspini_scaled, <span class="at">eps =</span> .<span class="dv">32</span>, <span class="at">minPts =</span> <span class="dv">4</span>)</span>
<span id="cb691-2"><a href="clustering-analysis.html#cb691-2" aria-hidden="true" tabindex="-1"></a>db</span></code></pre></div>
<pre><code>## DBSCAN clustering for 75 objects.
## Parameters: eps = 0.32, minPts = 4
## The clustering contains 4 cluster(s) and 5 noise points.
## 
##  0  1  2  3  4 
##  5 23 20 15 12 
## 
## Available fields: cluster, eps, minPts</code></pre>
<div class="sourceCode" id="cb693"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb693-1"><a href="clustering-analysis.html#cb693-1" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(db)</span></code></pre></div>
<pre><code>## List of 3
##  $ cluster: int [1:75] 1 2 2 3 3 2 0 1 1 1 ...
##  $ eps    : num 0.32
##  $ minPts : num 4
##  - attr(*, &quot;class&quot;)= chr [1:2] &quot;dbscan_fast&quot; &quot;dbscan&quot;</code></pre>
<div class="sourceCode" id="cb695"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb695-1"><a href="clustering-analysis.html#cb695-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(db<span class="sc">$</span>cluster)),</span>
<span id="cb695-2"><a href="clustering-analysis.html#cb695-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(x, y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-253-1.png" width="672" /></p>
<p><strong>Note:</strong> Cluster 0 represents outliers).</p>
<div class="sourceCode" id="cb696"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb696-1"><a href="clustering-analysis.html#cb696-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(db, ruspini_scaled, <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-254-1.png" width="672" /></p>
<p>Play with eps (neighborhood size) and MinPts (minimum of points needed for core cluster)</p>
</div>
<div id="partitioning-around-medoids-pam" class="section level3" number="7.2.4">
<h3><span class="header-section-number">7.2.4</span> Partitioning Around Medoids (PAM)</h3>
<p><a href="https://en.wikipedia.org/wiki/K-medoids">PAM</a> tries to solve the
<span class="math inline">\(k\)</span>-medoids problem.
The problem is similar to <span class="math inline">\(k\)</span>-means, but uses medoids instead of centroids to represent clusters. Like hierarchical clustering, it typically works with precomputed distance matrix.
An advantage is that you can use any distance metric not just Euclidean distances.
<strong>Note:</strong> The medoid is the most central data point in the middle of the cluster.</p>
<div class="sourceCode" id="cb697"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb697-1"><a href="clustering-analysis.html#cb697-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;cluster&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     ruspini</code></pre>
<div class="sourceCode" id="cb700"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb700-1"><a href="clustering-analysis.html#cb700-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">dist</span>(ruspini_scaled)</span>
<span id="cb700-2"><a href="clustering-analysis.html#cb700-2" aria-hidden="true" tabindex="-1"></a><span class="fu">str</span>(d)</span></code></pre></div>
<pre><code>##  &#39;dist&#39; num [1:2775] 1.89 1.51 3.04 2.98 1.81 ...
##  - attr(*, &quot;Size&quot;)= int 75
##  - attr(*, &quot;Diag&quot;)= logi FALSE
##  - attr(*, &quot;Upper&quot;)= logi FALSE
##  - attr(*, &quot;method&quot;)= chr &quot;Euclidean&quot;
##  - attr(*, &quot;call&quot;)= language dist(x = ruspini_scaled)</code></pre>
<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb702-1"><a href="clustering-analysis.html#cb702-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">pam</span>(d, <span class="at">k =</span> <span class="dv">4</span>)</span>
<span id="cb702-2"><a href="clustering-analysis.html#cb702-2" aria-hidden="true" tabindex="-1"></a>p</span></code></pre></div>
<pre><code>## Medoids:
##      ID   
## [1,] 66 66
## [2,] 56 56
## [3,] 33 33
## [4,] 28 28
## Clustering vector:
##  [1] 1 2 2 3 3 2 4 1 1 1 3 1 3 2 4 3 2 2 4 2 1 3 4 1 4
## [26] 1 3 4 3 1 4 2 3 2 2 4 4 1 4 2 1 4 1 1 2 3 3 1 1 2
## [51] 4 1 2 1 3 2 1 3 1 2 4 2 2 4 4 1 2 3 4 1 1 3 2 4 1
## Objective function:
## build  swap 
## 0.442 0.319 
## 
## Available components:
## [1] &quot;medoids&quot;    &quot;id.med&quot;     &quot;clustering&quot; &quot;objective&quot; 
## [5] &quot;isolation&quot;  &quot;clusinfo&quot;   &quot;silinfo&quot;    &quot;diss&quot;      
## [9] &quot;call&quot;</code></pre>
<div class="sourceCode" id="cb704"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb704-1"><a href="clustering-analysis.html#cb704-1" aria-hidden="true" tabindex="-1"></a>ruspini_clustered <span class="ot">&lt;-</span> ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(p<span class="sc">$</span>cluster))</span>
<span id="cb704-2"><a href="clustering-analysis.html#cb704-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb704-3"><a href="clustering-analysis.html#cb704-3" aria-hidden="true" tabindex="-1"></a>medoids <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(ruspini_scaled[p<span class="sc">$</span>medoids, ], <span class="at">rownames =</span> <span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb704-4"><a href="clustering-analysis.html#cb704-4" aria-hidden="true" tabindex="-1"></a>medoids</span></code></pre></div>
<pre><code>## # A tibble: 4 x 3
##   cluster      x      y
##   &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 1       -0.357  1.17 
## 2 2       -1.18  -0.555
## 3 3        0.463 -1.46 
## 4 4        1.45   0.554</code></pre>
<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb706-1"><a href="clustering-analysis.html#cb706-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_clustered, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb706-2"><a href="clustering-analysis.html#cb706-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> medoids, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster), <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">size =</span> <span class="dv">10</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-255-1.png" width="672" /></p>
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="clustering-analysis.html#cb707-1" aria-hidden="true" tabindex="-1"></a><span class="do">## __Note:__ `fviz_cluster` needs the original data.</span></span>
<span id="cb707-2"><a href="clustering-analysis.html#cb707-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_cluster</span>(<span class="fu">c</span>(p, <span class="fu">list</span>(<span class="at">data =</span> ruspini_scaled)), <span class="at">geom =</span> <span class="st">&quot;point&quot;</span>, <span class="at">ellipse.type =</span> <span class="st">&quot;norm&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-255-2.png" width="672" /></p>
</div>
<div id="gaussian-mixture-models" class="section level3" number="7.2.5">
<h3><span class="header-section-number">7.2.5</span> Gaussian Mixture Models</h3>
<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb708-1"><a href="clustering-analysis.html#cb708-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mclust)</span></code></pre></div>
<pre><code>## Package &#39;mclust&#39; version 5.4.7
## Type &#39;citation(&quot;mclust&quot;)&#39; for citing this R package in publications.</code></pre>
<pre><code>## 
## Attaching package: &#39;mclust&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     map</code></pre>
<p><a href="https://en.wikipedia.org/wiki/Mixture_model#Multivariate_Gaussian_mixture_model">Gaussian mixture models</a> assume that the data set is the
result of drawing data from a set of
Gaussian distributions where each distribution represents a cluster. Estimation algorithms try to identify the location parameters of the distributions and thus can be used to find clusters.
<code>Mclust()</code> uses Bayesian Information Criterion (BIC) to find the
number of clusters (model selection). BIC uses the likelihood and a
penalty term to guard against overfitting.</p>
<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb712-1"><a href="clustering-analysis.html#cb712-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(ruspini_scaled)</span>
<span id="cb712-2"><a href="clustering-analysis.html#cb712-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust EEI (diagonal, equal volume and shape) model
## with 5 components: 
## 
##  log-likelihood  n df  BIC  ICL
##           -91.3 75 16 -252 -252
## 
## Clustering table:
##  1  2  3  4  5 
## 23 20 15  3 14</code></pre>
<div class="sourceCode" id="cb714"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb714-1"><a href="clustering-analysis.html#cb714-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m, <span class="at">what =</span> <span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-257-1.png" width="672" /></p>
<p>Rerun with a fixed number of 4 clusters</p>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb715-1"><a href="clustering-analysis.html#cb715-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">Mclust</span>(ruspini_scaled, <span class="at">G=</span><span class="dv">4</span>)</span>
<span id="cb715-2"><a href="clustering-analysis.html#cb715-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m)</span></code></pre></div>
<pre><code>## ---------------------------------------------------- 
## Gaussian finite mixture model fitted by EM algorithm 
## ---------------------------------------------------- 
## 
## Mclust EEI (diagonal, equal volume and shape) model
## with 4 components: 
## 
##  log-likelihood  n df  BIC  ICL
##            -102 75 13 -259 -259
## 
## Clustering table:
##  1  2  3  4 
## 23 20 15 17</code></pre>
<div class="sourceCode" id="cb717"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb717-1"><a href="clustering-analysis.html#cb717-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(m, <span class="at">what =</span> <span class="st">&quot;classification&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-258-1.png" width="672" /></p>
</div>
<div id="spectral-clustering" class="section level3" number="7.2.6">
<h3><span class="header-section-number">7.2.6</span> Spectral clustering</h3>
<p><a href="https://en.wikipedia.org/wiki/Spectral_clustering">Spectral clustering</a> works by embedding the data points of the partitioning problem into the subspace of the k largest eigenvectors of a normalized affinity/kernel matrix. Then uses a simple clustering method like k-means.</p>
<div class="sourceCode" id="cb718"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb718-1"><a href="clustering-analysis.html#cb718-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;kernlab&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;kernlab&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:scales&#39;:
## 
##     alpha</code></pre>
<pre><code>## The following object is masked from &#39;package:arules&#39;:
## 
##     size</code></pre>
<pre><code>## The following object is masked from &#39;package:purrr&#39;:
## 
##     cross</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     alpha</code></pre>
<div class="sourceCode" id="cb724"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb724-1"><a href="clustering-analysis.html#cb724-1" aria-hidden="true" tabindex="-1"></a>cluster_spec <span class="ot">&lt;-</span> <span class="fu">specc</span>(<span class="fu">as.matrix</span>(ruspini_scaled), <span class="at">centers =</span> <span class="dv">4</span>)</span>
<span id="cb724-2"><a href="clustering-analysis.html#cb724-2" aria-hidden="true" tabindex="-1"></a>cluster_spec</span></code></pre></div>
<pre><code>## Spectral Clustering object of class &quot;specc&quot; 
## 
##  Cluster memberships: 
##  
## 1 3 3 2 2 3 4 1 1 1 2 1 2 3 4 2 3 3 4 3 1 2 4 1 4 1 2 4 2 1 4 3 2 3 3 4 4 1 4 3 1 4 1 1 3 2 2 1 1 3 4 1 3 1 2 3 1 2 1 3 4 3 3 4 4 1 3 2 4 1 1 2 3 4 1 
##  
## Gaussian Radial Basis kernel function. 
##  Hyperparameter : sigma =  41.7670067458421 
## 
## Centers:  
##        [,1]   [,2]
## [1,] -0.360  1.109
## [2,]  0.461 -1.491
## [3,] -1.139 -0.556
## [4,]  1.419  0.469
## 
## Cluster size:  
## [1] 23 15 20 17
## 
## Within-cluster sum of squares:  
## [1] 53.27 53.27  8.81 18.84</code></pre>
<div class="sourceCode" id="cb726"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb726-1"><a href="clustering-analysis.html#cb726-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(cluster_spec)),</span>
<span id="cb726-2"><a href="clustering-analysis.html#cb726-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">aes</span>(x, y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-259-1.png" width="672" /></p>
</div>
<div id="fuzzy-c-means-clustering" class="section level3" number="7.2.7">
<h3><span class="header-section-number">7.2.7</span> Fuzzy C-Means Clustering</h3>
<p>The <a href="https://en.wikipedia.org/wiki/Fuzzy_clustering">fuzzy clustering</a> version of the k-means clustering problem. Each data point
has a degree of membership to for each cluster.</p>
<div class="sourceCode" id="cb727"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb727-1"><a href="clustering-analysis.html#cb727-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;e1071&quot;</span>)</span>
<span id="cb727-2"><a href="clustering-analysis.html#cb727-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb727-3"><a href="clustering-analysis.html#cb727-3" aria-hidden="true" tabindex="-1"></a>cluster_cmeans <span class="ot">&lt;-</span> <span class="fu">cmeans</span>(<span class="fu">as.matrix</span>(ruspini_scaled), <span class="at">centers =</span> <span class="dv">4</span>)</span>
<span id="cb727-4"><a href="clustering-analysis.html#cb727-4" aria-hidden="true" tabindex="-1"></a>cluster_cmeans</span></code></pre></div>
<pre><code>## Fuzzy c-means clustering with 4 clusters
## 
## Cluster centers:
##        x      y
## 1 -1.137 -0.555
## 2  0.455 -1.476
## 3  1.505  0.516
## 4 -0.376  1.114
## 
## Memberships:
##              1        2        3        4
##  [1,] 0.012065 0.004750 7.76e-03 9.75e-01
##  [2,] 0.866509 0.074035 2.11e-02 3.84e-02
##  [3,] 0.971282 0.010239 4.91e-03 1.36e-02
##  [4,] 0.024935 0.947252 1.65e-02 1.14e-02
##  [5,] 0.020593 0.950361 1.82e-02 1.09e-02
##  [6,] 0.992095 0.003402 1.36e-03 3.14e-03
##  [7,] 0.039260 0.053619 8.11e-01 9.62e-02
##  [8,] 0.037605 0.013313 1.97e-02 9.29e-01
##  [9,] 0.024784 0.013940 3.40e-02 9.27e-01
## [10,] 0.025639 0.010355 1.73e-02 9.47e-01
## [11,] 0.008241 0.983990 4.42e-03 3.35e-03
## [12,] 0.001560 0.000705 1.32e-03 9.96e-01
## [13,] 0.003861 0.992177 2.30e-03 1.66e-03
## [14,] 0.768380 0.097124 4.14e-02 9.31e-02
## [15,] 0.005870 0.009963 9.73e-01 1.13e-02
## [16,] 0.024150 0.952363 1.34e-02 1.01e-02
## [17,] 0.828839 0.045276 2.77e-02 9.82e-02
## [18,] 0.904502 0.033979 1.64e-02 4.51e-02
## [19,] 0.003221 0.004747 9.85e-01 7.44e-03
## [20,] 0.934346 0.027260 1.15e-02 2.69e-02
## [21,] 0.003385 0.001497 2.77e-03 9.92e-01
## [22,] 0.020387 0.949234 1.93e-02 1.11e-02
## [23,] 0.107506 0.177387 5.41e-01 1.74e-01
## [24,] 0.011470 0.004817 8.41e-03 9.75e-01
## [25,] 0.018433 0.031839 9.16e-01 3.39e-02
## [26,] 0.004627 0.002182 4.27e-03 9.89e-01
## [27,] 0.003167 0.993633 1.85e-03 1.35e-03
## [28,] 0.000609 0.000943 9.97e-01 1.32e-03
## [29,] 0.028738 0.947019 1.34e-02 1.08e-02
## [30,] 0.071388 0.050971 1.76e-01 7.02e-01
## [31,] 0.000250 0.000411 9.99e-01 5.07e-04
## [32,] 0.939767 0.029086 1.01e-02 2.10e-02
## [33,] 0.000110 0.999766 7.43e-05 5.05e-05
## [34,] 0.860429 0.059383 2.50e-02 5.52e-02
## [35,] 0.895316 0.033633 1.80e-02 5.31e-02
## [36,] 0.065465 0.118857 7.06e-01 1.10e-01
## [37,] 0.128305 0.183755 4.70e-01 2.18e-01
## [38,] 0.011252 0.005928 1.35e-02 9.69e-01
## [39,] 0.007575 0.013540 9.65e-01 1.39e-02
## [40,] 0.890088 0.054964 1.83e-02 3.66e-02
## [41,] 0.067223 0.044821 1.33e-01 7.55e-01
## [42,] 0.022924 0.040523 8.96e-01 4.09e-02
## [43,] 0.009541 0.004635 9.54e-03 9.76e-01
## [44,] 0.048384 0.016805 2.45e-02 9.10e-01
## [45,] 0.914871 0.040505 1.46e-02 3.00e-02
## [46,] 0.049811 0.912543 2.04e-02 1.73e-02
## [47,] 0.038484 0.892180 4.59e-02 2.34e-02
## [48,] 0.004484 0.002237 4.75e-03 9.89e-01
## [49,] 0.015164 0.007890 1.73e-02 9.60e-01
## [50,] 0.872757 0.063345 2.13e-02 4.26e-02
## [51,] 0.006153 0.008725 9.70e-01 1.48e-02
## [52,] 0.075851 0.025668 3.63e-02 8.62e-01
## [53,] 0.942647 0.022073 9.90e-03 2.54e-02
## [54,] 0.041983 0.015519 2.38e-02 9.19e-01
## [55,] 0.017339 0.959100 1.45e-02 9.02e-03
## [56,] 0.998933 0.000436 1.84e-04 4.47e-04
## [57,] 0.020461 0.011470 2.85e-02 9.40e-01
## [58,] 0.018343 0.953743 1.78e-02 1.02e-02
## [59,] 0.037153 0.014629 2.37e-02 9.25e-01
## [60,] 0.962608 0.013809 6.49e-03 1.71e-02
## [61,] 0.013081 0.020545 9.40e-01 2.68e-02
## [62,] 0.930263 0.035820 1.14e-02 2.25e-02
## [63,] 0.954076 0.015519 7.84e-03 2.26e-02
## [64,] 0.010680 0.019237 9.51e-01 1.93e-02
## [65,] 0.039416 0.046127 7.88e-01 1.27e-01
## [66,] 0.000964 0.000451 8.88e-04 9.98e-01
## [67,] 0.973167 0.012776 4.51e-03 9.55e-03
## [68,] 0.025463 0.953144 1.19e-02 9.53e-03
## [69,] 0.003456 0.005041 9.83e-01 8.07e-03
## [70,] 0.010326 0.004135 6.88e-03 9.79e-01
## [71,] 0.033362 0.019994 5.51e-02 8.92e-01
## [72,] 0.003079 0.993497 2.03e-03 1.40e-03
## [73,] 0.887734 0.043108 2.00e-02 4.92e-02
## [74,] 0.001160 0.001840 9.95e-01 2.46e-03
## [75,] 0.092067 0.051905 1.05e-01 7.51e-01
## 
## Closest hard clustering:
##  [1] 4 1 1 2 2 1 3 4 4 4 2 4 2 1 3 2 1 1 3 1 4 2 3 4 3
## [26] 4 2 3 2 4 3 1 2 1 1 3 3 4 3 1 4 3 4 4 1 2 2 4 4 1
## [51] 3 4 1 4 2 1 4 2 4 1 3 1 1 3 3 4 1 2 3 4 4 2 1 3 4
## 
## Available components:
## [1] &quot;centers&quot;     &quot;size&quot;        &quot;cluster&quot;    
## [4] &quot;membership&quot;  &quot;iter&quot;        &quot;withinerror&quot;
## [7] &quot;call&quot;</code></pre>
<p>Plot membership (shown as small pie charts)</p>
<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb729-1"><a href="clustering-analysis.html#cb729-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;scatterpie&quot;</span>)</span>
<span id="cb729-2"><a href="clustering-analysis.html#cb729-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>()  <span class="sc">+</span></span>
<span id="cb729-3"><a href="clustering-analysis.html#cb729-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_scatterpie</span>(<span class="at">data =</span> <span class="fu">cbind</span>(ruspini_scaled, cluster_cmeans<span class="sc">$</span>membership),</span>
<span id="cb729-4"><a href="clustering-analysis.html#cb729-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">cols =</span> <span class="fu">colnames</span>(cluster_cmeans<span class="sc">$</span>membership), <span class="at">legend_name =</span> <span class="st">&quot;Membership&quot;</span>) <span class="sc">+</span> <span class="fu">coord_equal</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-261-1.png" width="672" /></p>
</div>
</div>
<div id="internal-cluster-validation" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Internal Cluster Validation</h2>
<div id="compare-the-clustering-quality" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Compare the Clustering Quality</h3>
<p>The two most popular quality metrics are the within-cluster sum of squares (WCSS) used
by <a href="https://en.wikipedia.org/wiki/K-means_clustering"><span class="math inline">\(k\)</span>-means</a> and
the <a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)">average silhouette width</a>.
Look at <code>within.cluster.ss</code> and <code>avg.silwidth</code> below.</p>
<div class="sourceCode" id="cb730"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb730-1"><a href="clustering-analysis.html#cb730-1" aria-hidden="true" tabindex="-1"></a><span class="do">##library(fpc)</span></span></code></pre></div>
<p>Notes:
* I do not load fpc since the NAMESPACE overwrites dbscan.
* The clustering (second argument below) has to be supplied as a vector with numbers (cluster IDs) and cannot be a factor (use <code>as.integer()</code> to convert the factor to an ID).</p>
<div class="sourceCode" id="cb731"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb731-1"><a href="clustering-analysis.html#cb731-1" aria-hidden="true" tabindex="-1"></a>fpc<span class="sc">::</span><span class="fu">cluster.stats</span>(d, km<span class="sc">$</span>cluster)</span></code></pre></div>
<pre><code>## $n
## [1] 75
## 
## $cluster.number
## [1] 4
## 
## $cluster.size
## [1] 17 23 15 20
## 
## $min.cluster.size
## [1] 15
## 
## $noisen
## [1] 0
## 
## $diameter
## [1] 1.463 1.159 0.836 1.119
## 
## $average.distance
## [1] 0.581 0.429 0.356 0.482
## 
## $median.distance
## [1] 0.502 0.393 0.338 0.449
## 
## $separation
## [1] 0.768 0.768 1.158 1.158
## 
## $average.toother
## [1] 2.29 2.15 2.31 2.16
## 
## $separation.matrix
##       [,1]  [,2] [,3] [,4]
## [1,] 0.000 0.768 1.31 1.34
## [2,] 0.768 0.000 1.96 1.22
## [3,] 1.308 1.958 0.00 1.16
## [4,] 1.340 1.220 1.16 0.00
## 
## $ave.between.matrix
##      [,1] [,2] [,3] [,4]
## [1,] 0.00 1.92 2.22 2.77
## [2,] 1.92 0.00 2.75 1.89
## [3,] 2.22 2.75 0.00 1.87
## [4,] 2.77 1.89 1.87 0.00
## 
## $average.between
## [1] 2.22
## 
## $average.within
## [1] 0.463
## 
## $n.between
## [1] 2091
## 
## $n.within
## [1] 684
## 
## $max.diameter
## [1] 1.46
## 
## $min.separation
## [1] 0.768
## 
## $within.cluster.ss
## [1] 10.1
## 
## $clus.avg.silwidths
##     1     2     3     4 
## 0.681 0.745 0.807 0.721 
## 
## $avg.silwidth
## [1] 0.737
## 
## $g2
## NULL
## 
## $g3
## NULL
## 
## $pearsongamma
## [1] 0.842
## 
## $dunn
## [1] 0.525
## 
## $dunn2
## [1] 3.23
## 
## $entropy
## [1] 1.37
## 
## $wb.ratio
## [1] 0.209
## 
## $ch
## [1] 324
## 
## $cwidegap
## [1] 0.415 0.315 0.235 0.261
## 
## $widestgap
## [1] 0.415
## 
## $sindex
## [1] 0.858
## 
## $corrected.rand
## NULL
## 
## $vi
## NULL</code></pre>
<p>Read <code>? cluster.stats</code> for an explanation of all the available indices.</p>
<div class="sourceCode" id="cb733"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb733-1"><a href="clustering-analysis.html#cb733-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sapply</span>(</span>
<span id="cb733-2"><a href="clustering-analysis.html#cb733-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb733-3"><a href="clustering-analysis.html#cb733-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">km =</span> km<span class="sc">$</span>cluster,</span>
<span id="cb733-4"><a href="clustering-analysis.html#cb733-4" aria-hidden="true" tabindex="-1"></a>    <span class="at">hc_compl =</span> <span class="fu">cutree</span>(hc, <span class="at">k =</span> <span class="dv">4</span>),</span>
<span id="cb733-5"><a href="clustering-analysis.html#cb733-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">hc_single =</span> <span class="fu">cutree</span>(hc_single, <span class="at">k =</span> <span class="dv">4</span>)</span>
<span id="cb733-6"><a href="clustering-analysis.html#cb733-6" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb733-7"><a href="clustering-analysis.html#cb733-7" aria-hidden="true" tabindex="-1"></a>  <span class="at">FUN =</span> <span class="cf">function</span>(x)</span>
<span id="cb733-8"><a href="clustering-analysis.html#cb733-8" aria-hidden="true" tabindex="-1"></a>    fpc<span class="sc">::</span><span class="fu">cluster.stats</span>(d, x))[<span class="fu">c</span>(<span class="st">&quot;within.cluster.ss&quot;</span>, <span class="st">&quot;avg.silwidth&quot;</span>), ]</span></code></pre></div>
<pre><code>##                   km    hc_compl hc_single
## within.cluster.ss 10.1  10.1     10.1     
## avg.silwidth      0.737 0.737    0.737</code></pre>
</div>
<div id="silhouette-plot" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Silhouette plot</h3>
<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb735-1"><a href="clustering-analysis.html#cb735-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb735-2"><a href="clustering-analysis.html#cb735-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">silhouette</span>(km<span class="sc">$</span>cluster, d))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-265-1.png" width="672" /></p>
<p><strong>Note:</strong> The silhouette plot does not show correctly in R Studio if you have too many objects (bars are missing). I will work when you open a new plotting device with <code>windows()</code>, <code>x11()</code> or <code>quartz()</code>.</p>
<p>ggplot visualization using <code>factoextra</code></p>
<div class="sourceCode" id="cb736"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb736-1"><a href="clustering-analysis.html#cb736-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_silhouette</span>(<span class="fu">silhouette</span>(km<span class="sc">$</span>cluster, d))</span></code></pre></div>
<pre><code>##   cluster size ave.sil.width
## 1       1   17          0.68
## 2       2   23          0.75
## 3       3   15          0.81
## 4       4   20          0.72</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-266-1.png" width="672" /></p>
</div>
<div id="find-optimal-number-of-clusters-for-k-means" class="section level3" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Find Optimal Number of Clusters for k-means</h3>
<div class="sourceCode" id="cb738"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb738-1"><a href="clustering-analysis.html#cb738-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled, <span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-267-1.png" width="672" /></p>
<div class="sourceCode" id="cb739"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb739-1"><a href="clustering-analysis.html#cb739-1" aria-hidden="true" tabindex="-1"></a><span class="do">## We will use different methods and try 1-10 clusters.</span></span>
<span id="cb739-2"><a href="clustering-analysis.html#cb739-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb739-3"><a href="clustering-analysis.html#cb739-3" aria-hidden="true" tabindex="-1"></a>ks <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">10</span></span></code></pre></div>
<div id="elbow-method-within-cluster-sum-of-squares" class="section level4" number="7.3.3.1">
<h4><span class="header-section-number">7.3.3.1</span> Elbow Method: Within-Cluster Sum of Squares</h4>
<p>Calculate the within-cluster sum of squares for different numbers of clusters and look for the <a href="https://en.wikipedia.org/wiki/Elbow_method_(clustering)">knee or elbow</a> in the plot.
(<code>nstart = 5</code> just repeats k-means 5 times and returns the best solution)</p>
<div class="sourceCode" id="cb740"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb740-1"><a href="clustering-analysis.html#cb740-1" aria-hidden="true" tabindex="-1"></a>WCSS <span class="ot">&lt;-</span> <span class="fu">sapply</span>(ks, <span class="at">FUN =</span> <span class="cf">function</span>(k) {</span>
<span id="cb740-2"><a href="clustering-analysis.html#cb740-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kmeans</span>(ruspini_scaled, <span class="at">centers =</span> k, <span class="at">nstart =</span> <span class="dv">5</span>)<span class="sc">$</span>tot.withinss</span>
<span id="cb740-3"><a href="clustering-analysis.html#cb740-3" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb740-4"><a href="clustering-analysis.html#cb740-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb740-5"><a href="clustering-analysis.html#cb740-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">as_tibble</span>(ks, WCSS), <span class="fu">aes</span>(ks, WCSS)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb740-6"><a href="clustering-analysis.html#cb740-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">4</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">linetype =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-268-1.png" width="672" /></p>
</div>
<div id="average-silhouette-width" class="section level4" number="7.3.3.2">
<h4><span class="header-section-number">7.3.3.2</span> Average Silhouette Width</h4>
<p>Plot the average silhouette width for different number of clusters and look for the maximum in the plot.</p>
<div class="sourceCode" id="cb741"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb741-1"><a href="clustering-analysis.html#cb741-1" aria-hidden="true" tabindex="-1"></a>ASW <span class="ot">&lt;-</span> <span class="fu">sapply</span>(ks, <span class="at">FUN=</span><span class="cf">function</span>(k) {</span>
<span id="cb741-2"><a href="clustering-analysis.html#cb741-2" aria-hidden="true" tabindex="-1"></a>  fpc<span class="sc">::</span><span class="fu">cluster.stats</span>(d, <span class="fu">kmeans</span>(ruspini_scaled, <span class="at">centers=</span>k, <span class="at">nstart =</span> <span class="dv">5</span>)<span class="sc">$</span>cluster)<span class="sc">$</span>avg.silwidth</span>
<span id="cb741-3"><a href="clustering-analysis.html#cb741-3" aria-hidden="true" tabindex="-1"></a>  })</span>
<span id="cb741-4"><a href="clustering-analysis.html#cb741-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb741-5"><a href="clustering-analysis.html#cb741-5" aria-hidden="true" tabindex="-1"></a>best_k <span class="ot">&lt;-</span> ks[<span class="fu">which.max</span>(ASW)]</span>
<span id="cb741-6"><a href="clustering-analysis.html#cb741-6" aria-hidden="true" tabindex="-1"></a>best_k</span></code></pre></div>
<pre><code>## [1] 4</code></pre>
<div class="sourceCode" id="cb743"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb743-1"><a href="clustering-analysis.html#cb743-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">as_tibble</span>(ks, ASW), <span class="fu">aes</span>(ks, ASW)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb743-2"><a href="clustering-analysis.html#cb743-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> best_k, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">linetype =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-269-1.png" width="672" /></p>
</div>
<div id="dunn-index" class="section level4" number="7.3.3.3">
<h4><span class="header-section-number">7.3.3.3</span> Dunn Index</h4>
<p>Use <a href="https://en.wikipedia.org/wiki/Dunn_index">Dunn index</a> (another internal measure given by min. separation/ max. diameter)</p>
<div class="sourceCode" id="cb744"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb744-1"><a href="clustering-analysis.html#cb744-1" aria-hidden="true" tabindex="-1"></a>DI <span class="ot">&lt;-</span> <span class="fu">sapply</span>(ks, <span class="at">FUN=</span><span class="cf">function</span>(k) {</span>
<span id="cb744-2"><a href="clustering-analysis.html#cb744-2" aria-hidden="true" tabindex="-1"></a>  fpc<span class="sc">::</span><span class="fu">cluster.stats</span>(d, <span class="fu">kmeans</span>(ruspini_scaled, <span class="at">centers=</span>k, <span class="at">nstart=</span><span class="dv">5</span>)<span class="sc">$</span>cluster)<span class="sc">$</span>dunn</span>
<span id="cb744-3"><a href="clustering-analysis.html#cb744-3" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb744-4"><a href="clustering-analysis.html#cb744-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb744-5"><a href="clustering-analysis.html#cb744-5" aria-hidden="true" tabindex="-1"></a>best_k <span class="ot">&lt;-</span> ks[<span class="fu">which.max</span>(DI)]</span>
<span id="cb744-6"><a href="clustering-analysis.html#cb744-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">as_tibble</span>(ks, DI), <span class="fu">aes</span>(ks, DI)) <span class="sc">+</span> <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb744-7"><a href="clustering-analysis.html#cb744-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> best_k, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">linetype =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-270-1.png" width="672" /></p>
</div>
<div id="gap-statistic" class="section level4" number="7.3.3.4">
<h4><span class="header-section-number">7.3.3.4</span> Gap Statistic</h4>
<p>Compares the change in within-cluster dispersion with that expected
from a null model (see <code>? clusGap</code>).
The default method is to
choose the smallest k such that its value Gap(k) is not more
than 1 standard error away from the first local maximum.</p>
<div class="sourceCode" id="cb745"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb745-1"><a href="clustering-analysis.html#cb745-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cluster)</span>
<span id="cb745-2"><a href="clustering-analysis.html#cb745-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">clusGap</span>(ruspini_scaled, <span class="at">FUN =</span> kmeans,  <span class="at">nstart =</span> <span class="dv">10</span>, <span class="at">K.max =</span> <span class="dv">10</span>)</span>
<span id="cb745-3"><a href="clustering-analysis.html#cb745-3" aria-hidden="true" tabindex="-1"></a>k</span></code></pre></div>
<pre><code>## Clustering Gap statistic [&quot;clusGap&quot;] from call:
## clusGap(x = ruspini_scaled, FUNcluster = kmeans, K.max = 10,     nstart = 10)
## B=100 simulated reference sets, k = 1..10; spaceH0=&quot;scaledPCA&quot;
##  --&gt; Number of clusters (method &#39;firstSEmax&#39;, SE.factor=1): 4
##       logW E.logW     gap SE.sim
##  [1,] 3.50   3.47 -0.0308 0.0357
##  [2,] 3.07   3.15  0.0762 0.0374
##  [3,] 2.68   2.90  0.2247 0.0380
##  [4,] 2.11   2.70  0.5971 0.0363
##  [5,] 1.99   2.57  0.5827 0.0347
##  [6,] 1.86   2.45  0.5871 0.0365
##  [7,] 1.73   2.35  0.6156 0.0395
##  [8,] 1.66   2.26  0.5987 0.0413
##  [9,] 1.61   2.17  0.5630 0.0409
## [10,] 1.50   2.09  0.5910 0.0393</code></pre>
<div class="sourceCode" id="cb747"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb747-1"><a href="clustering-analysis.html#cb747-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(k)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-271-1.png" width="672" /></p>
<p><strong>Note:</strong> these methods can also be used for hierarchical clustering.</p>
<p>There have been many other methods and indices proposed to determine
the number of clusters.
See, e.g., package <a href="https://cran.r-project.org/package=NbClust">NbClust</a>.</p>
</div>
</div>
<div id="visualizing-the-distance-matrix" class="section level3" number="7.3.4">
<h3><span class="header-section-number">7.3.4</span> Visualizing the Distance Matrix</h3>
<div class="sourceCode" id="cb748"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb748-1"><a href="clustering-analysis.html#cb748-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled, <span class="fu">aes</span>(x, y, <span class="at">color =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster))) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-272-1.png" width="672" /></p>
<div class="sourceCode" id="cb749"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb749-1"><a href="clustering-analysis.html#cb749-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">dist</span>(ruspini_scaled)</span></code></pre></div>
<p>Inspect the distance matrix between the first 5 objects.</p>
<div class="sourceCode" id="cb750"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb750-1"><a href="clustering-analysis.html#cb750-1" aria-hidden="true" tabindex="-1"></a><span class="fu">as.matrix</span>(d)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>]</span></code></pre></div>
<pre><code>##      1     2     3     4     5
## 1 0.00 1.887 1.511 3.041 2.978
## 2 1.89 0.000 0.522 1.640 1.746
## 3 1.51 0.522 0.000 2.131 2.207
## 4 3.04 1.640 2.131 0.000 0.282
## 5 2.98 1.746 2.207 0.282 0.000</code></pre>
<p>A false-color image visualizes each value in the matrix as a pixel with the color representing the value.</p>
<div class="sourceCode" id="cb752"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb752-1"><a href="clustering-analysis.html#cb752-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(seriation)</span>
<span id="cb752-2"><a href="clustering-analysis.html#cb752-2" aria-hidden="true" tabindex="-1"></a><span class="fu">pimage</span>(d, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-274-1.png" width="672" /></p>
<p>Rows and columns are the objects as they are ordered in the data set. The diagonal represents the distance between an object and itself and has by definition a distance of 0 (dark line).
Visualizing the unordered distance matrix does not show much structure, but we can reorder
the matrix (rows and columns) using the k-means cluster labels from cluster 1 to 4. A clear block structure representing the clusters becomes visible.</p>
<div class="sourceCode" id="cb753"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb753-1"><a href="clustering-analysis.html#cb753-1" aria-hidden="true" tabindex="-1"></a><span class="fu">pimage</span>(d, <span class="at">order=</span><span class="fu">order</span>(km<span class="sc">$</span>cluster), <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-275-1.png" width="672" /></p>
<p>Plot function <code>dissplot</code> in package <strong>seriation</strong> rearranges the matrix and adds lines and cluster labels. In the lower half of the plot, it shows average dissimilarities between clusters. The function
organizes the objects by cluster and then reorders clusters and objects within clusters so that more similar objects are closer together.</p>
<div class="sourceCode" id="cb754"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb754-1"><a href="clustering-analysis.html#cb754-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dissplot</span>(d, <span class="at">labels =</span> km<span class="sc">$</span>cluster, <span class="at">options=</span><span class="fu">list</span>(<span class="at">main=</span><span class="st">&quot;k-means with k=4&quot;</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-276-1.png" width="672" /></p>
<p>The reordering by <code>dissplot</code> makes the misspecification of k visible as blocks.</p>
<div class="sourceCode" id="cb755"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb755-1"><a href="clustering-analysis.html#cb755-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dissplot</span>(d, <span class="at">labels =</span> <span class="fu">kmeans</span>(ruspini_scaled, <span class="at">centers =</span> <span class="dv">3</span>)<span class="sc">$</span>cluster, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-277-1.png" width="672" /></p>
<div class="sourceCode" id="cb756"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb756-1"><a href="clustering-analysis.html#cb756-1" aria-hidden="true" tabindex="-1"></a><span class="fu">dissplot</span>(d, <span class="at">labels =</span> <span class="fu">kmeans</span>(ruspini_scaled, <span class="at">centers =</span> <span class="dv">9</span>)<span class="sc">$</span>cluster, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-277-2.png" width="672" /></p>
<p>Using <code>factoextra</code></p>
<div class="sourceCode" id="cb757"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb757-1"><a href="clustering-analysis.html#cb757-1" aria-hidden="true" tabindex="-1"></a><span class="fu">fviz_dist</span>(d)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-278-1.png" width="672" /></p>
</div>
</div>
<div id="external-cluster-validation" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> External Cluster Validation</h2>
<p>External cluster validation uses ground truth information. That is,
the user has an idea how the data should be grouped. This could be a known
class label not provided to the clustering algorithm.</p>
<p>We use an artificial data set with known groups.</p>
<div class="sourceCode" id="cb758"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb758-1"><a href="clustering-analysis.html#cb758-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlbench)</span>
<span id="cb758-2"><a href="clustering-analysis.html#cb758-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb758-3"><a href="clustering-analysis.html#cb758-3" aria-hidden="true" tabindex="-1"></a>shapes <span class="ot">&lt;-</span> <span class="fu">mlbench.smiley</span>(<span class="at">n =</span> <span class="dv">500</span>, <span class="at">sd1 =</span> <span class="fl">0.1</span>, <span class="at">sd2 =</span> <span class="fl">0.05</span>)</span>
<span id="cb758-4"><a href="clustering-analysis.html#cb758-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(shapes)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-279-1.png" width="672" /></p>
<p>Prepare data</p>
<div class="sourceCode" id="cb759"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb759-1"><a href="clustering-analysis.html#cb759-1" aria-hidden="true" tabindex="-1"></a>truth <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(shapes<span class="sc">$</span>class)</span>
<span id="cb759-2"><a href="clustering-analysis.html#cb759-2" aria-hidden="true" tabindex="-1"></a>shapes <span class="ot">&lt;-</span> <span class="fu">scale</span>(shapes<span class="sc">$</span>x)</span>
<span id="cb759-3"><a href="clustering-analysis.html#cb759-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(shapes) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>)</span>
<span id="cb759-4"><a href="clustering-analysis.html#cb759-4" aria-hidden="true" tabindex="-1"></a>shapes <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(shapes)</span>
<span id="cb759-5"><a href="clustering-analysis.html#cb759-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb759-6"><a href="clustering-analysis.html#cb759-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(shapes, <span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-280-1.png" width="672" /></p>
<p>Find optimal number of Clusters for k-means</p>
<div class="sourceCode" id="cb760"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb760-1"><a href="clustering-analysis.html#cb760-1" aria-hidden="true" tabindex="-1"></a>ks <span class="ot">&lt;-</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">20</span></span></code></pre></div>
<p>Use within sum of squares (look for the knee)</p>
<div class="sourceCode" id="cb761"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb761-1"><a href="clustering-analysis.html#cb761-1" aria-hidden="true" tabindex="-1"></a>WCSS <span class="ot">&lt;-</span> <span class="fu">sapply</span>(ks, <span class="at">FUN =</span> <span class="cf">function</span>(k) {</span>
<span id="cb761-2"><a href="clustering-analysis.html#cb761-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">kmeans</span>(shapes, <span class="at">centers =</span> k, <span class="at">nstart =</span> <span class="dv">10</span>)<span class="sc">$</span>tot.withinss</span>
<span id="cb761-3"><a href="clustering-analysis.html#cb761-3" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb761-4"><a href="clustering-analysis.html#cb761-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb761-5"><a href="clustering-analysis.html#cb761-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">as_tibble</span>(ks, WCSS), <span class="fu">aes</span>(ks, WCSS)) <span class="sc">+</span> <span class="fu">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-282-1.png" width="672" /></p>
<p>Looks like it could be 7 clusters</p>
<div class="sourceCode" id="cb762"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb762-1"><a href="clustering-analysis.html#cb762-1" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(shapes, <span class="at">centers =</span> <span class="dv">7</span>, <span class="at">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb762-2"><a href="clustering-analysis.html#cb762-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb762-3"><a href="clustering-analysis.html#cb762-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(shapes <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster)), <span class="fu">aes</span>(x, y, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb762-4"><a href="clustering-analysis.html#cb762-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-283-1.png" width="672" /></p>
<p>Hierarchical clustering: We use single-link because of the mouth is non-convex and chaining may help.</p>
<div class="sourceCode" id="cb763"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb763-1"><a href="clustering-analysis.html#cb763-1" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">dist</span>(shapes)</span>
<span id="cb763-2"><a href="clustering-analysis.html#cb763-2" aria-hidden="true" tabindex="-1"></a>hc <span class="ot">&lt;-</span> <span class="fu">hclust</span>(d, <span class="at">method =</span> <span class="st">&quot;single&quot;</span>)</span></code></pre></div>
<p>Find optimal number of clusters</p>
<div class="sourceCode" id="cb764"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb764-1"><a href="clustering-analysis.html#cb764-1" aria-hidden="true" tabindex="-1"></a>ASW <span class="ot">&lt;-</span> <span class="fu">sapply</span>(ks, <span class="at">FUN =</span> <span class="cf">function</span>(k) {</span>
<span id="cb764-2"><a href="clustering-analysis.html#cb764-2" aria-hidden="true" tabindex="-1"></a>  fpc<span class="sc">::</span><span class="fu">cluster.stats</span>(d, <span class="fu">cutree</span>(hc, k))<span class="sc">$</span>avg.silwidth</span>
<span id="cb764-3"><a href="clustering-analysis.html#cb764-3" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb764-4"><a href="clustering-analysis.html#cb764-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb764-5"><a href="clustering-analysis.html#cb764-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">as_tibble</span>(ks, ASW), <span class="fu">aes</span>(ks, ASW)) <span class="sc">+</span> <span class="fu">geom_line</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-285-1.png" width="672" /></p>
<p>The maximum is clearly at 4 clusters.</p>
<div class="sourceCode" id="cb765"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb765-1"><a href="clustering-analysis.html#cb765-1" aria-hidden="true" tabindex="-1"></a>hc_4 <span class="ot">&lt;-</span> <span class="fu">cutree</span>(hc, <span class="dv">4</span>)</span>
<span id="cb765-2"><a href="clustering-analysis.html#cb765-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb765-3"><a href="clustering-analysis.html#cb765-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(shapes <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(hc_4)), <span class="fu">aes</span>(x, y, <span class="at">color =</span> cluster)) <span class="sc">+</span></span>
<span id="cb765-4"><a href="clustering-analysis.html#cb765-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-286-1.png" width="672" /></p>
<p>Compare with ground truth with the <a href="https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index">corrected (=adjusted) Rand index (ARI)</a>,
the <a href="https://en.wikipedia.org/wiki/Variation_of_information">variation of information (VI) index</a>, <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a> and <a href="https://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation">purity</a>.</p>
<p><code>cluster_stats</code> computes ARI and VI as comparative measures. I define functions for
entropy and purity here:</p>
<div class="sourceCode" id="cb766"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb766-1"><a href="clustering-analysis.html#cb766-1" aria-hidden="true" tabindex="-1"></a>entropy <span class="ot">&lt;-</span> <span class="cf">function</span>(cluster, truth) {</span>
<span id="cb766-2"><a href="clustering-analysis.html#cb766-2" aria-hidden="true" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="fu">max</span>(cluster, truth)</span>
<span id="cb766-3"><a href="clustering-analysis.html#cb766-3" aria-hidden="true" tabindex="-1"></a>  cluster <span class="ot">&lt;-</span> <span class="fu">factor</span>(cluster, <span class="at">levels =</span> <span class="dv">1</span><span class="sc">:</span>k)</span>
<span id="cb766-4"><a href="clustering-analysis.html#cb766-4" aria-hidden="true" tabindex="-1"></a>  truth <span class="ot">&lt;-</span> <span class="fu">factor</span>(truth, <span class="at">levels =</span> <span class="dv">1</span><span class="sc">:</span>k)</span>
<span id="cb766-5"><a href="clustering-analysis.html#cb766-5" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="fu">table</span>(cluster)<span class="sc">/</span><span class="fu">length</span>(cluster)</span>
<span id="cb766-6"><a href="clustering-analysis.html#cb766-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb766-7"><a href="clustering-analysis.html#cb766-7" aria-hidden="true" tabindex="-1"></a>  cnts <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">split</span>(truth, cluster), table)</span>
<span id="cb766-8"><a href="clustering-analysis.html#cb766-8" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">sweep</span>(cnts, <span class="dv">1</span>, <span class="fu">rowSums</span>(cnts), <span class="st">&quot;/&quot;</span>)</span>
<span id="cb766-9"><a href="clustering-analysis.html#cb766-9" aria-hidden="true" tabindex="-1"></a>  p[<span class="fu">is.nan</span>(p)] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb766-10"><a href="clustering-analysis.html#cb766-10" aria-hidden="true" tabindex="-1"></a>  e <span class="ot">&lt;-</span> <span class="sc">-</span>p <span class="sc">*</span> <span class="fu">log</span>(p, <span class="dv">2</span>)</span>
<span id="cb766-11"><a href="clustering-analysis.html#cb766-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb766-12"><a href="clustering-analysis.html#cb766-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(w <span class="sc">*</span> <span class="fu">rowSums</span>(e, <span class="at">na.rm =</span> <span class="cn">TRUE</span>))</span>
<span id="cb766-13"><a href="clustering-analysis.html#cb766-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb766-14"><a href="clustering-analysis.html#cb766-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb766-15"><a href="clustering-analysis.html#cb766-15" aria-hidden="true" tabindex="-1"></a>purity <span class="ot">&lt;-</span> <span class="cf">function</span>(cluster, truth) {</span>
<span id="cb766-16"><a href="clustering-analysis.html#cb766-16" aria-hidden="true" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="fu">max</span>(cluster, truth)</span>
<span id="cb766-17"><a href="clustering-analysis.html#cb766-17" aria-hidden="true" tabindex="-1"></a>  cluster <span class="ot">&lt;-</span> <span class="fu">factor</span>(cluster, <span class="at">levels =</span> <span class="dv">1</span><span class="sc">:</span>k)</span>
<span id="cb766-18"><a href="clustering-analysis.html#cb766-18" aria-hidden="true" tabindex="-1"></a>  truth <span class="ot">&lt;-</span> <span class="fu">factor</span>(truth, <span class="at">levels =</span> <span class="dv">1</span><span class="sc">:</span>k)</span>
<span id="cb766-19"><a href="clustering-analysis.html#cb766-19" aria-hidden="true" tabindex="-1"></a>  w <span class="ot">&lt;-</span> <span class="fu">table</span>(cluster)<span class="sc">/</span><span class="fu">length</span>(cluster)</span>
<span id="cb766-20"><a href="clustering-analysis.html#cb766-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb766-21"><a href="clustering-analysis.html#cb766-21" aria-hidden="true" tabindex="-1"></a>  cnts <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">split</span>(truth, cluster), table)</span>
<span id="cb766-22"><a href="clustering-analysis.html#cb766-22" aria-hidden="true" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">sweep</span>(cnts, <span class="dv">1</span>, <span class="fu">rowSums</span>(cnts), <span class="st">&quot;/&quot;</span>)</span>
<span id="cb766-23"><a href="clustering-analysis.html#cb766-23" aria-hidden="true" tabindex="-1"></a>  p[<span class="fu">is.nan</span>(p)] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb766-24"><a href="clustering-analysis.html#cb766-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb766-25"><a href="clustering-analysis.html#cb766-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">sum</span>(w <span class="sc">*</span> <span class="fu">apply</span>(p, <span class="dv">1</span>, max))</span>
<span id="cb766-26"><a href="clustering-analysis.html#cb766-26" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>calculate measures (for comparison we also use random “clusterings”
with 4 and 6 clusters)</p>
<div class="sourceCode" id="cb767"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb767-1"><a href="clustering-analysis.html#cb767-1" aria-hidden="true" tabindex="-1"></a>random_4 <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>, <span class="fu">nrow</span>(shapes), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb767-2"><a href="clustering-analysis.html#cb767-2" aria-hidden="true" tabindex="-1"></a>random_6 <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>, <span class="fu">nrow</span>(shapes), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb767-3"><a href="clustering-analysis.html#cb767-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb767-4"><a href="clustering-analysis.html#cb767-4" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="fu">rbind</span>(</span>
<span id="cb767-5"><a href="clustering-analysis.html#cb767-5" aria-hidden="true" tabindex="-1"></a>  <span class="at">kmeans_7 =</span> <span class="fu">c</span>(</span>
<span id="cb767-6"><a href="clustering-analysis.html#cb767-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">unlist</span>(fpc<span class="sc">::</span><span class="fu">cluster.stats</span>(d, km<span class="sc">$</span>cluster, truth, <span class="at">compareonly =</span> <span class="cn">TRUE</span>)),</span>
<span id="cb767-7"><a href="clustering-analysis.html#cb767-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">entropy =</span> <span class="fu">entropy</span>(km<span class="sc">$</span>cluster, truth),</span>
<span id="cb767-8"><a href="clustering-analysis.html#cb767-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">purity =</span> <span class="fu">purity</span>(km<span class="sc">$</span>cluster, truth)</span>
<span id="cb767-9"><a href="clustering-analysis.html#cb767-9" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb767-10"><a href="clustering-analysis.html#cb767-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">hc_4 =</span> <span class="fu">c</span>(</span>
<span id="cb767-11"><a href="clustering-analysis.html#cb767-11" aria-hidden="true" tabindex="-1"></a>    <span class="fu">unlist</span>(fpc<span class="sc">::</span><span class="fu">cluster.stats</span>(d, hc_4, truth, <span class="at">compareonly =</span> <span class="cn">TRUE</span>)),</span>
<span id="cb767-12"><a href="clustering-analysis.html#cb767-12" aria-hidden="true" tabindex="-1"></a>    <span class="at">entropy =</span> <span class="fu">entropy</span>(hc_4, truth),</span>
<span id="cb767-13"><a href="clustering-analysis.html#cb767-13" aria-hidden="true" tabindex="-1"></a>    <span class="at">purity =</span> <span class="fu">purity</span>(hc_4, truth)</span>
<span id="cb767-14"><a href="clustering-analysis.html#cb767-14" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb767-15"><a href="clustering-analysis.html#cb767-15" aria-hidden="true" tabindex="-1"></a>  <span class="at">random_4 =</span> <span class="fu">c</span>(</span>
<span id="cb767-16"><a href="clustering-analysis.html#cb767-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">unlist</span>(fpc<span class="sc">::</span><span class="fu">cluster.stats</span>(d, random_4, truth, <span class="at">compareonly =</span> <span class="cn">TRUE</span>)),</span>
<span id="cb767-17"><a href="clustering-analysis.html#cb767-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">entropy =</span> <span class="fu">entropy</span>(random_4, truth),</span>
<span id="cb767-18"><a href="clustering-analysis.html#cb767-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">purity =</span> <span class="fu">purity</span>(random_4, truth)</span>
<span id="cb767-19"><a href="clustering-analysis.html#cb767-19" aria-hidden="true" tabindex="-1"></a>    ),</span>
<span id="cb767-20"><a href="clustering-analysis.html#cb767-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">random_6 =</span> <span class="fu">c</span>(</span>
<span id="cb767-21"><a href="clustering-analysis.html#cb767-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">unlist</span>(fpc<span class="sc">::</span><span class="fu">cluster.stats</span>(d, random_6, truth, <span class="at">compareonly =</span> <span class="cn">TRUE</span>)),</span>
<span id="cb767-22"><a href="clustering-analysis.html#cb767-22" aria-hidden="true" tabindex="-1"></a>    <span class="at">entropy =</span> <span class="fu">entropy</span>(random_6, truth),</span>
<span id="cb767-23"><a href="clustering-analysis.html#cb767-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">purity =</span> <span class="fu">purity</span>(random_6, truth)</span>
<span id="cb767-24"><a href="clustering-analysis.html#cb767-24" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb767-25"><a href="clustering-analysis.html#cb767-25" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb767-26"><a href="clustering-analysis.html#cb767-26" aria-hidden="true" tabindex="-1"></a>r</span></code></pre></div>
<pre><code>##          corrected.rand    vi entropy purity
## kmeans_7        0.63823 0.571   0.229  0.464
## hc_4            1.00000 0.000   0.000  1.000
## random_4       -0.00324 2.683   1.988  0.288
## random_6       -0.00213 3.076   1.728  0.144</code></pre>
<p>Notes:</p>
<ul>
<li>Hierarchical clustering found the perfect clustering.</li>
<li>Entropy and purity are heavily impacted by the number of clusters (more clusters improve the metric).</li>
<li>The corrected rand index shows clearly that the random clusterings have no relationship with the ground truth (very close to 0). This is a very helpful property.</li>
</ul>
<p>Read <code>? cluster.stats</code> for an explanation of all the available indices.</p>
</div>
<div id="advanced-data-preparation-for-clustering" class="section level2" number="7.5">
<h2><span class="header-section-number">7.5</span> Advanced Data Preparation for Clustering</h2>
<div id="outlier-removal" class="section level3" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Outlier Removal</h3>
<p>Most clustering algorithms perform complete assignment (i.e., all data points need to be assigned to a cluster). Outliers will affect the clustering. It is useful to identify outliers and remove strong outliers prior to clustering.
A density based method to identify outlier is <a href="https://en.wikipedia.org/wiki/Local_outlier_factor">LOF</a> (Local Outlier Factor).
It is related to dbscan and compares the density around a point with the
densities around its neighbors (you have to specify the neighborhood size <span class="math inline">\(k\)</span>).
The LOF value for a regular data point is 1.
The larger the LOF value gets, the more likely the point is an outlier.</p>
<div class="sourceCode" id="cb769"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb769-1"><a href="clustering-analysis.html#cb769-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dbscan)</span></code></pre></div>
<p>Add a clear outlier to the scaled Ruspini dataset that is 10 standard deviations above the average for the x axis.</p>
<div class="sourceCode" id="cb770"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb770-1"><a href="clustering-analysis.html#cb770-1" aria-hidden="true" tabindex="-1"></a>ruspini_scaled_outlier <span class="ot">&lt;-</span> ruspini_scaled <span class="sc">%&gt;%</span> <span class="fu">add_case</span>(<span class="at">x=</span><span class="dv">10</span>,<span class="at">y=</span><span class="dv">0</span>)</span></code></pre></div>
<div id="visual-inspection-of-the-data" class="section level4" number="7.5.1.1">
<h4><span class="header-section-number">7.5.1.1</span> Visual inspection of the data</h4>
<p>Outliers can be identified using summary statistics, histograms, scatterplots (pairs plots), and boxplots, etc. We use here a pairs plot (the diagonal contains smoothed histograms). The outlier is visible as the single separate point in the scatter plot and as the long tail of the smoothed histogram for <code>x</code> (we would expect most observations to fall in the range [-3,3] in normalized data).</p>
<div class="sourceCode" id="cb771"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb771-1"><a href="clustering-analysis.html#cb771-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;GGally&quot;</span>)</span>
<span id="cb771-2"><a href="clustering-analysis.html#cb771-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(ruspini_scaled_outlier)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-291-1.png" width="672" /></p>
<p>The outlier is a problem for k-means</p>
<div class="sourceCode" id="cb772"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb772-1"><a href="clustering-analysis.html#cb772-1" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(ruspini_scaled_outlier, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb772-2"><a href="clustering-analysis.html#cb772-2" aria-hidden="true" tabindex="-1"></a>ruspini_scaled_outlier_km <span class="ot">&lt;-</span> ruspini_scaled_outlier<span class="sc">%&gt;%</span></span>
<span id="cb772-3"><a href="clustering-analysis.html#cb772-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster))</span>
<span id="cb772-4"><a href="clustering-analysis.html#cb772-4" aria-hidden="true" tabindex="-1"></a>centroids <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(km<span class="sc">$</span>centers, <span class="at">rownames =</span> <span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb772-5"><a href="clustering-analysis.html#cb772-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb772-6"><a href="clustering-analysis.html#cb772-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled_outlier_km, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb772-7"><a href="clustering-analysis.html#cb772-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> centroids, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster), <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">size =</span> <span class="dv">10</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-292-1.png" width="672" /></p>
<p>This problem can be fixed by increasing the number of clusters and removing small clusters in a post-processing step or by identifying and removing outliers before clustering.</p>
</div>
<div id="local-outlier-factor-lof" class="section level4" number="7.5.1.2">
<h4><span class="header-section-number">7.5.1.2</span> Local Outlier Factor (LOF)</h4>
<p>The <a href="https://en.wikipedia.org/wiki/Local_outlier_factor">Local Outlier Factor</a> is related to concepts of DBSCAN can help to identify potential outliers.
Calculate the LOF (I choose a neighborhood size of 10 for density estimation),</p>
<div class="sourceCode" id="cb773"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb773-1"><a href="clustering-analysis.html#cb773-1" aria-hidden="true" tabindex="-1"></a>lof <span class="ot">&lt;-</span> <span class="fu">lof</span>(ruspini_scaled_outlier, <span class="at">k =</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## Warning in lof(ruspini_scaled_outlier, k = 10): lof: k
## is now deprecated. use minPts = 11 instead .</code></pre>
<div class="sourceCode" id="cb775"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb775-1"><a href="clustering-analysis.html#cb775-1" aria-hidden="true" tabindex="-1"></a>lof</span></code></pre></div>
<pre><code>##  [1]  0.978  1.044  0.926  1.024  1.022  0.966  1.154
##  [8]  1.071  1.124  1.058  1.008  0.933  1.000  1.074
## [15]  1.008  0.987  1.184  0.989  0.984  1.080  0.911
## [22]  1.019  1.524  0.979  1.045  0.958  1.022  0.934
## [29]  0.979  1.470  0.964  0.988  0.973  1.236  1.082
## [36]  1.326  1.566  1.018  0.998  1.029  1.378  1.107
## [43]  0.952  1.083  1.091  1.029  1.181  1.009  1.031
## [50]  1.030  1.002  1.201  1.001  1.071  0.968  0.954
## [57]  1.046  0.970  1.066  1.045  0.989  0.966  1.028
## [64]  0.991  1.152  0.942  0.977  1.000  0.984  0.998
## [71]  1.174  0.996  1.116  0.934  1.588 17.027</code></pre>
<div class="sourceCode" id="cb777"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb777-1"><a href="clustering-analysis.html#cb777-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled_outlier <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">lof =</span> lof), <span class="fu">aes</span>(x, y, <span class="at">color =</span> lof)) <span class="sc">+</span></span>
<span id="cb777-2"><a href="clustering-analysis.html#cb777-2" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">scale_color_gradient</span>(<span class="at">low =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">high =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-293-1.png" width="672" /></p>
<p>Plot the points sorted by increasing LOF and look for a knee.</p>
<div class="sourceCode" id="cb778"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb778-1"><a href="clustering-analysis.html#cb778-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">tibble</span>(<span class="at">index =</span> <span class="fu">seq_len</span>(<span class="fu">length</span>(lof)), <span class="at">lof =</span> <span class="fu">sort</span>(lof)), <span class="fu">aes</span>(index, lof)) <span class="sc">+</span></span>
<span id="cb778-2"><a href="clustering-analysis.html#cb778-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb778-3"><a href="clustering-analysis.html#cb778-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>, <span class="at">linetype =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-294-1.png" width="672" /></p>
<p>Choose a threshold above 1.</p>
<div class="sourceCode" id="cb779"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb779-1"><a href="clustering-analysis.html#cb779-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled_outlier <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">outlier =</span> lof <span class="sc">&gt;=</span> <span class="dv">2</span>), <span class="fu">aes</span>(x, y, <span class="at">color =</span> outlier)) <span class="sc">+</span></span>
<span id="cb779-2"><a href="clustering-analysis.html#cb779-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-295-1.png" width="672" /></p>
<div class="sourceCode" id="cb780"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb780-1"><a href="clustering-analysis.html#cb780-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Analyze the found outliers (they might be interesting data points) and then cluster the data without them.</span></span>
<span id="cb780-2"><a href="clustering-analysis.html#cb780-2" aria-hidden="true" tabindex="-1"></a>ruspini_scaled_clean <span class="ot">&lt;-</span> ruspini_scaled_outlier  <span class="sc">%&gt;%</span> <span class="fu">filter</span>(lof <span class="sc">&lt;</span> <span class="dv">2</span>)</span>
<span id="cb780-3"><a href="clustering-analysis.html#cb780-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb780-4"><a href="clustering-analysis.html#cb780-4" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(ruspini_scaled_clean, <span class="at">centers =</span> <span class="dv">4</span>, <span class="at">nstart =</span> <span class="dv">10</span>)</span>
<span id="cb780-5"><a href="clustering-analysis.html#cb780-5" aria-hidden="true" tabindex="-1"></a>ruspini_scaled_clean_km <span class="ot">&lt;-</span> ruspini_scaled_clean<span class="sc">%&gt;%</span></span>
<span id="cb780-6"><a href="clustering-analysis.html#cb780-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster))</span>
<span id="cb780-7"><a href="clustering-analysis.html#cb780-7" aria-hidden="true" tabindex="-1"></a>centroids <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(km<span class="sc">$</span>centers, <span class="at">rownames =</span> <span class="st">&quot;cluster&quot;</span>)</span>
<span id="cb780-8"><a href="clustering-analysis.html#cb780-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb780-9"><a href="clustering-analysis.html#cb780-9" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ruspini_scaled_clean_km, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb780-10"><a href="clustering-analysis.html#cb780-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> centroids, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster), <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">size =</span> <span class="dv">10</span>)</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-295-2.png" width="672" /></p>
<p>There are many other outlier removal strategies available. See, e.g., package
<a href="https://cran.r-project.org/package=outliers">outliers</a>.</p>
</div>
</div>
<div id="clustering-tendency" class="section level3" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Clustering Tendency</h3>
<p>Most clustering algorithms will always produce a clustering, even if the
data does not contain a cluster structure. It is typically good to check
cluster tendency before attempting to cluster the data.</p>
<p>We use again the smiley data.</p>
<div class="sourceCode" id="cb781"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb781-1"><a href="clustering-analysis.html#cb781-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mlbench)</span>
<span id="cb781-2"><a href="clustering-analysis.html#cb781-2" aria-hidden="true" tabindex="-1"></a>shapes <span class="ot">&lt;-</span> <span class="fu">mlbench.smiley</span>(<span class="at">n =</span> <span class="dv">500</span>, <span class="at">sd1 =</span> <span class="fl">0.1</span>, <span class="at">sd2 =</span> <span class="fl">0.05</span>)<span class="sc">$</span>x</span>
<span id="cb781-3"><a href="clustering-analysis.html#cb781-3" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(shapes) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>)</span>
<span id="cb781-4"><a href="clustering-analysis.html#cb781-4" aria-hidden="true" tabindex="-1"></a>shapes <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(shapes)</span></code></pre></div>
<div id="scatter-plots" class="section level4" number="7.5.2.1">
<h4><span class="header-section-number">7.5.2.1</span> Scatter plots</h4>
<p>The first step is visual inspection using scatter plots.</p>
<div class="sourceCode" id="cb782"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb782-1"><a href="clustering-analysis.html#cb782-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(shapes, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-297-1.png" width="672" /></p>
<p>Cluster tendency is typically indicated by several separated point clouds. Often an appropriate number of clusters can also be visually obtained by counting the number of point clouds. We see four clusters, but the mouth is not convex/spherical and thus will pose a problems to algorithms like k-means.</p>
<p>If the data has more than two features then you can use a pairs plot (scatterplot matrix) or look at a scatterplot of the first two principal components using PCA.
#### Visual Analysis for Cluster Tendency Assessment (VAT)</p>
<p>VAT reorders the
objects to show potential clustering tendency as a block structure
(dark blocks along the main diagonal). We scale the data before using Euclidean distance.</p>
<div class="sourceCode" id="cb783"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb783-1"><a href="clustering-analysis.html#cb783-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(seriation)</span>
<span id="cb783-2"><a href="clustering-analysis.html#cb783-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb783-3"><a href="clustering-analysis.html#cb783-3" aria-hidden="true" tabindex="-1"></a>d_shapes <span class="ot">&lt;-</span> <span class="fu">dist</span>(<span class="fu">scale</span>(shapes))</span>
<span id="cb783-4"><a href="clustering-analysis.html#cb783-4" aria-hidden="true" tabindex="-1"></a><span class="fu">VAT</span>(d_shapes, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-298-1.png" width="672" /></p>
<p>iVAT uses the largest distances for all possible paths between two objects
instead of the direct distances to make the block structure better visible.</p>
<div class="sourceCode" id="cb784"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb784-1"><a href="clustering-analysis.html#cb784-1" aria-hidden="true" tabindex="-1"></a><span class="fu">iVAT</span>(d_shapes, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-299-1.png" width="672" /></p>
</div>
<div id="hopkins-statistic" class="section level4" number="7.5.2.2">
<h4><span class="header-section-number">7.5.2.2</span> Hopkins statistic</h4>
<p><code>factoextra</code> can also create a VAT plot and calculate the <a href="https://en.wikipedia.org/wiki/Hopkins_statistic">Hopkins statistic</a> to assess clustering tendency. For the Hopkins statistic, a sample of size <span class="math inline">\(n\)</span> is drawn from the data and then compares the nearest neighbor distribution with a simulated dataset drawn from a random uniform distribution (see <a href="https://www.datanovia.com/en/lessons/assessing-clustering-tendency/#statistical-methods">detailed explanation</a>). A values &gt;.5 indicates usually a clustering tendency.</p>
<div class="sourceCode" id="cb785"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb785-1"><a href="clustering-analysis.html#cb785-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_clust_tendency</span>(shapes, <span class="at">n =</span> <span class="dv">10</span>)</span></code></pre></div>
<pre><code>## $hopkins_stat
## [1] 0.907
## 
## $plot</code></pre>
<p><img src="_main_files/figure-html/unnamed-chunk-300-1.png" width="672" /></p>
<p>Both plots show a strong cluster structure with 4 clusters.</p>
</div>
<div id="data-without-clustering-tendency" class="section level4" number="7.5.2.3">
<h4><span class="header-section-number">7.5.2.3</span> Data Without Clustering Tendency</h4>
<div class="sourceCode" id="cb787"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb787-1"><a href="clustering-analysis.html#cb787-1" aria-hidden="true" tabindex="-1"></a>data_random <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> <span class="fu">runif</span>(<span class="dv">500</span>), <span class="at">y =</span> <span class="fu">runif</span>(<span class="dv">500</span>))</span>
<span id="cb787-2"><a href="clustering-analysis.html#cb787-2" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(data_random, <span class="fu">aes</span>(x, y)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-301-1.png" width="672" /></p>
<p>No point clouds are visible, just noise.</p>
<div class="sourceCode" id="cb788"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb788-1"><a href="clustering-analysis.html#cb788-1" aria-hidden="true" tabindex="-1"></a>d_random <span class="ot">&lt;-</span> <span class="fu">dist</span>(data_random)</span>
<span id="cb788-2"><a href="clustering-analysis.html#cb788-2" aria-hidden="true" tabindex="-1"></a><span class="fu">VAT</span>(d_random, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-302-1.png" width="672" /></p>
<div class="sourceCode" id="cb789"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb789-1"><a href="clustering-analysis.html#cb789-1" aria-hidden="true" tabindex="-1"></a><span class="fu">iVAT</span>(d_random, <span class="at">col =</span> <span class="fu">bluered</span>(<span class="dv">100</span>))</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-302-2.png" width="672" /></p>
<div class="sourceCode" id="cb790"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb790-1"><a href="clustering-analysis.html#cb790-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_clust_tendency</span>(data_random, <span class="at">n =</span> <span class="dv">10</span>, <span class="at">graph =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<pre><code>## $hopkins_stat
## [1] 0.464
## 
## $plot
## NULL</code></pre>
<p>There is very little clustering structure visible indicating low clustering tendency and clustering should not be performed on this data. However, k-means can be used to partition the data into <span class="math inline">\(k\)</span> regions of roughly equivalent size. This can be used as a data-driven discretization of the space.</p>
</div>
<div id="k-means-on-data-without-clustering-tendency" class="section level4" number="7.5.2.4">
<h4><span class="header-section-number">7.5.2.4</span> k-means on Data Without Clustering Tendency</h4>
<p>What happens if we perform k-means on data that has no inherent clustering structure?</p>
<div class="sourceCode" id="cb792"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb792-1"><a href="clustering-analysis.html#cb792-1" aria-hidden="true" tabindex="-1"></a>km <span class="ot">&lt;-</span> <span class="fu">kmeans</span>(data_random, <span class="at">centers =</span> <span class="dv">4</span>)</span>
<span id="cb792-2"><a href="clustering-analysis.html#cb792-2" aria-hidden="true" tabindex="-1"></a>random_clustered<span class="ot">&lt;-</span> data_random <span class="sc">%&gt;%</span> <span class="fu">add_column</span>(<span class="at">cluster =</span> <span class="fu">factor</span>(km<span class="sc">$</span>cluster))</span>
<span id="cb792-3"><a href="clustering-analysis.html#cb792-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(random_clustered, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> cluster)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="_main_files/figure-html/unnamed-chunk-303-1.png" width="672" /></p>
<p>k-means discretizes the space into similarly sized regions.</p>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-R-mclust" class="csl-entry">
Fraley, C., Raftery, A. E., &amp; Scrucca, L. (2020). <em>Mclust: Gaussian mixture modelling for model-based clustering, classification, and density estimation</em>. <a href="https://mclust-org.github.io/mclust/">https://mclust-org.github.io/mclust/</a>
</div>
<div id="ref-R-seriation" class="csl-entry">
Hahsler, M., Buchta, C., &amp; Hornik, K. (2021). <em>Seriation: Infrastructure for ordering objects using seriation</em>. <a href="https://github.com/mhahsler/seriation">https://github.com/mhahsler/seriation</a>
</div>
<div id="ref-R-dbscan" class="csl-entry">
Hahsler, M., &amp; Piekenbrock, M. (2021). <em>Dbscan: Density based clustering of applications with noise (DBSCAN) and related algorithms</em>. <a href="https://github.com/mhahsler/dbscan">https://github.com/mhahsler/dbscan</a>
</div>
<div id="ref-R-fpc" class="csl-entry">
Hennig, C. (2020). <em>Fpc: Flexible procedures for clustering</em>. <a href="https://www.unibo.it/sitoweb/christian.hennig/en/">https://www.unibo.it/sitoweb/christian.hennig/en/</a>
</div>
<div id="ref-R-kernlab" class="csl-entry">
Karatzoglou, A., Smola, A., &amp; Hornik, K. (2019). <em>Kernlab: Kernel-based machine learning lab</em>. <a href="https://CRAN.R-project.org/package=kernlab">https://CRAN.R-project.org/package=kernlab</a>
</div>
<div id="ref-R-factoextra" class="csl-entry">
Kassambara, A., &amp; Mundt, F. (2020). <em>Factoextra: Extract and visualize the results of multivariate data analyses</em>. <a href="http://www.sthda.com/english/rpkgs/factoextra">http://www.sthda.com/english/rpkgs/factoextra</a>
</div>
<div id="ref-R-mlbench" class="csl-entry">
Leisch, F., &amp; Dimitriadou., E. (2021). <em>Mlbench: Machine learning benchmark problems</em>. <a href="https://CRAN.R-project.org/package=mlbench">https://CRAN.R-project.org/package=mlbench</a>
</div>
<div id="ref-R-cluster" class="csl-entry">
Maechler, M., Rousseeuw, P., Struyf, A., &amp; Hubert, M. (2021). <em>Cluster: "Finding groups in data": Cluster analysis extended rousseeuw et al.</em> <a href="https://svn.r-project.org/R-packages/trunk/cluster/">https://svn.r-project.org/R-packages/trunk/cluster/</a>
</div>
<div id="ref-R-e1071" class="csl-entry">
Meyer, D., Dimitriadou, E., Hornik, K., Weingessel, A., &amp; Leisch, F. (2021). <em>e1071: Misc functions of the department of statistics, probability theory group (formerly: E1071), TU wien</em>. <a href="https://CRAN.R-project.org/package=e1071">https://CRAN.R-project.org/package=e1071</a>
</div>
<div id="ref-R-GGally" class="csl-entry">
Schloerke, B., Cook, D., Larmarange, J., Briatte, F., Marbach, M., Thoen, E., Elberg, A., &amp; Crowley, J. (2021). <em>GGally: Extension to ggplot2</em>. <a href="https://CRAN.R-project.org/package=GGally">https://CRAN.R-project.org/package=GGally</a>
</div>
<div id="ref-Tan2005" class="csl-entry">
Tan, P.-N., Steinbach, M. S., &amp; Kumar, V. (2005). <em>Introduction to data mining</em>. Addison-Wesley. <a href="http://www-users.cs.umn.edu/\%7Ekumar/dmbook/">http://www-users.cs.umn.edu/\%7Ekumar/dmbook/</a>
</div>
<div id="ref-R-tidyverse" class="csl-entry">
Wickham, H. (2021c). <em>Tidyverse: Easily install and load the tidyverse</em>. <a href="https://CRAN.R-project.org/package=tidyverse">https://CRAN.R-project.org/package=tidyverse</a>
</div>
<div id="ref-R-scatterpie" class="csl-entry">
Yu, G. (2021). <em>Scatterpie: Scatter pie plot</em>. <a href="https://CRAN.R-project.org/package=scatterpie">https://CRAN.R-project.org/package=scatterpie</a>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="association-analysis-advanced-concepts.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/lunr.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/mhahsler/Introduction_to_Data_Mining_R_Examples/blob/master/book_src/07_cluster_analysis.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/mhahsler/Introduction_to_Data_Mining_R_Examples/blob/master/book_src/07_cluster_analysis.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
